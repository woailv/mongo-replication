2019-09-21T18:39:29.170+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T18:39:29.174+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=5488 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T18:39:29.174+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T18:39:29.174+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T18:39:29.174+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T18:39:29.174+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T18:39:29.174+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T18:39:29.175+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T18:39:29.175+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T18:39:29.175+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T18:39:29.175+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T18:39:29.175+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { port: 37017 }, replication: { oplogSizeMB: 2048, replSetName: "rs0" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T18:39:29.176+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb1 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T18:39:29.176+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T18:39:29.216+0800 I  STORAGE  [initandlisten] WiredTiger message [1569062369:215436][5488:140713979633712], txn-recover: Recovering log 10 through 11
2019-09-21T18:39:29.299+0800 I  STORAGE  [initandlisten] WiredTiger message [1569062369:298519][5488:140713979633712], txn-recover: Recovering log 11 through 11
2019-09-21T18:39:29.392+0800 I  STORAGE  [initandlisten] WiredTiger message [1569062369:391589][5488:140713979633712], txn-recover: Main recovery loop: starting at 10/6144 to 11/256
2019-09-21T18:39:29.553+0800 I  STORAGE  [initandlisten] WiredTiger message [1569062369:552739][5488:140713979633712], txn-recover: Recovering log 10 through 11
2019-09-21T18:39:29.647+0800 I  STORAGE  [initandlisten] WiredTiger message [1569062369:646828][5488:140713979633712], txn-recover: Recovering log 11 through 11
2019-09-21T18:39:29.731+0800 I  STORAGE  [initandlisten] WiredTiger message [1569062369:730933][5488:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T18:39:29.809+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T18:39:29.820+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T18:39:29.830+0800 I  CONTROL  [initandlisten] 
2019-09-21T18:39:29.830+0800 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] 
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T18:39:29.831+0800 I  CONTROL  [initandlisten] 
2019-09-21T18:39:29.853+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T18:39:29.854+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T18:39:29.854+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T18:39:29.854+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T18:39:29.855+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T18:39:29.985+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T18:39:29.987+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T18:39:29.987+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T18:39:29.987+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T18:39:29.987+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T18:39:29.987+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T18:39:29.989+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T18:39:29.989+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T18:39:29.989+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T18:39:29.989+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T18:39:29.989+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T18:39:30.002+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T18:39:49.084+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:60772 #1 (1 connection now open)
2019-09-21T18:39:49.087+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:60772 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T18:44:29.990+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T18:44:29.990+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T18:49:29.989+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T18:49:29.989+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T18:54:29.989+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T18:54:29.989+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T18:59:29.989+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T18:59:29.989+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:04:29.989+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:04:29.990+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:09:29.989+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:09:29.990+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:09:42.775+0800 I  CONTROL  [thread2] Ctrl-C signal
2019-09-21T19:09:42.775+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T19:09:42.776+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T19:09:42.776+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T19:09:42.776+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T19:09:42.776+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T19:09:42.777+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T19:09:42.777+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T19:09:42.779+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T19:09:42.779+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T19:09:42.779+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T19:09:42.779+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T19:09:42.779+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T19:09:42.779+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T19:09:42.819+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T19:09:42.819+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T19:09:42.819+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T19:09:42.850+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T19:09:42.850+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T19:09:42.850+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T19:10:11.678+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T19:10:12.126+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=6560 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T19:10:12.171+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T19:10:12.173+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb1 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T19:10:12.173+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T19:10:12.210+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064212:209854][6560:140713979633712], txn-recover: Recovering log 11 through 12
2019-09-21T19:10:12.291+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064212:290929][6560:140713979633712], txn-recover: Recovering log 12 through 12
2019-09-21T19:10:12.383+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064212:383018][6560:140713979633712], txn-recover: Main recovery loop: starting at 11/6272 to 12/256
2019-09-21T19:10:12.542+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064212:542170][6560:140713979633712], txn-recover: Recovering log 11 through 12
2019-09-21T19:10:12.636+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064212:636258][6560:140713979633712], txn-recover: Recovering log 12 through 12
2019-09-21T19:10:12.717+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064212:717361][6560:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T19:10:12.763+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T19:10:12.774+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T19:10:12.780+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:10:12.780+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T19:10:12.780+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T19:10:12.780+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T19:10:12.780+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T19:10:12.780+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T19:10:12.780+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T19:10:12.780+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:10:12.802+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T19:10:12.802+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T19:10:12.803+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T19:10:12.803+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T19:10:12.804+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T19:10:12.932+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T19:10:12.933+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T19:10:12.933+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T19:10:12.933+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T19:10:12.934+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T19:10:12.934+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T19:10:12.936+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:10:12.936+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T19:10:12.936+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T19:10:12.936+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T19:10:12.936+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:10:13.002+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T19:10:16.205+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61046 #1 (1 connection now open)
2019-09-21T19:10:16.206+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T19:10:16.206+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T19:10:16.206+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61046 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:10:22.858+0800 I  NETWORK  [conn1] end connection 127.0.0.1:61046 (0 connections now open)
2019-09-21T19:10:26.124+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61051 #2 (1 connection now open)
2019-09-21T19:10:26.126+0800 I  NETWORK  [conn2] received client metadata from 127.0.0.1:61051 conn2: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:10:43.952+0800 I  COMMAND  [conn2] initiate : no configuration specified. Using a default configuration for the set
2019-09-21T19:10:43.952+0800 I  COMMAND  [conn2] created this configuration for initiation : { _id: "rs0", version: 1, members: [ { _id: 0, host: "localhost:37017" } ] }
2019-09-21T19:10:43.952+0800 I  REPL     [conn2] replSetInitiate admin command received from client
2019-09-21T19:10:43.959+0800 I  REPL     [conn2] replSetInitiate config object with 1 members parses ok
2019-09-21T19:10:43.959+0800 I  REPL     [conn2] ******
2019-09-21T19:10:43.959+0800 I  REPL     [conn2] creating replication oplog of size: 990MB...
2019-09-21T19:10:43.959+0800 I  STORAGE  [conn2] createCollection: local.oplog.rs with generated UUID: 5783e95a-925f-4aa4-9f4b-95d8572bddc6 and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T19:10:43.972+0800 I  STORAGE  [conn2] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T19:10:43.972+0800 I  STORAGE  [conn2] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T19:10:43.972+0800 I  STORAGE  [conn2] Scanning the oplog to determine where to place markers for truncation
2019-09-21T19:10:44.020+0800 I  REPL     [conn2] ******
2019-09-21T19:10:44.020+0800 I  STORAGE  [conn2] createCollection: local.system.replset with generated UUID: a65dd018-9351-4fe6-b6c6-427869ca2dc3 and options: {}
2019-09-21T19:10:44.044+0800 I  INDEX    [conn2] index build: done building index _id_ on ns local.system.replset
2019-09-21T19:10:44.050+0800 I  REPL     [conn2] New replica set config in use: { _id: "rs0", version: 1, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:10:44.050+0800 I  REPL     [conn2] This node is localhost:37017 in the config
2019-09-21T19:10:44.050+0800 I  REPL     [conn2] transition to STARTUP2 from STARTUP
2019-09-21T19:10:44.050+0800 I  REPL     [conn2] Starting replication storage threads
2019-09-21T19:10:44.051+0800 I  REPL     [conn2] transition to RECOVERING from STARTUP2
2019-09-21T19:10:44.051+0800 I  REPL     [conn2] Starting replication fetcher thread
2019-09-21T19:10:44.052+0800 I  REPL     [conn2] Starting replication applier thread
2019-09-21T19:10:44.052+0800 I  REPL     [conn2] Starting replication reporter thread
2019-09-21T19:10:44.052+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T19:10:44.052+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T19:10:44.052+0800 I  ELECTION [rsSync-0] conducting a dry run election to see if we could be elected. current term: 0
2019-09-21T19:10:44.052+0800 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 1
2019-09-21T19:10:44.058+0800 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 1
2019-09-21T19:10:44.058+0800 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
2019-09-21T19:10:44.058+0800 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
2019-09-21T19:10:44.058+0800 I  REPL     [replexec-0] Entering primary catch-up mode.
2019-09-21T19:10:44.058+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2019-09-21T19:10:44.058+0800 I  REPL     [replexec-0] Stopping replication producer
2019-09-21T19:10:45.052+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:10:45.052+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:10:46.052+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:10:46.052+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:10:46.053+0800 I  SHARDING [rsSync-0] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T19:10:46.053+0800 I  STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: 59ce2d95-3f83-4e14-bc03-abb88b892ca8 and options: {}
2019-09-21T19:10:46.077+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.transactions
2019-09-21T19:10:46.078+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T19:10:46.078+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T19:10:46.078+0800 I  STORAGE  [monitoring-keys-for-HMAC] createCollection: admin.system.keys with generated UUID: 0ad2349c-7899-4b1a-9675-f6c15381c2ed and options: {}
2019-09-21T19:10:46.102+0800 I  INDEX    [monitoring-keys-for-HMAC] index build: done building index _id_ on ns admin.system.keys
2019-09-21T19:10:46.107+0800 I  STORAGE  [monitoring-keys-for-HMAC] Triggering the first stable checkpoint. Initial Data: Timestamp(1569064244, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569064246, 4)
2019-09-21T19:10:53.658+0800 I  ACCESS   [conn2] Unauthorized: not authorized on test to execute command { listCollections: 1.0, filter: {}, nameOnly: true, authorizedCollections: true, lsid: { id: UUID("d2e9a134-e83b-467a-bce9-0d1b572d21dc") }, $clusterTime: { clusterTime: Timestamp(1569064246, 5), signature: { hash: BinData(0, 74B73D2E572123081ACC7F14B866CEA56006BF61), keyId: 6739079621892898818 } }, $db: "test" }
2019-09-21T19:12:02.624+0800 I  STORAGE  [conn2] createCollection: admin.system.users with generated UUID: 3d47f0c4-ade7-43b3-826a-d1b46280ed7e and options: {}
2019-09-21T19:12:02.650+0800 I  INDEX    [conn2] index build: done building index _id_ on ns admin.system.users
2019-09-21T19:12:02.663+0800 I  INDEX    [conn2] index build: done building index user_1_db_1 on ns admin.system.users
2019-09-21T19:12:37.480+0800 I  ACCESS   [conn2] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61051
2019-09-21T19:13:13.923+0800 I  SHARDING [conn2] Marking collection test.c as collection version: <unsharded>
2019-09-21T19:13:13.924+0800 I  STORAGE  [conn2] createCollection: test.c with generated UUID: b034ea59-f24e-42ca-8085-4048a1182b15 and options: {}
2019-09-21T19:13:13.950+0800 I  INDEX    [conn2] index build: done building index _id_ on ns test.c
2019-09-21T19:16:35.383+0800 I  REPL     [conn2] replSetReconfig admin command received from client; new config: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1.0, host: "localhost:37018" } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:16:35.419+0800 I  REPL     [conn2] replSetReconfig config object with 2 members parses ok
2019-09-21T19:16:35.419+0800 I  REPL     [conn2] Scheduling remote command request for reconfig quorum check: RemoteCommand 1 -- target:localhost:37018 db:admin cmd:{ replSetHeartbeat: "rs0", configVersion: 2, hbv: 1, from: "localhost:37017", fromId: 0, term: 1 }
2019-09-21T19:16:35.419+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:16:35.453+0800 I  REPL     [conn2] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:16:35.453+0800 I  REPL     [conn2] This node is localhost:37017 in the config
2019-09-21T19:16:35.454+0800 I  REPL     [replexec-0] Member localhost:37018 is now in state STARTUP
2019-09-21T19:16:35.456+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61119 #5 (2 connections now open)
2019-09-21T19:16:35.457+0800 I  NETWORK  [conn5] received client metadata from 127.0.0.1:61119 conn5: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:35.469+0800 I  ACCESS   [conn5] Successfully authenticated as principal __system on local from client 127.0.0.1:61119
2019-09-21T19:16:35.471+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61120 #6 (3 connections now open)
2019-09-21T19:16:35.505+0800 I  ACCESS   [conn6] Successfully authenticated as principal __system on local from client 127.0.0.1:61120
2019-09-21T19:16:35.505+0800 I  NETWORK  [conn6] end connection 127.0.0.1:61120 (2 connections now open)
2019-09-21T19:16:35.713+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61121 #7 (3 connections now open)
2019-09-21T19:16:35.713+0800 I  NETWORK  [conn7] received client metadata from 127.0.0.1:61121 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:35.725+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 127.0.0.1:61121
2019-09-21T19:16:35.728+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61122 #8 (4 connections now open)
2019-09-21T19:16:35.728+0800 I  NETWORK  [conn8] received client metadata from 127.0.0.1:61122 conn8: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:35.740+0800 I  ACCESS   [conn8] Successfully authenticated as principal __system on local from client 127.0.0.1:61122
2019-09-21T19:16:35.768+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61123 #9 (5 connections now open)
2019-09-21T19:16:35.768+0800 I  NETWORK  [conn9] received client metadata from 127.0.0.1:61123 conn9: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:35.801+0800 I  ACCESS   [conn9] Successfully authenticated as principal __system on local from client 127.0.0.1:61123
2019-09-21T19:16:35.802+0800 I  NETWORK  [conn9] end connection 127.0.0.1:61123 (4 connections now open)
2019-09-21T19:16:35.853+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61124 #10 (5 connections now open)
2019-09-21T19:16:35.854+0800 I  NETWORK  [conn10] received client metadata from 127.0.0.1:61124 conn10: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:35.887+0800 I  ACCESS   [conn10] Successfully authenticated as principal __system on local from client 127.0.0.1:61124
2019-09-21T19:16:35.888+0800 I  NETWORK  [conn10] end connection 127.0.0.1:61124 (4 connections now open)
2019-09-21T19:16:35.931+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61125 #11 (5 connections now open)
2019-09-21T19:16:35.932+0800 I  NETWORK  [conn11] received client metadata from 127.0.0.1:61125 conn11: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:35.966+0800 I  ACCESS   [conn11] Successfully authenticated as principal __system on local from client 127.0.0.1:61125
2019-09-21T19:16:35.967+0800 I  NETWORK  [conn11] end connection 127.0.0.1:61125 (4 connections now open)
2019-09-21T19:16:36.031+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61126 #12 (5 connections now open)
2019-09-21T19:16:36.031+0800 I  NETWORK  [conn12] received client metadata from 127.0.0.1:61126 conn12: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:36.063+0800 I  ACCESS   [conn12] Successfully authenticated as principal __system on local from client 127.0.0.1:61126
2019-09-21T19:16:36.064+0800 I  NETWORK  [conn12] end connection 127.0.0.1:61126 (4 connections now open)
2019-09-21T19:16:36.164+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61127 #13 (5 connections now open)
2019-09-21T19:16:36.165+0800 I  NETWORK  [conn13] received client metadata from 127.0.0.1:61127 conn13: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:36.197+0800 I  ACCESS   [conn13] Successfully authenticated as principal __system on local from client 127.0.0.1:61127
2019-09-21T19:16:36.198+0800 I  NETWORK  [conn13] end connection 127.0.0.1:61127 (4 connections now open)
2019-09-21T19:16:36.277+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61128 #14 (5 connections now open)
2019-09-21T19:16:36.278+0800 I  NETWORK  [conn14] received client metadata from 127.0.0.1:61128 conn14: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:36.310+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61128
2019-09-21T19:16:36.312+0800 I  NETWORK  [conn14] end connection 127.0.0.1:61128 (4 connections now open)
2019-09-21T19:16:37.454+0800 I  REPL     [replexec-0] Member localhost:37018 is now in state SECONDARY
2019-09-21T19:16:45.728+0800 I  NETWORK  [conn7] end connection 127.0.0.1:61121 (3 connections now open)
2019-09-21T19:16:46.357+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61129 #15 (4 connections now open)
2019-09-21T19:16:46.357+0800 I  NETWORK  [conn15] received client metadata from 127.0.0.1:61129 conn15: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:46.369+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61129
2019-09-21T19:19:26.006+0800 I  CONTROL  [thread14] Ctrl-C signal
2019-09-21T19:19:26.006+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T19:19:26.007+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:19:26.007+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:19:26.007+0800 I  REPL     [consoleTerminate] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 1 }
2019-09-21T19:19:26.007+0800 I  REPL     [consoleTerminate] transition to SECONDARY from PRIMARY
2019-09-21T19:19:26.007+0800 I  REPL     [consoleTerminate] Handing off election to localhost:37018
2019-09-21T19:19:26.007+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T19:19:26.007+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T19:19:26.008+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T19:19:26.008+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T19:19:26.008+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T19:19:26.008+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T19:19:26.008+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T19:19:26.242+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T19:19:26.242+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T19:19:26.242+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:19:26.243+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:19:26.244+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T19:19:26.244+0800 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T19:19:26.244+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T19:19:26.244+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T19:19:26.248+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T19:19:26.249+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T19:19:26.249+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T19:19:26.249+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T19:19:26.254+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T19:19:26.254+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T19:19:26.254+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T19:19:26.310+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T19:19:26.310+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T19:19:26.310+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T19:19:26.360+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T19:19:26.360+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T19:19:26.360+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T19:19:27.326+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T19:19:27.326+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=3620 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T19:19:27.802+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { port: 37017 }, replication: { replSetName: "rs0" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T19:19:27.804+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb1 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T19:19:27.804+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T19:19:27.843+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064767:842874][3620:140713979633712], txn-recover: Recovering log 12 through 13
2019-09-21T19:19:27.929+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064767:928979][3620:140713979633712], txn-recover: Recovering log 13 through 13
2019-09-21T19:19:28.021+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064768:21067][3620:140713979633712], txn-recover: Main recovery loop: starting at 12/57472 to 13/256
2019-09-21T19:19:28.022+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064768:22068][3620:140713979633712], txn-recover: Recovering log 12 through 13
2019-09-21T19:19:28.118+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064768:117934][3620:140713979633712], txn-recover: Recovering log 13 through 13
2019-09-21T19:19:28.198+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064768:198014][3620:140713979633712], txn-recover: Set global recovery timestamp: (1569064756,1)
2019-09-21T19:19:28.276+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569064756, 1)
2019-09-21T19:19:28.286+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T19:19:28.286+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 60 records totaling to 8133 bytes
2019-09-21T19:19:28.286+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T19:19:28.295+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T19:19:28.307+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:19:28.340+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T19:19:28.341+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T19:19:28.341+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T19:19:28.341+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T19:19:28.342+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T19:19:28.473+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T19:19:28.475+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T19:19:28.475+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T19:19:28.476+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T19:19:28.476+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569064756, 1) (top of oplog: { ts: Timestamp(1569064756, 1), t: 1 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T19:19:28.476+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569064756, 1)
2019-09-21T19:19:28.476+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T19:19:28.476+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T19:19:28.477+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T19:19:28.478+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:19:28.478+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T19:19:28.478+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T19:19:28.478+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:19:28.478+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T19:19:28.482+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:19:28.482+0800 I  REPL     [replexec-0] This node is localhost:37017 in the config
2019-09-21T19:19:28.482+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T19:19:28.483+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T19:19:28.483+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:19:28.485+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T19:19:28.485+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T19:19:28.485+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T19:19:28.485+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T19:19:28.485+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T19:19:28.485+0800 I  REPL     [rsBackgroundSync] waiting for 1 pings from other members before syncing
2019-09-21T19:19:28.638+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61140 #3 (1 connection now open)
2019-09-21T19:19:28.639+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61140 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:28.640+0800 I  ACCESS   [conn3] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61140 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:19:28.640+0800 I  NETWORK  [conn3] end connection 127.0.0.1:61140 (0 connections now open)
2019-09-21T19:19:29.141+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61144 #4 (1 connection now open)
2019-09-21T19:19:29.143+0800 I  NETWORK  [conn4] received client metadata from 127.0.0.1:61144 conn4: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:29.144+0800 I  ACCESS   [conn4] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61144 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:19:29.144+0800 I  NETWORK  [conn4] end connection 127.0.0.1:61144 (0 connections now open)
2019-09-21T19:19:29.473+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61146 #5 (1 connection now open)
2019-09-21T19:19:29.474+0800 I  NETWORK  [conn5] received client metadata from 127.0.0.1:61146 conn5: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:29.474+0800 I  ACCESS   [conn5] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61146 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:19:29.475+0800 I  NETWORK  [conn5] end connection 127.0.0.1:61146 (0 connections now open)
2019-09-21T19:19:29.644+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61148 #6 (1 connection now open)
2019-09-21T19:19:29.645+0800 I  NETWORK  [conn6] received client metadata from 127.0.0.1:61148 conn6: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:29.645+0800 I  ACCESS   [conn6] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61148 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:19:29.646+0800 I  NETWORK  [conn6] end connection 127.0.0.1:61148 (0 connections now open)
2019-09-21T19:19:30.487+0800 W  NETWORK  [replexec-1] Failed to check socket connectivity: 操作成功完成。
2019-09-21T19:19:30.487+0800 I  CONNPOOL [replexec-1] dropping unhealthy pooled connection to localhost:37018
2019-09-21T19:19:30.487+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:19:31.444+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61151 #7 (1 connection now open)
2019-09-21T19:19:31.444+0800 I  NETWORK  [conn7] end connection 127.0.0.1:61151 (0 connections now open)
2019-09-21T19:19:31.448+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61152 #8 (1 connection now open)
2019-09-21T19:19:31.449+0800 I  NETWORK  [conn8] received client metadata from 127.0.0.1:61152 conn8: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:31.490+0800 I  REPL     [replexec-0] Member localhost:37018 is now in state SECONDARY
2019-09-21T19:19:31.490+0800 I  REPL     [replexec-0] transition to SECONDARY from RECOVERING
2019-09-21T19:19:31.490+0800 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
2019-09-21T19:19:35.533+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61153 #10 (2 connections now open)
2019-09-21T19:19:35.534+0800 I  NETWORK  [conn10] received client metadata from 127.0.0.1:61153 conn10: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:35.534+0800 I  SHARDING [conn10] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T19:19:35.537+0800 I  ACCESS   [conn10] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61153
2019-09-21T19:19:41.548+0800 I  ELECTION [conn8] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064756, 1), t: 1 } }
2019-09-21T19:19:41.548+0800 I  ELECTION [conn8] Sending vote response: { term: 2, voteGranted: true, reason: "" }
2019-09-21T19:19:41.554+0800 I  ELECTION [conn8] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 3, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064756, 1), t: 1 } }
2019-09-21T19:19:41.554+0800 I  ELECTION [conn8] Sending vote response: { term: 3, voteGranted: true, reason: "" }
2019-09-21T19:19:42.001+0800 I  REPL     [replexec-0] Member localhost:37018 is now in state PRIMARY
2019-09-21T19:19:44.497+0800 I  REPL     [rsBackgroundSync] sync source candidate: localhost:37018
2019-09-21T19:19:44.497+0800 I  CONNPOOL [RS] Connecting to localhost:37018
2019-09-21T19:19:44.500+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:37018
2019-09-21T19:19:44.501+0800 I  SHARDING [rsSync-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T19:19:46.503+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T19:21:48.199+0800 I  SHARDING [conn10] Marking collection test.system.users as collection version: <unsharded>
2019-09-21T19:22:29.102+0800 I  CONTROL  [thread8] Ctrl-C signal
2019-09-21T19:22:29.102+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T19:22:29.102+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T19:22:29.103+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T19:22:29.103+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T19:22:29.103+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T19:22:29.103+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:37018: CallbackCanceled: Reporter no longer valid
2019-09-21T19:22:29.103+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T19:22:29.103+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T19:22:29.103+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2019-09-21T19:22:29.103+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T19:22:29.104+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T19:22:29.104+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T19:22:29.104+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:22:29.104+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:22:29.104+0800 I  CONNPOOL [RS] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T19:22:29.105+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T19:22:29.106+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T19:22:29.106+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T19:22:29.110+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T19:22:29.110+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T19:22:29.110+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T19:22:29.110+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T19:22:29.116+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T19:22:29.116+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T19:22:29.116+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T19:22:29.148+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T19:22:29.148+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T19:22:29.149+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T19:22:29.198+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T19:22:29.198+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T19:22:29.198+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T19:22:29.606+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T19:22:29.609+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=5796 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T19:22:30.109+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T19:22:30.111+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb1 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T19:22:30.111+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T19:22:30.149+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064950:149448][5796:140713979633712], txn-recover: Recovering log 13 through 14
2019-09-21T19:22:30.241+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064950:241564][5796:140713979633712], txn-recover: Recovering log 14 through 14
2019-09-21T19:22:30.341+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064950:341633][5796:140713979633712], txn-recover: Main recovery loop: starting at 13/19456 to 14/256
2019-09-21T19:22:30.518+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064950:517800][5796:140713979633712], txn-recover: Recovering log 13 through 14
2019-09-21T19:22:30.633+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064950:632909][5796:140713979633712], txn-recover: Recovering log 14 through 14
2019-09-21T19:22:30.726+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064950:726004][5796:140713979633712], txn-recover: Set global recovery timestamp: (1569064943,1)
2019-09-21T19:22:30.779+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569064943, 1)
2019-09-21T19:22:30.790+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T19:22:30.790+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 77 records totaling to 10001 bytes
2019-09-21T19:22:30.790+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T19:22:30.802+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T19:22:30.813+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:22:30.813+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T19:22:30.813+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T19:22:30.813+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T19:22:30.813+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T19:22:30.813+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T19:22:30.813+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T19:22:30.813+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:22:30.852+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T19:22:30.853+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T19:22:30.853+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T19:22:30.854+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T19:22:30.855+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T19:22:30.998+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T19:22:30.999+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T19:22:30.999+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T19:22:31.000+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T19:22:31.001+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569064943, 1) (top of oplog: { ts: Timestamp(1569064943, 1), t: 3 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T19:22:31.001+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569064943, 1)
2019-09-21T19:22:31.001+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T19:22:31.001+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T19:22:31.001+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T19:22:31.003+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:22:31.003+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T19:22:31.003+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T19:22:31.004+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:22:31.004+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T19:22:31.007+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:22:31.007+0800 I  REPL     [replexec-0] This node is localhost:37017 in the config
2019-09-21T19:22:31.007+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T19:22:31.008+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T19:22:31.008+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:31.009+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T19:22:31.009+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T19:22:31.009+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T19:22:31.009+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T19:22:31.009+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T19:22:31.010+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2019-09-21T19:22:31.089+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61259 #3 (1 connection now open)
2019-09-21T19:22:31.089+0800 I  SHARDING [conn3] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T19:22:31.090+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61259 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:32.012+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:33.017+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:34.022+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:35.026+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:36.030+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:37.035+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:38.040+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:38.544+0800 I  NETWORK  [conn3] end connection 127.0.0.1:61259 (0 connections now open)
2019-09-21T19:22:39.677+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:39.958+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61292 #26 (1 connection now open)
2019-09-21T19:22:39.996+0800 I  ACCESS   [conn26] Successfully authenticated as principal __system on local from client 127.0.0.1:61292
2019-09-21T19:22:39.997+0800 I  NETWORK  [conn26] end connection 127.0.0.1:61292 (0 connections now open)
2019-09-21T19:22:40.001+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61293 #27 (1 connection now open)
2019-09-21T19:22:40.002+0800 I  NETWORK  [conn27] received client metadata from 127.0.0.1:61293 conn27: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:40.017+0800 I  ACCESS   [conn27] Successfully authenticated as principal __system on local from client 127.0.0.1:61293
2019-09-21T19:22:40.107+0800 I  NETWORK  [conn27] end connection 127.0.0.1:61293 (0 connections now open)
2019-09-21T19:22:40.677+0800 I  REPL_HB  [replexec-1] Heartbeat to localhost:37018 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37018 (127.0.0.1:37018) :: caused by :: Ŀܾ޷ӡ
2019-09-21T19:22:40.677+0800 I  REPL     [replexec-1] Member localhost:37018 is now in state RS_DOWN - Error connecting to localhost:37018 (127.0.0.1:37018) :: caused by :: Ŀܾ޷ӡ
2019-09-21T19:22:40.678+0800 I  REPL     [replexec-1] transition to SECONDARY from RECOVERING
2019-09-21T19:22:40.678+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
2019-09-21T19:22:41.010+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:42.011+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:22:42.768+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61298 #28 (1 connection now open)
2019-09-21T19:22:42.801+0800 I  ACCESS   [conn28] Successfully authenticated as principal __system on local from client 127.0.0.1:61298
2019-09-21T19:22:42.801+0800 I  NETWORK  [conn28] end connection 127.0.0.1:61298 (0 connections now open)
2019-09-21T19:22:42.805+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61299 #29 (1 connection now open)
2019-09-21T19:22:42.805+0800 I  NETWORK  [conn29] received client metadata from 127.0.0.1:61299 conn29: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:42.818+0800 I  ACCESS   [conn29] Successfully authenticated as principal __system on local from client 127.0.0.1:61299
2019-09-21T19:22:43.024+0800 I  REPL     [replexec-0] Member localhost:37018 is now in state SECONDARY
2019-09-21T19:22:51.366+0800 I  ELECTION [replexec-0] Starting an election, since we've seen no PRIMARY in the past 10000ms
2019-09-21T19:22:51.366+0800 I  ELECTION [replexec-0] conducting a dry run election to see if we could be elected. current term: 3
2019-09-21T19:22:51.366+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 37 -- target:localhost:37018 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 3, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064943, 1), t: 3 } }
2019-09-21T19:22:51.367+0800 I  ELECTION [replexec-1] VoteRequester(term 3 dry run) received a yes vote from localhost:37018; response message: { term: 3, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569064943, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569064943, 1) }
2019-09-21T19:22:51.367+0800 I  ELECTION [replexec-1] dry election run succeeded, running for election in term 4
2019-09-21T19:22:51.373+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 38 -- target:localhost:37018 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 4, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064943, 1), t: 3 } }
2019-09-21T19:22:51.379+0800 I  ELECTION [replexec-1] VoteRequester(term 4) received a yes vote from localhost:37018; response message: { term: 4, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569064943, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569064943, 1) }
2019-09-21T19:22:51.380+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 4
2019-09-21T19:22:51.380+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2019-09-21T19:22:51.380+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
2019-09-21T19:22:51.380+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2019-09-21T19:22:51.381+0800 I  REPL     [replexec-2] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1569064943, 1), t: 3 }. My Last Applied: { ts: Timestamp(1569064943, 1), t: 3 }
2019-09-21T19:22:51.381+0800 I  REPL     [replexec-2] Exited primary catch-up mode.
2019-09-21T19:22:51.381+0800 I  REPL     [replexec-2] Stopping replication producer
2019-09-21T19:22:52.016+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:22:52.016+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:22:53.016+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:22:53.016+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:22:53.017+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T19:22:53.017+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T19:22:53.815+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61301 #31 (2 connections now open)
2019-09-21T19:22:53.816+0800 I  NETWORK  [conn31] received client metadata from 127.0.0.1:61301 conn31: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:53.828+0800 I  ACCESS   [conn31] Successfully authenticated as principal __system on local from client 127.0.0.1:61301
2019-09-21T19:22:53.837+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61302 #32 (3 connections now open)
2019-09-21T19:22:53.838+0800 I  NETWORK  [conn32] received client metadata from 127.0.0.1:61302 conn32: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:53.849+0800 I  ACCESS   [conn32] Successfully authenticated as principal __system on local from client 127.0.0.1:61302
2019-09-21T19:23:26.370+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61315 #33 (4 connections now open)
2019-09-21T19:23:26.371+0800 I  NETWORK  [conn33] received client metadata from 127.0.0.1:61315 conn33: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:23:26.375+0800 I  ACCESS   [conn33] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61315
2019-09-21T19:23:35.111+0800 I  SHARDING [conn33] Marking collection admin.test as collection version: <unsharded>
2019-09-21T19:23:55.919+0800 I  SHARDING [conn33] Marking collection test.c as collection version: <unsharded>
2019-09-21T19:26:22.773+0800 I  CONTROL  [thread9] Ctrl-C signal
2019-09-21T19:26:22.773+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T19:26:22.774+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:26:22.774+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:26:22.774+0800 I  REPL     [consoleTerminate] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 1 }
2019-09-21T19:26:22.774+0800 I  REPL     [consoleTerminate] transition to SECONDARY from PRIMARY
2019-09-21T19:26:22.774+0800 I  REPL     [consoleTerminate] Handing off election to localhost:37018
2019-09-21T19:26:22.774+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T19:26:22.775+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T19:26:22.775+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T19:26:22.775+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T19:26:22.775+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T19:26:22.775+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T19:26:22.775+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T19:26:23.110+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T19:26:23.110+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T19:26:23.110+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:26:23.111+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:26:23.111+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T19:26:23.111+0800 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T19:26:23.112+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T19:26:23.112+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T19:26:23.114+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T19:26:23.114+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T19:26:23.114+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T19:26:23.114+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T19:26:23.119+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T19:26:23.119+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T19:26:23.119+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T19:26:23.127+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T19:26:23.127+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T19:26:23.127+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T19:26:23.171+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T19:26:23.172+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T19:26:23.172+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T19:26:38.006+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T19:26:38.457+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=5752 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T19:26:38.506+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T19:26:38.508+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb1 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T19:26:38.508+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T19:26:38.545+0800 I  STORAGE  [initandlisten] WiredTiger message [1569065198:545193][5752:140713979633712], txn-recover: Recovering log 14 through 15
2019-09-21T19:26:38.636+0800 I  STORAGE  [initandlisten] WiredTiger message [1569065198:636283][5752:140713979633712], txn-recover: Recovering log 15 through 15
2019-09-21T19:26:38.737+0800 I  STORAGE  [initandlisten] WiredTiger message [1569065198:737381][5752:140713979633712], txn-recover: Main recovery loop: starting at 14/13952 to 15/256
2019-09-21T19:26:38.738+0800 I  STORAGE  [initandlisten] WiredTiger message [1569065198:737381][5752:140713979633712], txn-recover: Recovering log 14 through 15
2019-09-21T19:26:38.839+0800 I  STORAGE  [initandlisten] WiredTiger message [1569065198:839475][5752:140713979633712], txn-recover: Recovering log 15 through 15
2019-09-21T19:26:38.923+0800 I  STORAGE  [initandlisten] WiredTiger message [1569065198:923556][5752:140713979633712], txn-recover: Set global recovery timestamp: (1569065173,1)
2019-09-21T19:26:38.968+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569065173, 1)
2019-09-21T19:26:38.979+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T19:26:38.979+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 98 records totaling to 12309 bytes
2019-09-21T19:26:38.979+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T19:26:38.988+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T19:26:38.997+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:26:38.997+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T19:26:38.997+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T19:26:38.998+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T19:26:38.998+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T19:26:38.998+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T19:26:38.998+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T19:26:38.998+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:26:39.031+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T19:26:39.032+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T19:26:39.032+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T19:26:39.032+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T19:26:39.033+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T19:26:39.169+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T19:26:39.172+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T19:26:39.172+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T19:26:39.173+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T19:26:39.173+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569065173, 1) (top of oplog: { ts: Timestamp(1569065173, 1), t: 4 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T19:26:39.173+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569065173, 1)
2019-09-21T19:26:39.173+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T19:26:39.173+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T19:26:39.174+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T19:26:39.175+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:26:39.175+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T19:26:39.175+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T19:26:39.175+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T19:26:39.176+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:26:39.211+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:26:39.211+0800 I  REPL     [replexec-0] This node is localhost:37017 in the config
2019-09-21T19:26:39.211+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T19:26:39.212+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T19:26:39.212+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T19:26:39.214+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T19:26:39.214+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T19:26:39.214+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T19:26:39.214+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T19:26:39.214+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T19:26:39.214+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2019-09-21T19:26:39.214+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T19:26:39.214+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T19:26:39.226+0800 I  REPL     [replexec-1] Member localhost:37018 is now in state SECONDARY
2019-09-21T19:26:39.331+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61373 #3 (1 connection now open)
2019-09-21T19:26:39.331+0800 I  SHARDING [conn3] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T19:26:39.332+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61373 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:26:39.343+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 127.0.0.1:61373
2019-09-21T19:26:39.371+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T19:26:44.154+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 5, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569065173, 1), t: 4 } }
2019-09-21T19:26:44.154+0800 I  ELECTION [conn3] Sending vote response: { term: 5, voteGranted: true, reason: "" }
2019-09-21T19:26:44.160+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 6, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569065173, 1), t: 4 } }
2019-09-21T19:26:44.160+0800 I  ELECTION [conn3] Sending vote response: { term: 6, voteGranted: true, reason: "" }
2019-09-21T19:26:44.230+0800 I  REPL     [replexec-0] Member localhost:37018 is now in state PRIMARY
2019-09-21T19:26:46.170+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61376 #4 (2 connections now open)
2019-09-21T19:26:46.171+0800 I  NETWORK  [conn4] received client metadata from 127.0.0.1:61376 conn4: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:26:46.174+0800 I  ACCESS   [conn4] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61376
2019-09-21T19:26:47.220+0800 I  REPL     [rsBackgroundSync] sync source candidate: localhost:37018
2019-09-21T19:26:47.221+0800 I  CONNPOOL [RS] Connecting to localhost:37018
2019-09-21T19:26:47.234+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:37018
2019-09-21T19:26:47.236+0800 I  SHARDING [rsSync-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T19:27:23.843+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61379 #7 (3 connections now open)
2019-09-21T19:27:23.843+0800 I  NETWORK  [conn7] received client metadata from 127.0.0.1:61379 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:27:23.854+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 127.0.0.1:61379
2019-09-21T19:27:44.867+0800 I  SHARDING [repl-writer-worker-15] Marking collection test.c as collection version: <unsharded>
2019-09-21T19:31:23.856+0800 I  NETWORK  [conn7] end connection 127.0.0.1:61379 (2 connections now open)
2019-09-21T19:31:39.175+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:31:39.176+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to localhost:37018
2019-09-21T19:31:39.176+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to localhost:37017
2019-09-21T19:31:39.177+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61410 #9 (3 connections now open)
2019-09-21T19:31:39.178+0800 I  NETWORK  [conn9] received client metadata from 127.0.0.1:61410 conn9: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:31:39.199+0800 I  ACCESS   [conn9] Successfully authenticated as principal __system on local from client 127.0.0.1:61410
2019-09-21T19:31:39.199+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:31:39.201+0800 I  NETWORK  [LogicalSessionCacheReap] Successfully connected to localhost:37018 (1 connections now open to localhost:37018 with a 0 second timeout)
2019-09-21T19:31:39.210+0800 I  NETWORK  [LogicalSessionCacheRefresh] Successfully connected to localhost:37018 (2 connections now open to localhost:37018 with a 0 second timeout)
2019-09-21T19:31:39.250+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:31:39.251+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:31:39.298+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:31:39.299+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:31:39.331+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:31:39.332+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:36:39.176+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:36:39.176+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:36:39.211+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:36:39.212+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:36:39.243+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:36:39.244+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:36:39.276+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:36:39.277+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:41:39.176+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:41:39.177+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:41:39.212+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:41:39.212+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:41:39.244+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:41:39.244+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:41:39.276+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:41:39.277+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:46:39.176+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:46:39.177+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:46:39.243+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:46:39.244+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:46:39.275+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:46:39.276+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:51:39.176+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:51:39.176+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:51:39.244+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:51:39.244+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:51:39.278+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:51:39.278+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:56:39.176+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:56:39.176+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:56:39.243+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:56:39.243+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T19:56:39.275+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T19:56:39.276+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:01:39.176+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:01:39.177+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:01:39.243+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:01:39.244+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:01:39.276+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:01:39.276+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:06:39.176+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:06:39.177+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:06:39.243+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:06:39.244+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:06:39.276+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:06:39.276+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:11:39.177+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:11:39.177+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:11:39.245+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:11:39.246+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:11:39.279+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:11:39.279+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:13:36.012+0800 I  NETWORK  [conn4] end connection 127.0.0.1:61376 (2 connections now open)
2019-09-21T20:13:45.448+0800 I  CONTROL  [thread5] Ctrl-C signal
2019-09-21T20:13:45.448+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:13:45.448+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:13:45.448+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:13:45.449+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:13:45.449+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:13:45.449+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:37018: CallbackCanceled: Reporter no longer valid
2019-09-21T20:13:45.449+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:13:45.449+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:13:45.449+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2019-09-21T20:13:45.449+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:13:45.449+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:13:45.449+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:13:45.450+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:13:45.450+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:13:45.450+0800 I  CONNPOOL [RS] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:13:45.451+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:13:45.451+0800 I  ASIO     [ReplicaSetMonitor-TaskExecutor] Killing all outstanding egress activity.
2019-09-21T20:13:45.451+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Dropping all pooled connections to localhost:37017 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:13:45.451+0800 I  NETWORK  [conn9] end connection 127.0.0.1:61410 (1 connection now open)
2019-09-21T20:13:45.452+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:13:45.452+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:13:45.455+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:13:45.455+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:13:45.456+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:13:45.456+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:13:45.461+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:13:45.461+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:13:45.461+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:13:45.510+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:13:45.510+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:13:45.510+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:13:45.562+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:13:45.562+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:13:45.562+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:15:10.243+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:15:10.700+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=6888 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:15:10.704+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { port: 37017 }, replication: { replSetName: "rs0" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T20:15:10.706+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:15:10.769+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068110:769200][6888:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:15:10.804+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:15:10.835+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T20:15:10.856+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:15:10.860+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:15:10.860+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:15:10.860+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:15:10.860+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:15:10.860+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 06e4da57-f426-4f65-8428-9ff8a51b2f87 and options: { capped: true, size: 10485760 }
2019-09-21T20:15:10.886+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2019-09-21T20:15:10.886+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:15:11.019+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T20:15:11.020+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 342d85f2-bfe8-4026-ac93-47193b50a13e and options: {}
2019-09-21T20:15:11.042+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2019-09-21T20:15:11.043+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: edeb3cb0-a0eb-4819-a934-7f768086a48f and options: {}
2019-09-21T20:15:11.065+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2019-09-21T20:15:11.065+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:15:11.066+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 3fa0ac43-2297-45bf-a0b3-60b1f582d855 and options: {}
2019-09-21T20:15:11.089+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2019-09-21T20:15:11.089+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:15:11.090+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:15:11.090+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2019-09-21T20:15:11.090+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: f2b7d047-9aff-41e3-8fa5-c9fa713937be and options: {}
2019-09-21T20:15:11.114+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2019-09-21T20:15:11.114+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:15:11.114+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2019-09-21T20:15:11.114+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:15:11.116+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:15:11.116+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:15:11.116+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T20:15:11.116+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:15:11.116+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T20:15:12.002+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:15:27.145+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61537 #1 (1 connection now open)
2019-09-21T20:15:27.147+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61537 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:16:11.376+0800 I  CONTROL  [thread2] Ctrl-C signal
2019-09-21T20:16:11.376+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:16:11.376+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:16:11.376+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:16:11.377+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:16:11.377+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:16:11.377+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:16:11.377+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:16:11.379+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:16:11.379+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:16:11.380+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:16:11.380+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:16:11.380+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:16:11.380+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:16:11.457+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:16:11.458+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:16:11.458+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:16:11.458+0800 I  STORAGE  [consoleTerminate] Downgrading WiredTiger datafiles.
2019-09-21T20:16:11.533+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068171:533230][6888:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T20:16:11.627+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068171:627275][6888:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T20:16:11.718+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068171:718359][6888:140713979633712], txn-recover: Main recovery loop: starting at 1/23168 to 2/256
2019-09-21T20:16:11.881+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068171:881495][6888:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T20:16:11.981+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068171:981591][6888:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T20:16:12.066+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068172:65693][6888:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:16:12.160+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:16:12.160+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:16:12.160+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:16:32.573+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:16:32.574+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=4528 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:16:33.067+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:16:33.068+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T20:16:33.069+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb1 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T20:16:33.069+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:16:33.105+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068193:105349][4528:140713979633712], txn-recover: Recovering log 2 through 3
2019-09-21T20:16:33.189+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068193:188451][4528:140713979633712], txn-recover: Recovering log 3 through 3
2019-09-21T20:16:33.282+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068193:281540][4528:140713979633712], txn-recover: Main recovery loop: starting at 2/2048 to 3/256
2019-09-21T20:16:33.444+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068193:443692][4528:140713979633712], txn-recover: Recovering log 2 through 3
2019-09-21T20:16:33.540+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068193:539763][4528:140713979633712], txn-recover: Recovering log 3 through 3
2019-09-21T20:16:33.628+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068193:627852][4528:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:16:33.674+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:16:33.683+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:16:33.693+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:16:33.693+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T20:16:33.693+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T20:16:33.693+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T20:16:33.693+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T20:16:33.693+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T20:16:33.693+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T20:16:33.693+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:16:33.710+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:16:33.711+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:16:33.711+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:16:33.711+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:16:33.712+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:16:33.845+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T20:16:33.847+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:16:33.847+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:16:33.847+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:16:33.847+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T20:16:33.848+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:16:33.849+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:16:33.849+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:16:33.849+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T20:16:33.850+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:16:33.850+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T20:16:34.002+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:16:36.860+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61543 #1 (1 connection now open)
2019-09-21T20:16:36.861+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:16:36.861+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T20:16:36.861+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61543 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:16:55.288+0800 I  COMMAND  [conn1] initiate : no configuration specified. Using a default configuration for the set
2019-09-21T20:16:55.288+0800 I  COMMAND  [conn1] created this configuration for initiation : { _id: "rs0", version: 1, members: [ { _id: 0, host: "localhost:37017" } ] }
2019-09-21T20:16:55.288+0800 I  REPL     [conn1] replSetInitiate admin command received from client
2019-09-21T20:16:55.298+0800 I  REPL     [conn1] replSetInitiate config object with 1 members parses ok
2019-09-21T20:16:55.298+0800 I  REPL     [conn1] ******
2019-09-21T20:16:55.298+0800 I  REPL     [conn1] creating replication oplog of size: 990MB...
2019-09-21T20:16:55.298+0800 I  STORAGE  [conn1] createCollection: local.oplog.rs with generated UUID: 1e412f4b-b052-44aa-b6a5-2c24df088df7 and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T20:16:55.312+0800 I  STORAGE  [conn1] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:16:55.312+0800 I  STORAGE  [conn1] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T20:16:55.312+0800 I  STORAGE  [conn1] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:16:55.359+0800 I  REPL     [conn1] ******
2019-09-21T20:16:55.359+0800 I  STORAGE  [conn1] createCollection: local.system.replset with generated UUID: f4a0a88f-eb77-4cfb-af7e-ca838e3e4f64 and options: {}
2019-09-21T20:16:55.385+0800 I  INDEX    [conn1] index build: done building index _id_ on ns local.system.replset
2019-09-21T20:16:55.392+0800 I  SHARDING [conn1] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:16:55.392+0800 I  SHARDING [conn1] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:16:55.392+0800 I  STORAGE  [conn1] createCollection: admin.system.version with provided UUID: d0fb4bcc-6cbd-43a2-8f0e-715768ad09ca and options: { uuid: UUID("d0fb4bcc-6cbd-43a2-8f0e-715768ad09ca") }
2019-09-21T20:16:55.415+0800 I  INDEX    [conn1] index build: done building index _id_ on ns admin.system.version
2019-09-21T20:16:55.415+0800 I  COMMAND  [conn1] setting featureCompatibilityVersion to 4.2
2019-09-21T20:16:55.415+0800 I  NETWORK  [conn1] Skip closing connection for connection # 1
2019-09-21T20:16:55.415+0800 I  REPL     [conn1] New replica set config in use: { _id: "rs0", version: 1, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d8614b7a12331c3a99b2c9e') } }
2019-09-21T20:16:55.416+0800 I  REPL     [conn1] This node is localhost:37017 in the config
2019-09-21T20:16:55.416+0800 I  REPL     [conn1] transition to STARTUP2 from STARTUP
2019-09-21T20:16:55.416+0800 I  REPL     [conn1] Starting replication storage threads
2019-09-21T20:16:55.420+0800 I  REPL     [conn1] transition to RECOVERING from STARTUP2
2019-09-21T20:16:55.420+0800 I  REPL     [conn1] Starting replication fetcher thread
2019-09-21T20:16:55.421+0800 I  REPL     [conn1] Starting replication applier thread
2019-09-21T20:16:55.421+0800 I  REPL     [conn1] Starting replication reporter thread
2019-09-21T20:16:55.421+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:16:55.421+0800 I  COMMAND  [conn1] command local.system.replset appName: "MongoDB Shell" command: replSetInitiate { replSetInitiate: undefined, lsid: { id: UUID("caf4508e-577a-4b23-9c15-246980468092") }, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $db: "admin" } numYields:0 reslen:143 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 19 } }, ReplicationStateTransition: { acquireCount: { w: 23 } }, Global: { acquireCount: { r: 8, w: 13, W: 2 }, acquireWaitCount: { W: 1 }, timeAcquiringMicros: { W: 30 } }, Database: { acquireCount: { r: 6, w: 2, W: 11 } }, Collection: { acquireCount: { r: 4, w: 2 } }, Mutex: { acquireCount: { r: 16 } }, oplog: { acquireCount: { r: 1, w: 1 } } } flowControl:{ acquireCount: 4 } storage:{} protocol:op_msg 132ms
2019-09-21T20:16:55.421+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:16:55.421+0800 I  ELECTION [rsSync-0] conducting a dry run election to see if we could be elected. current term: 0
2019-09-21T20:16:55.421+0800 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 1
2019-09-21T20:16:55.427+0800 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 1
2019-09-21T20:16:55.427+0800 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
2019-09-21T20:16:55.427+0800 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
2019-09-21T20:16:55.427+0800 I  REPL     [replexec-0] Entering primary catch-up mode.
2019-09-21T20:16:55.427+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2019-09-21T20:16:55.427+0800 I  REPL     [replexec-0] Stopping replication producer
2019-09-21T20:16:56.421+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:16:56.421+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:16:57.421+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:16:57.421+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:16:57.421+0800 I  SHARDING [rsSync-0] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:16:57.421+0800 I  STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: 529f7366-1b69-48e5-97ca-8fdd8a540ebc and options: {}
2019-09-21T20:16:57.447+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.transactions
2019-09-21T20:16:57.448+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T20:16:57.448+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:16:57.448+0800 I  STORAGE  [monitoring-keys-for-HMAC] createCollection: admin.system.keys with generated UUID: 205d36c8-6009-49e3-a1e1-0e163c157633 and options: {}
2019-09-21T20:16:57.473+0800 I  INDEX    [monitoring-keys-for-HMAC] index build: done building index _id_ on ns admin.system.keys
2019-09-21T20:16:57.478+0800 I  STORAGE  [monitoring-keys-for-HMAC] Triggering the first stable checkpoint. Initial Data: Timestamp(1569068215, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569068217, 4)
2019-09-21T20:17:06.025+0800 I  ACCESS   [conn1] Unauthorized: not authorized on test to execute command { listCollections: 1.0, filter: {}, nameOnly: true, authorizedCollections: true, lsid: { id: UUID("caf4508e-577a-4b23-9c15-246980468092") }, $clusterTime: { clusterTime: Timestamp(1569068217, 5), signature: { hash: BinData(0, 1274CB68B2929D1321C2FC0E5F1925A828C03954), keyId: 6739096677208031234 } }, $db: "test" }
2019-09-21T20:17:32.319+0800 I  STORAGE  [conn1] createCollection: admin.system.users with generated UUID: f7606230-2246-40c4-8242-8bd0c1aabfd2 and options: {}
2019-09-21T20:17:32.346+0800 I  INDEX    [conn1] index build: done building index _id_ on ns admin.system.users
2019-09-21T20:17:32.359+0800 I  INDEX    [conn1] index build: done building index user_1_db_1 on ns admin.system.users
2019-09-21T20:18:10.334+0800 I  ACCESS   [conn1] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61543
2019-09-21T20:18:21.136+0800 I  COMMAND  [conn1] initiate : no configuration specified. Using a default configuration for the set
2019-09-21T20:18:21.136+0800 I  COMMAND  [conn1] created this configuration for initiation : { _id: "rs0", version: 1, members: [ { _id: 0, host: "localhost:37017" } ] }
2019-09-21T20:18:21.136+0800 I  REPL     [conn1] replSetInitiate admin command received from client
2019-09-21T20:18:31.882+0800 I  SHARDING [conn1] Marking collection test.c as collection version: <unsharded>
2019-09-21T20:18:38.436+0800 I  STORAGE  [conn1] createCollection: test.c with generated UUID: 53faa251-cc6c-4675-a3cb-a9893b5a4790 and options: {}
2019-09-21T20:18:38.463+0800 I  INDEX    [conn1] index build: done building index _id_ on ns test.c
2019-09-21T20:19:09.723+0800 I  REPL     [conn1] replSetReconfig admin command received from client; new config: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1.0, host: "localhost:37018" } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d8614b7a12331c3a99b2c9e') } }
2019-09-21T20:19:09.762+0800 I  REPL     [conn1] replSetReconfig config object with 2 members parses ok
2019-09-21T20:19:09.762+0800 I  REPL     [conn1] Scheduling remote command request for reconfig quorum check: RemoteCommand 1 -- target:localhost:37018 db:admin cmd:{ replSetHeartbeat: "rs0", configVersion: 2, hbv: 1, from: "localhost:37017", fromId: 0, term: 1 }
2019-09-21T20:19:09.762+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T20:19:09.796+0800 I  REPL     [conn1] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d8614b7a12331c3a99b2c9e') } }
2019-09-21T20:19:09.796+0800 I  REPL     [conn1] This node is localhost:37017 in the config
2019-09-21T20:19:09.796+0800 I  REPL     [replexec-1] Member localhost:37018 is now in state STARTUP
2019-09-21T20:19:09.799+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61551 #4 (2 connections now open)
2019-09-21T20:19:09.799+0800 I  NETWORK  [conn4] received client metadata from 127.0.0.1:61551 conn4: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:09.814+0800 I  ACCESS   [conn4] Successfully authenticated as principal __system on local from client 127.0.0.1:61551
2019-09-21T20:19:09.816+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61552 #5 (3 connections now open)
2019-09-21T20:19:09.849+0800 I  ACCESS   [conn5] Successfully authenticated as principal __system on local from client 127.0.0.1:61552
2019-09-21T20:19:09.849+0800 I  NETWORK  [conn5] end connection 127.0.0.1:61552 (2 connections now open)
2019-09-21T20:19:10.019+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61553 #6 (3 connections now open)
2019-09-21T20:19:10.020+0800 I  NETWORK  [conn6] received client metadata from 127.0.0.1:61553 conn6: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:10.033+0800 I  ACCESS   [conn6] Successfully authenticated as principal __system on local from client 127.0.0.1:61553
2019-09-21T20:19:10.035+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61554 #7 (4 connections now open)
2019-09-21T20:19:10.036+0800 I  NETWORK  [conn7] received client metadata from 127.0.0.1:61554 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:10.047+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 127.0.0.1:61554
2019-09-21T20:19:10.079+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61555 #8 (5 connections now open)
2019-09-21T20:19:10.079+0800 I  NETWORK  [conn8] received client metadata from 127.0.0.1:61555 conn8: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:10.112+0800 I  ACCESS   [conn8] Successfully authenticated as principal __system on local from client 127.0.0.1:61555
2019-09-21T20:19:10.112+0800 I  NETWORK  [conn8] end connection 127.0.0.1:61555 (4 connections now open)
2019-09-21T20:19:10.149+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61556 #9 (5 connections now open)
2019-09-21T20:19:10.150+0800 I  NETWORK  [conn9] received client metadata from 127.0.0.1:61556 conn9: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:10.182+0800 I  ACCESS   [conn9] Successfully authenticated as principal __system on local from client 127.0.0.1:61556
2019-09-21T20:19:10.183+0800 I  NETWORK  [conn9] end connection 127.0.0.1:61556 (4 connections now open)
2019-09-21T20:19:10.282+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61557 #10 (5 connections now open)
2019-09-21T20:19:10.283+0800 I  NETWORK  [conn10] received client metadata from 127.0.0.1:61557 conn10: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:10.315+0800 I  ACCESS   [conn10] Successfully authenticated as principal __system on local from client 127.0.0.1:61557
2019-09-21T20:19:10.316+0800 I  NETWORK  [conn10] end connection 127.0.0.1:61557 (4 connections now open)
2019-09-21T20:19:10.408+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61558 #11 (5 connections now open)
2019-09-21T20:19:10.408+0800 I  NETWORK  [conn11] received client metadata from 127.0.0.1:61558 conn11: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:10.441+0800 I  ACCESS   [conn11] Successfully authenticated as principal __system on local from client 127.0.0.1:61558
2019-09-21T20:19:10.442+0800 I  NETWORK  [conn11] end connection 127.0.0.1:61558 (4 connections now open)
2019-09-21T20:19:10.513+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61559 #12 (5 connections now open)
2019-09-21T20:19:10.514+0800 I  NETWORK  [conn12] received client metadata from 127.0.0.1:61559 conn12: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:10.546+0800 I  ACCESS   [conn12] Successfully authenticated as principal __system on local from client 127.0.0.1:61559
2019-09-21T20:19:10.547+0800 I  NETWORK  [conn12] end connection 127.0.0.1:61559 (4 connections now open)
2019-09-21T20:19:11.797+0800 I  REPL     [replexec-1] Member localhost:37018 is now in state SECONDARY
2019-09-21T20:19:20.037+0800 I  NETWORK  [conn6] end connection 127.0.0.1:61553 (3 connections now open)
2019-09-21T20:19:27.600+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61560 #13 (4 connections now open)
2019-09-21T20:19:27.601+0800 I  NETWORK  [conn13] received client metadata from 127.0.0.1:61560 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:27.613+0800 I  ACCESS   [conn13] Successfully authenticated as principal __system on local from client 127.0.0.1:61560
2019-09-21T20:21:31.329+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61567 #14 (5 connections now open)
2019-09-21T20:21:31.329+0800 I  NETWORK  [conn14] received client metadata from 127.0.0.1:61567 conn14: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:21:31.371+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61567
2019-09-21T20:21:31.372+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61568 #15 (6 connections now open)
2019-09-21T20:21:31.373+0800 I  NETWORK  [conn15] received client metadata from 127.0.0.1:61568 conn15: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:21:31.405+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61568
2019-09-21T20:21:31.405+0800 I  NETWORK  [conn15] end connection 127.0.0.1:61568 (5 connections now open)
2019-09-21T20:21:33.849+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:21:33.850+0800 I  STORAGE  [LogicalSessionCacheRefresh] createCollection: config.system.sessions with provided UUID: 3c3606ca-79f0-4c36-9929-e90585780fd5 and options: { uuid: UUID("3c3606ca-79f0-4c36-9929-e90585780fd5") }
2019-09-21T20:21:33.874+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: done building index _id_ on ns config.system.sessions
2019-09-21T20:21:33.910+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: starting on config.system.sessions properties: { v: 2, key: { lastUse: 1 }, name: "lsidTTLIndex", ns: "config.system.sessions", expireAfterSeconds: 1800 } using method: Hybrid
2019-09-21T20:21:33.910+0800 I  INDEX    [LogicalSessionCacheRefresh] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:21:33.910+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: collection scan done. scanned 0 total records in 0 seconds
2019-09-21T20:21:33.910+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:21:33.921+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: done building index lsidTTLIndex on ns config.system.sessions
2019-09-21T20:22:39.743+0800 I  CONTROL  [thread14] Ctrl-C signal
2019-09-21T20:22:39.743+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:22:39.743+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:22:39.743+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:22:39.743+0800 I  REPL     [consoleTerminate] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 1 }
2019-09-21T20:22:39.743+0800 I  REPL     [consoleTerminate] transition to SECONDARY from PRIMARY
2019-09-21T20:22:39.744+0800 I  REPL     [consoleTerminate] Handing off election to localhost:37018
2019-09-21T20:22:39.744+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:22:39.744+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:22:39.744+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:22:39.744+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:22:39.744+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:22:39.744+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:22:39.745+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:22:40.555+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:22:40.555+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:22:40.555+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:22:40.555+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:22:40.556+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:22:40.556+0800 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:22:40.557+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:22:40.557+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:22:40.559+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:22:40.559+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:22:40.559+0800 E  QUERY    [conn13] GetMore command executor error: FAILURE, stats: { stage: "COLLSCAN", nReturned: 24, executionTimeMillisEstimate: 0, works: 514, advanced: 24, needTime: 245, needYield: 0, saveState: 245, restoreState: 244, isEOF: 0, direction: "forward", docsExamined: 24 }
2019-09-21T20:22:40.559+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:22:40.560+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:22:40.566+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:22:40.567+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:22:40.567+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:22:40.621+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:22:40.621+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:22:40.622+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:22:40.673+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:22:40.674+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:22:40.674+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:23:10.868+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:23:10.869+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:23:11.366+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=6248 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:23:11.367+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T20:23:11.368+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:23:11.428+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068591:427780][6248:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:23:11.453+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:23:11.485+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:23:11.503+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:23:11.503+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T20:23:11.503+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T20:23:11.504+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T20:23:11.504+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T20:23:11.504+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T20:23:11.504+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T20:23:11.504+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:23:11.507+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:23:11.507+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:23:11.507+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:23:11.507+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:23:11.508+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: c182095c-3a42-44a0-84da-48b03b402f28 and options: { capped: true, size: 10485760 }
2019-09-21T20:23:11.531+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2019-09-21T20:23:11.532+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:23:11.662+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T20:23:11.663+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: a30bf114-1919-4462-96db-d7f54f336cc0 and options: {}
2019-09-21T20:23:11.687+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2019-09-21T20:23:11.687+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 376b475c-d4d8-4d3a-b9a7-c5895c41e6a5 and options: {}
2019-09-21T20:23:11.711+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2019-09-21T20:23:11.711+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:23:11.711+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 27710629-cc23-45e9-84d9-e2808a44209b and options: {}
2019-09-21T20:23:11.735+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2019-09-21T20:23:11.736+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:23:11.736+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:23:11.736+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2019-09-21T20:23:11.736+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 064351f6-c300-4dc7-8965-fc93589875cc and options: {}
2019-09-21T20:23:11.759+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2019-09-21T20:23:11.759+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:23:11.759+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2019-09-21T20:23:11.759+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:23:11.761+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:23:11.761+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:23:11.761+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T20:23:11.761+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:23:11.761+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T20:23:12.001+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:23:20.396+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61576 #1 (1 connection now open)
2019-09-21T20:23:20.396+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:23:20.397+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T20:23:20.397+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61576 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:23:20.398+0800 I  ACCESS   [conn1] SASL SCRAM-SHA-256 authentication failed for user1 on admin from client 127.0.0.1:61576 ; UserNotFound: Could not find user "user1" for db "admin"
2019-09-21T20:23:32.409+0800 I  NETWORK  [conn1] end connection 127.0.0.1:61576 (0 connections now open)
2019-09-21T20:23:43.243+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61578 #2 (1 connection now open)
2019-09-21T20:23:43.245+0800 I  NETWORK  [conn2] received client metadata from 127.0.0.1:61578 conn2: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:24:28.231+0800 I  COMMAND  [conn2] initiate : no configuration specified. Using a default configuration for the set
2019-09-21T20:24:28.231+0800 I  COMMAND  [conn2] created this configuration for initiation : { _id: "rs0", version: 1, members: [ { _id: 0, host: "localhost:37017" } ] }
2019-09-21T20:24:28.231+0800 I  REPL     [conn2] replSetInitiate admin command received from client
2019-09-21T20:24:28.239+0800 I  REPL     [conn2] replSetInitiate config object with 1 members parses ok
2019-09-21T20:24:28.240+0800 I  REPL     [conn2] ******
2019-09-21T20:24:28.240+0800 I  REPL     [conn2] creating replication oplog of size: 990MB...
2019-09-21T20:24:28.240+0800 I  STORAGE  [conn2] createCollection: local.oplog.rs with generated UUID: 4a3ac10e-8d58-4350-b92e-8f4b92ec67d8 and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T20:24:28.252+0800 I  STORAGE  [conn2] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:24:28.252+0800 I  STORAGE  [conn2] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T20:24:28.252+0800 I  STORAGE  [conn2] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:24:28.294+0800 I  REPL     [conn2] ******
2019-09-21T20:24:28.295+0800 I  STORAGE  [conn2] createCollection: local.system.replset with generated UUID: dd94f155-60d4-431f-aee9-bc16cefc7e7c and options: {}
2019-09-21T20:24:28.320+0800 I  INDEX    [conn2] index build: done building index _id_ on ns local.system.replset
2019-09-21T20:24:28.327+0800 I  SHARDING [conn2] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:24:28.327+0800 I  STORAGE  [conn2] createCollection: admin.system.version with provided UUID: 1629d018-06b6-4f39-85b0-28dd32db4f49 and options: { uuid: UUID("1629d018-06b6-4f39-85b0-28dd32db4f49") }
2019-09-21T20:24:28.356+0800 I  INDEX    [conn2] index build: done building index _id_ on ns admin.system.version
2019-09-21T20:24:28.356+0800 I  COMMAND  [conn2] setting featureCompatibilityVersion to 4.2
2019-09-21T20:24:28.356+0800 I  NETWORK  [conn2] Skip closing connection for connection # 2
2019-09-21T20:24:28.356+0800 I  REPL     [conn2] New replica set config in use: { _id: "rs0", version: 1, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d86167c4c70307682f0187c') } }
2019-09-21T20:24:28.356+0800 I  REPL     [conn2] This node is localhost:37017 in the config
2019-09-21T20:24:28.356+0800 I  REPL     [conn2] transition to STARTUP2 from STARTUP
2019-09-21T20:24:28.357+0800 I  REPL     [conn2] Starting replication storage threads
2019-09-21T20:24:28.361+0800 I  REPL     [conn2] transition to RECOVERING from STARTUP2
2019-09-21T20:24:28.361+0800 I  REPL     [conn2] Starting replication fetcher thread
2019-09-21T20:24:28.362+0800 I  REPL     [conn2] Starting replication applier thread
2019-09-21T20:24:28.362+0800 I  REPL     [conn2] Starting replication reporter thread
2019-09-21T20:24:28.362+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:24:28.362+0800 I  COMMAND  [conn2] command local.system.replset appName: "MongoDB Shell" command: replSetInitiate { replSetInitiate: undefined, lsid: { id: UUID("80c2c0a1-1831-4d1f-bb4d-8a3d33eec976") }, $db: "admin" } numYields:0 reslen:143 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 19 } }, ReplicationStateTransition: { acquireCount: { w: 23 } }, Global: { acquireCount: { r: 8, w: 13, W: 2 }, acquireWaitCount: { W: 1 }, timeAcquiringMicros: { W: 28 } }, Database: { acquireCount: { r: 6, w: 2, W: 11 } }, Collection: { acquireCount: { r: 4, w: 2 } }, Mutex: { acquireCount: { r: 16 } }, oplog: { acquireCount: { r: 1, w: 1 } } } flowControl:{ acquireCount: 4 } storage:{} protocol:op_msg 130ms
2019-09-21T20:24:28.362+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:24:28.362+0800 I  ELECTION [rsSync-0] conducting a dry run election to see if we could be elected. current term: 0
2019-09-21T20:24:28.362+0800 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 1
2019-09-21T20:24:28.367+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 1
2019-09-21T20:24:28.367+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2019-09-21T20:24:28.367+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
2019-09-21T20:24:28.367+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2019-09-21T20:24:28.367+0800 I  REPL     [replexec-1] Exited primary catch-up mode.
2019-09-21T20:24:28.367+0800 I  REPL     [replexec-1] Stopping replication producer
2019-09-21T20:24:29.362+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:24:29.362+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:24:30.362+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:24:30.362+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:24:30.362+0800 I  SHARDING [rsSync-0] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:24:30.362+0800 I  STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: 23567881-1715-416d-9a57-965da591b80b and options: {}
2019-09-21T20:24:30.387+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.transactions
2019-09-21T20:24:30.388+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T20:24:30.388+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:24:30.389+0800 I  STORAGE  [monitoring-keys-for-HMAC] createCollection: admin.system.keys with generated UUID: 2a84f089-92dc-46d5-b77e-6e9839988a81 and options: {}
2019-09-21T20:24:30.410+0800 I  STORAGE  [WTJournalFlusher] Triggering the first stable checkpoint. Initial Data: Timestamp(1569068668, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569068670, 2)
2019-09-21T20:24:30.415+0800 I  INDEX    [monitoring-keys-for-HMAC] index build: done building index _id_ on ns admin.system.keys
2019-09-21T20:24:45.928+0800 I  ACCESS   [conn2] Unauthorized: not authorized on test to execute command { listCollections: 1.0, filter: {}, nameOnly: true, authorizedCollections: true, lsid: { id: UUID("80c2c0a1-1831-4d1f-bb4d-8a3d33eec976") }, $clusterTime: { clusterTime: Timestamp(1569068670, 5), signature: { hash: BinData(0, BDCF7F179559F1388CF14A809BE2048E1FEF55D0), keyId: 6739098622828216322 } }, $db: "test" }
2019-09-21T20:25:16.842+0800 I  ACCESS   [conn2] Unauthorized: not authorized on local to execute command { count: "system.replset", query: {}, lsid: { id: UUID("80c2c0a1-1831-4d1f-bb4d-8a3d33eec976") }, $clusterTime: { clusterTime: Timestamp(1569068670, 5), signature: { hash: BinData(0, BDCF7F179559F1388CF14A809BE2048E1FEF55D0), keyId: 6739098622828216322 } }, $db: "local" }
2019-09-21T20:25:41.173+0800 I  STORAGE  [conn2] createCollection: admin.system.users with generated UUID: 55163fe6-d944-4b90-8f6f-c9e50c70cc72 and options: {}
2019-09-21T20:25:41.198+0800 I  INDEX    [conn2] index build: done building index _id_ on ns admin.system.users
2019-09-21T20:25:41.210+0800 I  INDEX    [conn2] index build: done building index user_1_db_1 on ns admin.system.users
2019-09-21T20:25:51.317+0800 I  ACCESS   [conn2] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61578
2019-09-21T20:26:04.462+0800 I  REPL     [conn2] replSetReconfig admin command received from client; new config: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1.0, host: "localhost:37018" } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d86167c4c70307682f0187c') } }
2019-09-21T20:26:04.497+0800 I  REPL     [conn2] replSetReconfig config object with 2 members parses ok
2019-09-21T20:26:04.497+0800 I  REPL     [conn2] Scheduling remote command request for reconfig quorum check: RemoteCommand 1 -- target:localhost:37018 db:admin cmd:{ replSetHeartbeat: "rs0", configVersion: 2, hbv: 1, from: "localhost:37017", fromId: 0, term: 1 }
2019-09-21T20:26:04.497+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T20:26:04.531+0800 I  REPL     [conn2] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d86167c4c70307682f0187c') } }
2019-09-21T20:26:04.532+0800 I  REPL     [conn2] This node is localhost:37017 in the config
2019-09-21T20:26:04.532+0800 I  REPL     [replexec-1] Member localhost:37018 is now in state STARTUP
2019-09-21T20:26:04.535+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61588 #5 (2 connections now open)
2019-09-21T20:26:04.535+0800 I  NETWORK  [conn5] received client metadata from 127.0.0.1:61588 conn5: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:04.548+0800 I  ACCESS   [conn5] Successfully authenticated as principal __system on local from client 127.0.0.1:61588
2019-09-21T20:26:04.549+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61589 #6 (3 connections now open)
2019-09-21T20:26:04.582+0800 I  ACCESS   [conn6] Successfully authenticated as principal __system on local from client 127.0.0.1:61589
2019-09-21T20:26:04.583+0800 I  NETWORK  [conn6] end connection 127.0.0.1:61589 (2 connections now open)
2019-09-21T20:26:04.754+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61590 #7 (3 connections now open)
2019-09-21T20:26:04.755+0800 I  NETWORK  [conn7] received client metadata from 127.0.0.1:61590 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:04.766+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 127.0.0.1:61590
2019-09-21T20:26:04.768+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61591 #8 (4 connections now open)
2019-09-21T20:26:04.769+0800 I  NETWORK  [conn8] received client metadata from 127.0.0.1:61591 conn8: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:04.780+0800 I  ACCESS   [conn8] Successfully authenticated as principal __system on local from client 127.0.0.1:61591
2019-09-21T20:26:04.805+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61592 #9 (5 connections now open)
2019-09-21T20:26:04.806+0800 I  NETWORK  [conn9] received client metadata from 127.0.0.1:61592 conn9: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:04.838+0800 I  ACCESS   [conn9] Successfully authenticated as principal __system on local from client 127.0.0.1:61592
2019-09-21T20:26:04.839+0800 I  NETWORK  [conn9] end connection 127.0.0.1:61592 (4 connections now open)
2019-09-21T20:26:04.900+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61593 #10 (5 connections now open)
2019-09-21T20:26:04.900+0800 I  NETWORK  [conn10] received client metadata from 127.0.0.1:61593 conn10: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:04.932+0800 I  ACCESS   [conn10] Successfully authenticated as principal __system on local from client 127.0.0.1:61593
2019-09-21T20:26:04.933+0800 I  NETWORK  [conn10] end connection 127.0.0.1:61593 (4 connections now open)
2019-09-21T20:26:05.037+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61594 #11 (5 connections now open)
2019-09-21T20:26:05.037+0800 I  NETWORK  [conn11] received client metadata from 127.0.0.1:61594 conn11: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:05.069+0800 I  ACCESS   [conn11] Successfully authenticated as principal __system on local from client 127.0.0.1:61594
2019-09-21T20:26:05.070+0800 I  NETWORK  [conn11] end connection 127.0.0.1:61594 (4 connections now open)
2019-09-21T20:26:05.154+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61595 #12 (5 connections now open)
2019-09-21T20:26:05.154+0800 I  NETWORK  [conn12] received client metadata from 127.0.0.1:61595 conn12: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:05.186+0800 I  ACCESS   [conn12] Successfully authenticated as principal __system on local from client 127.0.0.1:61595
2019-09-21T20:26:05.187+0800 I  NETWORK  [conn12] end connection 127.0.0.1:61595 (4 connections now open)
2019-09-21T20:26:06.533+0800 I  REPL     [replexec-1] Member localhost:37018 is now in state SECONDARY
2019-09-21T20:26:14.769+0800 I  NETWORK  [conn7] end connection 127.0.0.1:61590 (3 connections now open)
2019-09-21T20:26:21.251+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61600 #13 (4 connections now open)
2019-09-21T20:26:21.251+0800 I  NETWORK  [conn13] received client metadata from 127.0.0.1:61600 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:21.262+0800 I  ACCESS   [conn13] Successfully authenticated as principal __system on local from client 127.0.0.1:61600
2019-09-21T20:27:47.355+0800 I  SHARDING [conn2] Marking collection test.c as collection version: <unsharded>
2019-09-21T20:27:47.355+0800 I  STORAGE  [conn2] createCollection: test.c with generated UUID: 41b275b8-8ec1-4f94-8ca3-6d0e30b67687 and options: {}
2019-09-21T20:27:47.382+0800 I  INDEX    [conn2] index build: done building index _id_ on ns test.c
2019-09-21T20:28:11.761+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:28:11.762+0800 I  STORAGE  [LogicalSessionCacheRefresh] createCollection: config.system.sessions with provided UUID: 40e35112-7e85-4522-abe6-7df5722656aa and options: { uuid: UUID("40e35112-7e85-4522-abe6-7df5722656aa") }
2019-09-21T20:28:11.786+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: done building index _id_ on ns config.system.sessions
2019-09-21T20:28:11.823+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: starting on config.system.sessions properties: { v: 2, key: { lastUse: 1 }, name: "lsidTTLIndex", ns: "config.system.sessions", expireAfterSeconds: 1800 } using method: Hybrid
2019-09-21T20:28:11.823+0800 I  INDEX    [LogicalSessionCacheRefresh] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:28:11.823+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: collection scan done. scanned 0 total records in 0 seconds
2019-09-21T20:28:11.823+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:28:11.833+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: done building index lsidTTLIndex on ns config.system.sessions
2019-09-21T20:28:14.556+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61605 #14 (5 connections now open)
2019-09-21T20:28:14.557+0800 I  NETWORK  [conn14] received client metadata from 127.0.0.1:61605 conn14: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:28:14.612+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61605
2019-09-21T20:28:14.613+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61607 #15 (6 connections now open)
2019-09-21T20:28:14.613+0800 I  NETWORK  [conn15] received client metadata from 127.0.0.1:61607 conn15: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:28:14.613+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61608 #16 (7 connections now open)
2019-09-21T20:28:14.614+0800 I  NETWORK  [conn16] received client metadata from 127.0.0.1:61608 conn16: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:28:14.659+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:28:14.673+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61607
2019-09-21T20:28:14.698+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:28:14.742+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:28:14.775+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:33:14.589+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61607
2019-09-21T20:33:14.589+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:33:14.622+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61607
2019-09-21T20:33:14.654+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61607
2019-09-21T20:33:14.689+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61607
2019-09-21T20:36:11.832+0800 I  QUERY    [clientcursormon] Cursor id 5576477804491563056 timed out, idle since 2019-09-21T20:26:09.769+0800
2019-09-21T20:38:14.589+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:38:14.590+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61607
2019-09-21T20:38:14.621+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:38:14.653+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:38:14.685+0800 I  ACCESS   [conn16] Successfully authenticated as principal __system on local from client 127.0.0.1:61608
2019-09-21T20:39:36.512+0800 I  NETWORK  [conn13] end connection 127.0.0.1:61600 (6 connections now open)
2019-09-21T20:39:36.513+0800 I  NETWORK  [conn5] end connection 127.0.0.1:61588 (5 connections now open)
2019-09-21T20:39:36.513+0800 I  NETWORK  [conn14] end connection 127.0.0.1:61605 (4 connections now open)
2019-09-21T20:39:36.642+0800 I  NETWORK  [conn15] end connection 127.0.0.1:61607 (3 connections now open)
2019-09-21T20:39:36.642+0800 I  NETWORK  [conn16] end connection 127.0.0.1:61608 (2 connections now open)
2019-09-21T20:39:36.663+0800 W  NETWORK  [replexec-1] Failed to check socket connectivity: 操作成功完成。
2019-09-21T20:39:36.663+0800 I  CONNPOOL [replexec-1] dropping unhealthy pooled connection to localhost:37018
2019-09-21T20:39:36.663+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T20:39:37.666+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T20:39:38.598+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61756 #17 (3 connections now open)
2019-09-21T20:39:38.631+0800 I  ACCESS   [conn17] Successfully authenticated as principal __system on local from client 127.0.0.1:61756
2019-09-21T20:39:38.631+0800 I  NETWORK  [conn17] end connection 127.0.0.1:61756 (2 connections now open)
2019-09-21T20:39:38.635+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61757 #18 (3 connections now open)
2019-09-21T20:39:38.635+0800 I  NETWORK  [conn18] received client metadata from 127.0.0.1:61757 conn18: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:39:38.647+0800 I  ACCESS   [conn18] Successfully authenticated as principal __system on local from client 127.0.0.1:61757
2019-09-21T20:39:39.605+0800 I  CONTROL  [thread17] Ctrl-C signal
2019-09-21T20:39:39.605+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:39:39.605+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:39:39.605+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:39:39.605+0800 I  REPL     [consoleTerminate] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 1 }
2019-09-21T20:39:39.605+0800 I  REPL     [consoleTerminate] transition to SECONDARY from PRIMARY
2019-09-21T20:39:39.605+0800 I  REPL     [consoleTerminate] Handing off election to localhost:37018
2019-09-21T20:39:39.606+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:39:39.606+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:39:39.606+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:39:39.606+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:39:39.606+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:39:39.607+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:39:39.607+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:39:39.718+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:39:39.718+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:39:39.718+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:39:39.718+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:39:39.719+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:39:39.719+0800 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:39:39.719+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:39:39.719+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:39:39.723+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:39:39.723+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:39:39.723+0800 E  QUERY    [conn8] GetMore command executor error: FAILURE, stats: { stage: "COLLSCAN", nReturned: 86, executionTimeMillisEstimate: 0, works: 1936, advanced: 86, needTime: 925, needYield: 0, saveState: 925, restoreState: 924, isEOF: 0, direction: "forward", docsExamined: 86 }
2019-09-21T20:39:39.723+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:39:39.723+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:39:39.724+0800 I  NETWORK  [conn8] Error sending response to client: HostUnreachable: Connection reset by peer. Ending connection from 127.0.0.1:61591 (connection id: 8)
2019-09-21T20:39:39.724+0800 I  NETWORK  [conn8] end connection 127.0.0.1:61591 (2 connections now open)
2019-09-21T20:39:39.728+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:39:39.728+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:39:39.728+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:39:39.782+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:39:39.782+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:39:39.782+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:39:39.827+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:39:39.827+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:39:39.827+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:40:14.659+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:40:15.101+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=6496 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:40:15.147+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:40:15.148+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:40:15.148+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { bindIp: "0.0.0.0", port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T20:40:15.149+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb1 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T20:40:15.149+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:40:15.186+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069615:186111][6496:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T20:40:15.280+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069615:279223][6496:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T20:40:15.372+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069615:372290][6496:140713979633712], txn-recover: Main recovery loop: starting at 1/109312 to 2/256
2019-09-21T20:40:15.373+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069615:373296][6496:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T20:40:15.468+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069615:467401][6496:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T20:40:15.550+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069615:549460][6496:140713979633712], txn-recover: Set global recovery timestamp: (1569069570,1)
2019-09-21T20:40:15.629+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569069570, 1)
2019-09-21T20:40:15.639+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:40:15.639+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 103 records totaling to 13299 bytes
2019-09-21T20:40:15.639+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:40:15.647+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:40:15.690+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:40:15.691+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:40:15.691+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:40:15.691+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:40:15.692+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:40:15.824+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T20:40:15.825+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:40:15.825+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:40:15.826+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T20:40:15.826+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569069570, 1) (top of oplog: { ts: Timestamp(1569069570, 1), t: 1 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T20:40:15.826+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569069570, 1)
2019-09-21T20:40:15.826+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T20:40:15.826+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:40:15.827+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:40:15.828+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:40:15.828+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:40:15.828+0800 I  NETWORK  [initandlisten] Listening on 0.0.0.0
2019-09-21T20:40:15.828+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T20:40:15.828+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T20:40:15.865+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d86167c4c70307682f0187c') } }
2019-09-21T20:40:15.865+0800 I  REPL     [replexec-0] This node is localhost:37017 in the config
2019-09-21T20:40:15.865+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T20:40:15.866+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T20:40:15.866+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T20:40:15.867+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T20:40:15.867+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T20:40:15.868+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T20:40:15.868+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T20:40:15.868+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:40:15.868+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2019-09-21T20:40:15.868+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:40:15.868+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T20:40:15.900+0800 I  REPL     [replexec-1] Member localhost:37018 is now in state SECONDARY
2019-09-21T20:40:16.024+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:40:16.146+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61804 #3 (1 connection now open)
2019-09-21T20:40:16.147+0800 I  SHARDING [conn3] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:40:16.147+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61804 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:16.158+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 127.0.0.1:61804
2019-09-21T20:40:21.630+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069570, 1), t: 1 } }
2019-09-21T20:40:21.630+0800 I  ELECTION [conn3] Sending vote response: { term: 2, voteGranted: true, reason: "" }
2019-09-21T20:40:21.639+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 3, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069570, 1), t: 1 } }
2019-09-21T20:40:21.639+0800 I  ELECTION [conn3] Sending vote response: { term: 3, voteGranted: true, reason: "" }
2019-09-21T20:40:21.907+0800 I  REPL     [replexec-0] Member localhost:37018 is now in state PRIMARY
2019-09-21T20:40:23.873+0800 I  REPL     [rsBackgroundSync] sync source candidate: localhost:37018
2019-09-21T20:40:23.874+0800 I  CONNPOOL [RS] Connecting to localhost:37018
2019-09-21T20:40:23.908+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:37018
2019-09-21T20:40:23.909+0800 I  SHARDING [rsSync-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:40:31.118+0800 I  COMMAND  [conn3] Received replSetStepUp request
2019-09-21T20:40:31.118+0800 I  ELECTION [conn3] Starting an election due to step up request
2019-09-21T20:40:31.118+0800 I  ELECTION [conn3] skipping dry run and running for election in term 4
2019-09-21T20:40:31.124+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 31 -- target:localhost:37018 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 4, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069622, 1), t: 3 } }
2019-09-21T20:40:31.124+0800 I  ELECTION [replexec-1] VoteRequester(term 4) received an invalid response from localhost:37018: ShutdownInProgress: In the process of shutting down; response message: { operationTime: Timestamp(1569069622, 1), ok: 0.0, errmsg: "In the process of shutting down", code: 91, codeName: "ShutdownInProgress", $clusterTime: { clusterTime: Timestamp(1569069622, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
2019-09-21T20:40:31.124+0800 I  ELECTION [replexec-1] not becoming primary, we received insufficient votes
2019-09-21T20:40:31.124+0800 I  ELECTION [replexec-1] Lost election due to internal error
2019-09-21T20:40:31.124+0800 I  COMMAND  [conn3] replSetStepUp request failed :: caused by :: CommandFailed: Election failed.
2019-09-21T20:40:31.673+0800 I  NETWORK  [conn3] end connection 127.0.0.1:61804 (0 connections now open)
2019-09-21T20:40:31.676+0800 I  REPL     [replication-1] Restarting oplog query due to error: InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown. Last fetched optime: { ts: Timestamp(1569069622, 1), t: 3 }. Restarts remaining: 1
2019-09-21T20:40:31.676+0800 I  REPL     [replication-1] Scheduled new oplog query Fetcher source: localhost:37018 database: local query: { find: "oplog.rs", filter: { ts: { $gte: Timestamp(1569069622, 1) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 4, readConcern: { afterClusterTime: Timestamp(1569069622, 1) } } query metadata: { $replData: 1, $oplogQueryData: 1, $readPreference: { mode: "secondaryPreferred" } } active: 1 findNetworkTimeout: 7000ms getMoreNetworkTimeout: 10000ms shutting down?: 0 first: 1 firstCommandScheduler: RemoteCommandRetryScheduler request: RemoteCommand 32 -- target:localhost:37018 db:local cmd:{ find: "oplog.rs", filter: { ts: { $gte: Timestamp(1569069622, 1) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 4, readConcern: { afterClusterTime: Timestamp(1569069622, 1) } } active: 1 callbackHandle.valid: 1 callbackHandle.cancelled: 0 attempt: 1 retryPolicy: RetryPolicyImpl maxAttempts: 1 maxTimeMillis: -1ms
2019-09-21T20:40:31.677+0800 I  REPL     [replication-0] Error returned from oplog query (no more query restarts left): InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown
2019-09-21T20:40:31.677+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown
2019-09-21T20:40:31.677+0800 I  REPL     [rsBackgroundSync] Clearing sync source localhost:37018 to choose a new one.
2019-09-21T20:40:31.677+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T20:40:31.678+0800 I  REPL_HB  [replexec-0] Heartbeat to localhost:37018 failed after 2 retries, response status: InterruptedAtShutdown: interrupted at shutdown
2019-09-21T20:40:31.678+0800 I  REPL     [replexec-0] Member localhost:37018 is now in state RS_DOWN - interrupted at shutdown
2019-09-21T20:40:32.178+0800 W  NETWORK  [replexec-2] Failed to check socket connectivity: 操作成功完成。
2019-09-21T20:40:32.178+0800 I  CONNPOOL [replexec-2] dropping unhealthy pooled connection to localhost:37018
2019-09-21T20:40:32.178+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T20:40:33.179+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T20:40:33.831+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61817 #6 (1 connection now open)
2019-09-21T20:40:33.864+0800 I  ACCESS   [conn6] Successfully authenticated as principal __system on local from client 127.0.0.1:61817
2019-09-21T20:40:33.864+0800 I  NETWORK  [conn6] end connection 127.0.0.1:61817 (0 connections now open)
2019-09-21T20:40:33.868+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61818 #7 (1 connection now open)
2019-09-21T20:40:33.868+0800 I  NETWORK  [conn7] received client metadata from 127.0.0.1:61818 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:33.880+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 127.0.0.1:61818
2019-09-21T20:40:33.951+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:37018: InvalidSyncSource: Sync source was cleared. Was localhost:37018
2019-09-21T20:40:34.212+0800 I  REPL     [replexec-2] Member localhost:37018 is now in state SECONDARY
2019-09-21T20:40:41.343+0800 I  ELECTION [replexec-0] Starting an election, since we've seen no PRIMARY in the past 10000ms
2019-09-21T20:40:41.343+0800 I  ELECTION [replexec-0] conducting a dry run election to see if we could be elected. current term: 4
2019-09-21T20:40:41.343+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 52 -- target:localhost:37018 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 4, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069622, 1), t: 3 } }
2019-09-21T20:40:41.344+0800 I  ELECTION [replexec-2] VoteRequester(term 4 dry run) received a yes vote from localhost:37018; response message: { term: 4, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569069622, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569069622, 1) }
2019-09-21T20:40:41.344+0800 I  ELECTION [replexec-2] dry election run succeeded, running for election in term 5
2019-09-21T20:40:41.349+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 53 -- target:localhost:37018 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 5, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069622, 1), t: 3 } }
2019-09-21T20:40:41.355+0800 I  ELECTION [replexec-0] VoteRequester(term 5) received a yes vote from localhost:37018; response message: { term: 5, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569069622, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569069622, 1) }
2019-09-21T20:40:41.355+0800 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 5
2019-09-21T20:40:41.355+0800 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
2019-09-21T20:40:41.355+0800 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
2019-09-21T20:40:41.355+0800 I  REPL     [replexec-0] Entering primary catch-up mode.
2019-09-21T20:40:41.356+0800 I  REPL     [replexec-2] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1569069622, 1), t: 3 }. My Last Applied: { ts: Timestamp(1569069622, 1), t: 3 }
2019-09-21T20:40:41.356+0800 I  REPL     [replexec-2] Exited primary catch-up mode.
2019-09-21T20:40:41.356+0800 I  REPL     [replexec-2] Stopping replication producer
2019-09-21T20:40:41.917+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:40:41.917+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:40:42.917+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:40:42.917+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:40:42.918+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T20:40:43.877+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61819 #9 (2 connections now open)
2019-09-21T20:40:43.878+0800 I  NETWORK  [conn9] received client metadata from 127.0.0.1:61819 conn9: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:43.889+0800 I  ACCESS   [conn9] Successfully authenticated as principal __system on local from client 127.0.0.1:61819
2019-09-21T20:40:43.898+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61820 #10 (3 connections now open)
2019-09-21T20:40:43.898+0800 I  NETWORK  [conn10] received client metadata from 127.0.0.1:61820 conn10: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:43.909+0800 I  ACCESS   [conn10] Successfully authenticated as principal __system on local from client 127.0.0.1:61820
2019-09-21T20:41:11.340+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61826 #11 (4 connections now open)
2019-09-21T20:41:11.340+0800 I  NETWORK  [conn11] received client metadata from 127.0.0.1:61826 conn11: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:41:11.343+0800 I  ACCESS   [conn11] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61826
2019-09-21T20:41:28.951+0800 I  CONNPOOL [RS] Ending idle connection to host localhost:37018 because the pool meets constraints; 1 connections to that host remain open
2019-09-21T20:41:31.880+0800 I  CONNPOOL [RS] Connecting to localhost:37018
2019-09-21T20:41:39.945+0800 I  NETWORK  [conn11] end connection 127.0.0.1:61826 (3 connections now open)
2019-09-21T20:41:55.847+0800 I  NETWORK  [conn10] end connection 127.0.0.1:61820 (2 connections now open)
2019-09-21T20:41:55.848+0800 I  NETWORK  [conn7] end connection 127.0.0.1:61818 (1 connection now open)
2019-09-21T20:41:57.309+0800 I  CONTROL  [thread7] Ctrl-C signal
2019-09-21T20:41:57.309+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:41:57.309+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:41:57.309+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:41:57.309+0800 I  REPL     [consoleTerminate] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 1 }
2019-09-21T20:41:57.309+0800 I  REPL     [consoleTerminate] transition to SECONDARY from PRIMARY
2019-09-21T20:41:57.309+0800 I  REPL     [consoleTerminate] Handing off election to localhost:37018
2019-09-21T20:41:57.310+0800 W  NETWORK  [consoleTerminate] Failed to check socket connectivity: 操作成功完成。
2019-09-21T20:41:57.310+0800 I  CONNPOOL [consoleTerminate] dropping unhealthy pooled connection to localhost:37018
2019-09-21T20:41:57.310+0800 I  CONNPOOL [Replication] Connecting to localhost:37018
2019-09-21T20:41:57.310+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:41:57.310+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:41:57.310+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:41:57.310+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:41:57.310+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:41:57.310+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:41:57.311+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:41:57.715+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:41:57.715+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:41:57.715+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:41:57.715+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:41:57.715+0800 I  CONNPOOL [RS] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:41:57.716+0800 I  REPL     [replexec-5] replSetStepUp request to localhost:37018 failed due to CallbackCanceled: Callback canceled
2019-09-21T20:41:57.717+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:41:57.717+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:41:57.717+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:41:57.720+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:41:57.721+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:41:57.721+0800 E  QUERY    [conn9] GetMore command executor error: FAILURE, stats: { stage: "COLLSCAN", nReturned: 9, executionTimeMillisEstimate: 0, works: 175, advanced: 9, needTime: 83, needYield: 0, saveState: 83, restoreState: 82, isEOF: 0, direction: "forward", docsExamined: 9 }
2019-09-21T20:41:57.721+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:41:57.721+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:41:57.721+0800 I  NETWORK  [conn9] Error sending response to client: HostUnreachable: Connection reset by peer. Ending connection from 127.0.0.1:61819 (connection id: 9)
2019-09-21T20:41:57.721+0800 I  NETWORK  [conn9] end connection 127.0.0.1:61819 (0 connections now open)
2019-09-21T20:41:57.728+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:41:57.728+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:41:57.728+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:41:57.761+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:41:57.761+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:41:57.761+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:41:57.807+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:41:57.807+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:41:57.807+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:42:29.194+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:42:29.196+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=6480 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:42:29.681+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { bindIp: "0.0.0.0", port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T20:42:29.682+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:42:29.743+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069749:743114][6480:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:42:29.776+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:42:29.802+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:42:29.822+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:42:29.822+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:42:29.823+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:42:29.823+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:42:29.823+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 64c92370-05db-46ff-83fd-39c249cfaeab and options: { capped: true, size: 10485760 }
2019-09-21T20:42:29.847+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2019-09-21T20:42:29.848+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:42:29.975+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T20:42:29.976+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: f4c90c48-73b8-49dc-954e-7147ea45a5e0 and options: {}
2019-09-21T20:42:30.002+0800 W  REPL     [ftdc] Rollback ID is not initialized yet.
2019-09-21T20:42:30.004+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2019-09-21T20:42:30.004+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:42:30.004+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 5c708a21-666b-4d0b-af41-81be9aa0571e and options: {}
2019-09-21T20:42:30.028+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2019-09-21T20:42:30.028+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:42:30.029+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: afbb4b53-3ec1-426d-a995-1dd1c6e42804 and options: {}
2019-09-21T20:42:30.051+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2019-09-21T20:42:30.051+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:42:30.052+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:42:30.052+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2019-09-21T20:42:30.052+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 84d8d813-2232-4024-80e2-2818a6d6c146 and options: {}
2019-09-21T20:42:30.076+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2019-09-21T20:42:30.076+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:42:30.077+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2019-09-21T20:42:30.077+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:42:30.078+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:42:30.078+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:42:30.078+0800 I  NETWORK  [initandlisten] Listening on 0.0.0.0
2019-09-21T20:42:30.078+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:42:30.078+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T20:42:37.382+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61846 #1 (1 connection now open)
2019-09-21T20:42:37.385+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:42:37.385+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T20:42:37.385+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61846 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:43:00.749+0800 I  COMMAND  [conn1] initiate : no configuration specified. Using a default configuration for the set
2019-09-21T20:43:00.749+0800 I  COMMAND  [conn1] created this configuration for initiation : { _id: "rs0", version: 1, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017" } ] }
2019-09-21T20:43:00.750+0800 I  REPL     [conn1] replSetInitiate admin command received from client
2019-09-21T20:43:00.754+0800 I  REPL     [conn1] replSetInitiate config object with 1 members parses ok
2019-09-21T20:43:00.754+0800 I  REPL     [conn1] ******
2019-09-21T20:43:00.754+0800 I  REPL     [conn1] creating replication oplog of size: 990MB...
2019-09-21T20:43:00.754+0800 I  STORAGE  [conn1] createCollection: local.oplog.rs with generated UUID: 4171e105-4d18-40cc-b9e1-5cf4d5d34eca and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T20:43:00.769+0800 I  STORAGE  [conn1] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:43:00.769+0800 I  STORAGE  [conn1] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T20:43:00.769+0800 I  STORAGE  [conn1] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:43:00.844+0800 I  REPL     [conn1] ******
2019-09-21T20:43:00.844+0800 I  STORAGE  [conn1] createCollection: local.system.replset with generated UUID: 215e0bae-4bf9-4860-a613-caf69d4be9a9 and options: {}
2019-09-21T20:43:00.868+0800 I  INDEX    [conn1] index build: done building index _id_ on ns local.system.replset
2019-09-21T20:43:00.875+0800 I  SHARDING [conn1] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:43:00.875+0800 I  STORAGE  [conn1] createCollection: admin.system.version with provided UUID: e47bf2a6-668b-44ad-8a44-3f6a652fe41a and options: { uuid: UUID("e47bf2a6-668b-44ad-8a44-3f6a652fe41a") }
2019-09-21T20:43:00.900+0800 I  INDEX    [conn1] index build: done building index _id_ on ns admin.system.version
2019-09-21T20:43:00.900+0800 I  COMMAND  [conn1] setting featureCompatibilityVersion to 4.2
2019-09-21T20:43:00.900+0800 I  NETWORK  [conn1] Skip closing connection for connection # 1
2019-09-21T20:43:00.900+0800 I  REPL     [conn1] New replica set config in use: { _id: "rs0", version: 1, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861ad440a6a8c4150264b7') } }
2019-09-21T20:43:00.900+0800 I  REPL     [conn1] This node is DESKTOP-ERRND3C:37017 in the config
2019-09-21T20:43:00.900+0800 I  REPL     [conn1] transition to STARTUP2 from STARTUP
2019-09-21T20:43:00.900+0800 I  REPL     [conn1] Starting replication storage threads
2019-09-21T20:43:00.905+0800 I  REPL     [conn1] transition to RECOVERING from STARTUP2
2019-09-21T20:43:00.906+0800 I  REPL     [conn1] Starting replication fetcher thread
2019-09-21T20:43:00.906+0800 I  REPL     [conn1] Starting replication applier thread
2019-09-21T20:43:00.906+0800 I  REPL     [conn1] Starting replication reporter thread
2019-09-21T20:43:00.906+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:43:00.906+0800 I  COMMAND  [conn1] command local.replset.oplogTruncateAfterPoint appName: "MongoDB Shell" command: replSetInitiate { replSetInitiate: undefined, lsid: { id: UUID("d457b6fc-5e1d-4659-b25f-38d5187189ae") }, $db: "admin" } numYields:0 reslen:149 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 19 } }, ReplicationStateTransition: { acquireCount: { w: 23 } }, Global: { acquireCount: { r: 8, w: 13, W: 2 }, acquireWaitCount: { W: 1 }, timeAcquiringMicros: { W: 26 } }, Database: { acquireCount: { r: 6, w: 2, W: 11 } }, Collection: { acquireCount: { r: 4, w: 2 } }, Mutex: { acquireCount: { r: 16 } }, oplog: { acquireCount: { r: 1, w: 1 } } } flowControl:{ acquireCount: 4 } storage:{} protocol:op_msg 156ms
2019-09-21T20:43:00.906+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:43:00.906+0800 I  ELECTION [rsSync-0] conducting a dry run election to see if we could be elected. current term: 0
2019-09-21T20:43:00.906+0800 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 1
2019-09-21T20:43:00.912+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 1
2019-09-21T20:43:00.912+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2019-09-21T20:43:00.912+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
2019-09-21T20:43:00.912+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2019-09-21T20:43:00.912+0800 I  REPL     [replexec-1] Exited primary catch-up mode.
2019-09-21T20:43:00.912+0800 I  REPL     [replexec-1] Stopping replication producer
2019-09-21T20:43:01.907+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:43:01.907+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:43:02.907+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:43:02.908+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:43:02.908+0800 I  SHARDING [rsSync-0] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:43:02.908+0800 I  STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: 5077e197-1f73-47b0-b32b-ade493638346 and options: {}
2019-09-21T20:43:02.935+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.transactions
2019-09-21T20:43:02.935+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T20:43:02.935+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:43:02.936+0800 I  STORAGE  [monitoring-keys-for-HMAC] createCollection: admin.system.keys with generated UUID: 0f782f2b-d926-44bf-ac90-539b73a2115c and options: {}
2019-09-21T20:43:02.960+0800 I  STORAGE  [WTJournalFlusher] Triggering the first stable checkpoint. Initial Data: Timestamp(1569069780, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569069782, 2)
2019-09-21T20:43:02.966+0800 I  INDEX    [monitoring-keys-for-HMAC] index build: done building index _id_ on ns admin.system.keys
2019-09-21T20:43:18.742+0800 I  ACCESS   [conn1] Unauthorized: not authorized on test to execute command { listCollections: 1.0, filter: {}, nameOnly: true, authorizedCollections: true, lsid: { id: UUID("d457b6fc-5e1d-4659-b25f-38d5187189ae") }, $clusterTime: { clusterTime: Timestamp(1569069782, 5), signature: { hash: BinData(0, 86457ECD01AABA86010FBED68BE14230108F8D85), keyId: 6739103398831849474 } }, $db: "test" }
2019-09-21T20:43:36.768+0800 I  NETWORK  [conn1] end connection 127.0.0.1:61846 (0 connections now open)
2019-09-21T20:43:41.364+0800 I  CONTROL  [thread2] Ctrl-C signal
2019-09-21T20:43:41.364+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:43:41.365+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:43:41.365+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:43:51.364+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:43:51.364+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:43:51.364+0800 I  STORAGE  [consoleTerminate] Failed to stepDown in non-command initiated shutdown path ExceededTimeLimit: No electable secondaries caught up as of 2019-09-21T20:43:51.364+0800. Please use the replSetStepDown command with the argument {force: true} to force node to step down.
2019-09-21T20:43:51.365+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:43:51.365+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:43:51.365+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:43:51.365+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:43:51.365+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:43:51.365+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:43:51.366+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:43:51.922+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:43:51.923+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:43:51.923+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:43:51.923+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:43:51.924+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:43:51.924+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:43:51.924+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:43:51.927+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:43:51.927+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:43:51.927+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:43:51.927+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:43:51.933+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:43:51.933+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:43:51.933+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:43:51.953+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:43:51.954+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:43:51.954+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:43:52.012+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:43:52.012+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:43:52.012+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:44:23.241+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:44:23.244+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=6440 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:44:23.730+0800 I  CONTROL  [initandlisten] options: { config: "m1.conf", net: { bindIp: "0.0.0.0", port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T20:44:23.732+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:44:23.797+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069863:796861][6440:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:44:23.833+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:44:23.859+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:44:23.880+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:44:23.880+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:44:23.880+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:44:23.880+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:44:23.881+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: e09910c6-940c-4046-9c20-e223999d563e and options: { capped: true, size: 10485760 }
2019-09-21T20:44:23.905+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2019-09-21T20:44:23.905+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:44:24.034+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T20:44:24.036+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 3ceefe11-831c-4644-9b32-58551c62e308 and options: {}
2019-09-21T20:44:24.062+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2019-09-21T20:44:24.062+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: f67d0a09-cdb7-4afe-8bbc-5fdcd2244f6d and options: {}
2019-09-21T20:44:24.088+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2019-09-21T20:44:24.088+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:44:24.088+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 94a1cea9-add4-4895-a820-7c265b1d223c and options: {}
2019-09-21T20:44:24.114+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2019-09-21T20:44:24.114+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:44:24.114+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:44:24.114+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2019-09-21T20:44:24.114+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: f6fdf0e1-4128-4de0-ab35-dcf61a205147 and options: {}
2019-09-21T20:44:24.139+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2019-09-21T20:44:24.140+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:44:24.140+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2019-09-21T20:44:24.140+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:44:24.141+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:44:24.141+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:44:24.142+0800 I  NETWORK  [initandlisten] Listening on 0.0.0.0
2019-09-21T20:44:24.142+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T20:44:24.142+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:44:25.002+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:44:27.976+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61853 #1 (1 connection now open)
2019-09-21T20:44:27.978+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:44:27.979+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T20:44:27.979+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61853 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:45:20.429+0800 I  COMMAND  [conn1] initiate : no configuration specified. Using a default configuration for the set
2019-09-21T20:45:20.429+0800 I  COMMAND  [conn1] created this configuration for initiation : { _id: "rs0", version: 1, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017" } ] }
2019-09-21T20:45:20.429+0800 I  REPL     [conn1] replSetInitiate admin command received from client
2019-09-21T20:45:20.437+0800 I  REPL     [conn1] replSetInitiate config object with 1 members parses ok
2019-09-21T20:45:20.437+0800 I  REPL     [conn1] ******
2019-09-21T20:45:20.437+0800 I  REPL     [conn1] creating replication oplog of size: 990MB...
2019-09-21T20:45:20.437+0800 I  STORAGE  [conn1] createCollection: local.oplog.rs with generated UUID: d420ce1f-8bf7-4c05-ba08-7b35b36eb754 and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T20:45:20.447+0800 I  STORAGE  [conn1] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:45:20.448+0800 I  STORAGE  [conn1] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T20:45:20.448+0800 I  STORAGE  [conn1] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:45:20.532+0800 I  REPL     [conn1] ******
2019-09-21T20:45:20.532+0800 I  STORAGE  [conn1] createCollection: local.system.replset with generated UUID: 3d7d7ead-53ae-48f2-95cd-45250a9b375a and options: {}
2019-09-21T20:45:20.556+0800 I  INDEX    [conn1] index build: done building index _id_ on ns local.system.replset
2019-09-21T20:45:20.563+0800 I  SHARDING [conn1] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:45:20.563+0800 I  STORAGE  [conn1] createCollection: admin.system.version with provided UUID: 7797de70-114c-41ac-8482-c04ac37dc118 and options: { uuid: UUID("7797de70-114c-41ac-8482-c04ac37dc118") }
2019-09-21T20:45:20.588+0800 I  INDEX    [conn1] index build: done building index _id_ on ns admin.system.version
2019-09-21T20:45:20.588+0800 I  COMMAND  [conn1] setting featureCompatibilityVersion to 4.2
2019-09-21T20:45:20.588+0800 I  NETWORK  [conn1] Skip closing connection for connection # 1
2019-09-21T20:45:20.588+0800 I  REPL     [conn1] New replica set config in use: { _id: "rs0", version: 1, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T20:45:20.588+0800 I  REPL     [conn1] This node is DESKTOP-ERRND3C:37017 in the config
2019-09-21T20:45:20.588+0800 I  REPL     [conn1] transition to STARTUP2 from STARTUP
2019-09-21T20:45:20.588+0800 I  REPL     [conn1] Starting replication storage threads
2019-09-21T20:45:20.593+0800 I  REPL     [conn1] transition to RECOVERING from STARTUP2
2019-09-21T20:45:20.593+0800 I  REPL     [conn1] Starting replication fetcher thread
2019-09-21T20:45:20.593+0800 I  REPL     [conn1] Starting replication applier thread
2019-09-21T20:45:20.593+0800 I  REPL     [conn1] Starting replication reporter thread
2019-09-21T20:45:20.593+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:45:20.593+0800 I  COMMAND  [conn1] command local.system.rollback.id appName: "MongoDB Shell" command: replSetInitiate { replSetInitiate: undefined, lsid: { id: UUID("ee6aacdc-dbe4-46ac-8259-2d5f3443b4e1") }, $db: "admin" } numYields:0 reslen:149 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 19 } }, ReplicationStateTransition: { acquireCount: { w: 23 } }, Global: { acquireCount: { r: 8, w: 13, W: 2 }, acquireWaitCount: { W: 1 }, timeAcquiringMicros: { W: 28 } }, Database: { acquireCount: { r: 6, w: 2, W: 11 } }, Collection: { acquireCount: { r: 4, w: 2 } }, Mutex: { acquireCount: { r: 16 } }, oplog: { acquireCount: { r: 1, w: 1 } } } flowControl:{ acquireCount: 4 } storage:{} protocol:op_msg 164ms
2019-09-21T20:45:20.594+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:45:20.594+0800 I  ELECTION [rsSync-0] conducting a dry run election to see if we could be elected. current term: 0
2019-09-21T20:45:20.594+0800 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 1
2019-09-21T20:45:20.599+0800 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 1
2019-09-21T20:45:20.599+0800 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
2019-09-21T20:45:20.599+0800 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
2019-09-21T20:45:20.599+0800 I  REPL     [replexec-0] Entering primary catch-up mode.
2019-09-21T20:45:20.599+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2019-09-21T20:45:20.599+0800 I  REPL     [replexec-0] Stopping replication producer
2019-09-21T20:45:21.594+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:45:21.594+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:45:22.594+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:45:22.594+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:45:22.594+0800 I  SHARDING [rsSync-0] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:45:22.594+0800 I  STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: 06cc708c-e982-4201-bc18-5f35a64b54cb and options: {}
2019-09-21T20:45:22.618+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.transactions
2019-09-21T20:45:22.619+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T20:45:22.619+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:45:22.619+0800 I  STORAGE  [monitoring-keys-for-HMAC] createCollection: admin.system.keys with generated UUID: f6199117-0e8e-4524-a7b7-47f3aff47559 and options: {}
2019-09-21T20:45:22.644+0800 I  STORAGE  [WTJournalFlusher] Triggering the first stable checkpoint. Initial Data: Timestamp(1569069920, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569069922, 2)
2019-09-21T20:45:22.645+0800 I  INDEX    [monitoring-keys-for-HMAC] index build: done building index _id_ on ns admin.system.keys
2019-09-21T20:45:33.118+0800 I  STORAGE  [conn1] createCollection: admin.system.users with generated UUID: 8c86bb41-37ea-4d19-a79a-a242316598a3 and options: {}
2019-09-21T20:45:33.142+0800 I  INDEX    [conn1] index build: done building index _id_ on ns admin.system.users
2019-09-21T20:45:33.154+0800 I  INDEX    [conn1] index build: done building index user_1_db_1 on ns admin.system.users
2019-09-21T20:45:45.562+0800 I  ACCESS   [conn1] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61853
2019-09-21T20:46:22.880+0800 I  REPL     [conn1] replSetReconfig admin command received from client; new config: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1.0, host: "localhost:37018" } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T20:46:22.881+0800 E  REPL     [conn1] replSetReconfig got BadValue: Either all host names in a replica set configuration must be localhost references, or none must be; found 1 out of 2 while validating { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1.0, host: "localhost:37018" } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T20:47:31.358+0800 I  REPL     [conn1] replSetReconfig admin command received from client; new config: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1.0, host: "localhost:37018" } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T20:47:31.358+0800 E  REPL     [conn1] replSetReconfig got BadValue: Either all host names in a replica set configuration must be localhost references, or none must be; found 1 out of 2 while validating { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1.0, host: "localhost:37018" } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T20:48:33.030+0800 I  REPL     [conn1] replSetReconfig admin command received from client; new config: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1.0, host: "192.168.1.7:37018" } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T20:48:33.067+0800 I  REPL     [conn1] replSetReconfig config object with 2 members parses ok
2019-09-21T20:48:33.067+0800 I  REPL     [conn1] Scheduling remote command request for reconfig quorum check: RemoteCommand 1 -- target:192.168.1.7:37018 db:admin cmd:{ replSetHeartbeat: "rs0", configVersion: 2, hbv: 1, from: "DESKTOP-ERRND3C:37017", fromId: 0, term: 1 }
2019-09-21T20:48:33.067+0800 I  CONNPOOL [Replication] Connecting to 192.168.1.7:37018
2019-09-21T20:48:33.102+0800 I  REPL     [conn1] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "192.168.1.7:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T20:48:33.102+0800 I  REPL     [conn1] This node is DESKTOP-ERRND3C:37017 in the config
2019-09-21T20:48:33.102+0800 I  REPL     [replexec-1] Member 192.168.1.7:37018 is now in state STARTUP
2019-09-21T20:48:33.105+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61864 #4 (2 connections now open)
2019-09-21T20:48:33.105+0800 I  NETWORK  [conn4] received client metadata from 192.168.1.7:61864 conn4: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:33.141+0800 I  ACCESS   [conn4] Successfully authenticated as principal __system on local from client 192.168.1.7:61864
2019-09-21T20:48:33.142+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61865 #5 (3 connections now open)
2019-09-21T20:48:33.176+0800 I  ACCESS   [conn5] Successfully authenticated as principal __system on local from client 192.168.1.7:61865
2019-09-21T20:48:33.176+0800 I  NETWORK  [conn5] end connection 192.168.1.7:61865 (2 connections now open)
2019-09-21T20:48:33.347+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61866 #6 (3 connections now open)
2019-09-21T20:48:33.348+0800 I  NETWORK  [conn6] received client metadata from 192.168.1.7:61866 conn6: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:33.380+0800 I  ACCESS   [conn6] Successfully authenticated as principal __system on local from client 192.168.1.7:61866
2019-09-21T20:48:33.382+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61867 #7 (4 connections now open)
2019-09-21T20:48:33.382+0800 I  NETWORK  [conn7] received client metadata from 192.168.1.7:61867 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:33.414+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 192.168.1.7:61867
2019-09-21T20:48:33.441+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61868 #8 (5 connections now open)
2019-09-21T20:48:33.441+0800 I  NETWORK  [conn8] received client metadata from 192.168.1.7:61868 conn8: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:33.473+0800 I  ACCESS   [conn8] Successfully authenticated as principal __system on local from client 192.168.1.7:61868
2019-09-21T20:48:33.474+0800 I  NETWORK  [conn8] end connection 192.168.1.7:61868 (4 connections now open)
2019-09-21T20:48:33.573+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61869 #9 (5 connections now open)
2019-09-21T20:48:33.573+0800 I  NETWORK  [conn9] received client metadata from 192.168.1.7:61869 conn9: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:33.605+0800 I  ACCESS   [conn9] Successfully authenticated as principal __system on local from client 192.168.1.7:61869
2019-09-21T20:48:33.605+0800 I  NETWORK  [conn9] end connection 192.168.1.7:61869 (4 connections now open)
2019-09-21T20:48:33.688+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61870 #10 (5 connections now open)
2019-09-21T20:48:33.688+0800 I  NETWORK  [conn10] received client metadata from 192.168.1.7:61870 conn10: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:33.720+0800 I  ACCESS   [conn10] Successfully authenticated as principal __system on local from client 192.168.1.7:61870
2019-09-21T20:48:33.721+0800 I  NETWORK  [conn10] end connection 192.168.1.7:61870 (4 connections now open)
2019-09-21T20:48:33.792+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61871 #11 (5 connections now open)
2019-09-21T20:48:33.792+0800 I  NETWORK  [conn11] received client metadata from 192.168.1.7:61871 conn11: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:33.824+0800 I  ACCESS   [conn11] Successfully authenticated as principal __system on local from client 192.168.1.7:61871
2019-09-21T20:48:33.825+0800 I  NETWORK  [conn11] end connection 192.168.1.7:61871 (4 connections now open)
2019-09-21T20:48:35.102+0800 I  REPL     [replexec-1] Member 192.168.1.7:37018 is now in state SECONDARY
2019-09-21T20:48:43.382+0800 I  NETWORK  [conn6] end connection 192.168.1.7:61866 (3 connections now open)
2019-09-21T20:48:53.886+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61872 #12 (4 connections now open)
2019-09-21T20:48:53.886+0800 I  NETWORK  [conn12] received client metadata from 192.168.1.7:61872 conn12: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:53.918+0800 I  ACCESS   [conn12] Successfully authenticated as principal __system on local from client 192.168.1.7:61872
2019-09-21T20:49:24.141+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:49:24.141+0800 I  STORAGE  [LogicalSessionCacheRefresh] createCollection: config.system.sessions with provided UUID: 8ca74314-c408-4ad1-ac17-b993991be758 and options: { uuid: UUID("8ca74314-c408-4ad1-ac17-b993991be758") }
2019-09-21T20:49:24.168+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: done building index _id_ on ns config.system.sessions
2019-09-21T20:49:24.208+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: starting on config.system.sessions properties: { v: 2, key: { lastUse: 1 }, name: "lsidTTLIndex", ns: "config.system.sessions", expireAfterSeconds: 1800 } using method: Hybrid
2019-09-21T20:49:24.208+0800 I  INDEX    [LogicalSessionCacheRefresh] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:49:24.208+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: collection scan done. scanned 0 total records in 0 seconds
2019-09-21T20:49:24.209+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:49:24.219+0800 I  INDEX    [LogicalSessionCacheRefresh] index build: done building index lsidTTLIndex on ns config.system.sessions
2019-09-21T20:52:26.917+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62066 #13 (5 connections now open)
2019-09-21T20:52:26.918+0800 I  NETWORK  [conn13] received client metadata from 192.168.1.7:62066 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:52:26.980+0800 I  ACCESS   [conn13] Successfully authenticated as principal __system on local from client 192.168.1.7:62066
2019-09-21T20:52:26.981+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62067 #14 (6 connections now open)
2019-09-21T20:52:26.982+0800 I  NETWORK  [conn14] received client metadata from 192.168.1.7:62067 conn14: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:52:26.982+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62068 #15 (7 connections now open)
2019-09-21T20:52:26.982+0800 I  NETWORK  [conn15] received client metadata from 192.168.1.7:62068 conn15: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:52:27.034+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 192.168.1.7:62068
2019-09-21T20:52:27.034+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 192.168.1.7:62067
2019-09-21T20:52:27.069+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 192.168.1.7:62067
2019-09-21T20:52:27.101+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 192.168.1.7:62067
2019-09-21T20:52:27.137+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 192.168.1.7:62067
2019-09-21T20:53:59.216+0800 I  SHARDING [conn1] Marking collection d.c as collection version: <unsharded>
2019-09-21T20:53:59.216+0800 I  STORAGE  [conn1] createCollection: d.c with generated UUID: ec6c1548-a84d-4e64-948c-2cb60921b24d and options: {}
2019-09-21T20:53:59.245+0800 I  INDEX    [conn1] index build: done building index _id_ on ns d.c
2019-09-21T20:57:26.954+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 192.168.1.7:62067
2019-09-21T20:57:26.954+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 192.168.1.7:62068
2019-09-21T20:57:26.986+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 192.168.1.7:62067
2019-09-21T20:57:27.035+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 192.168.1.7:62067
2019-09-21T20:57:27.068+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 192.168.1.7:62067
2019-09-21T20:58:40.236+0800 I  QUERY    [clientcursormon] Cursor id 6758260776625534593 timed out, idle since 2019-09-21T20:48:38.384+0800
2019-09-21T21:00:02.926+0800 I  NETWORK  [conn1] end connection 127.0.0.1:61853 (6 connections now open)
2019-09-21T21:00:18.258+0800 I  CONTROL  [thread14] Ctrl-C signal
2019-09-21T21:00:18.259+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T21:00:18.259+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T21:00:18.259+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T21:00:18.259+0800 I  REPL     [consoleTerminate] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 1 }
2019-09-21T21:00:18.259+0800 I  REPL     [consoleTerminate] transition to SECONDARY from PRIMARY
2019-09-21T21:00:18.259+0800 I  REPL     [consoleTerminate] Handing off election to 192.168.1.7:37018
2019-09-21T21:00:18.260+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T21:00:18.260+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T21:00:18.260+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T21:00:18.260+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T21:00:18.260+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T21:00:18.260+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T21:00:18.260+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T21:00:18.956+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T21:00:18.956+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T21:00:18.957+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T21:00:18.958+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T21:00:18.958+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T21:00:18.959+0800 I  CONNPOOL [Replication] Dropping all pooled connections to 192.168.1.7:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T21:00:18.959+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T21:00:18.959+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T21:00:18.962+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T21:00:18.962+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T21:00:18.963+0800 E  QUERY    [conn12] GetMore command executor error: FAILURE, stats: { stage: "COLLSCAN", nReturned: 74, executionTimeMillisEstimate: 0, works: 1692, advanced: 74, needTime: 809, needYield: 0, saveState: 809, restoreState: 808, isEOF: 0, direction: "forward", docsExamined: 74 }
2019-09-21T21:00:18.963+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T21:00:18.963+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T21:00:18.968+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T21:00:18.969+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T21:00:18.969+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T21:00:19.021+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T21:00:19.021+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T21:00:19.021+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T21:00:19.076+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T21:00:19.076+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T21:00:19.076+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T21:00:25.011+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T21:00:25.012+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=4528 port=37017 dbpath=C:\Users\xy\mongo\mdb1 64-bit host=DESKTOP-ERRND3C
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T21:00:25.520+0800 I  CONTROL  [initandlisten] options: { config: "mongo_win1.conf", net: { bindIp: "0.0.0.0", port: 37017 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb1", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb1.log" } }
2019-09-21T21:00:25.521+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb1 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T21:00:25.522+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T21:00:25.560+0800 I  STORAGE  [initandlisten] WiredTiger message [1569070825:559882][4528:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T21:00:25.663+0800 I  STORAGE  [initandlisten] WiredTiger message [1569070825:663003][4528:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T21:00:25.800+0800 I  STORAGE  [initandlisten] WiredTiger message [1569070825:800112][4528:140713979633712], txn-recover: Main recovery loop: starting at 1/107648 to 2/256
2019-09-21T21:00:25.801+0800 I  STORAGE  [initandlisten] WiredTiger message [1569070825:800112][4528:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T21:00:25.900+0800 I  STORAGE  [initandlisten] WiredTiger message [1569070825:900227][4528:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T21:00:25.984+0800 I  STORAGE  [initandlisten] WiredTiger message [1569070825:983290][4528:140713979633712], txn-recover: Set global recovery timestamp: (1569070812,1)
2019-09-21T21:00:26.024+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569070812, 1)
2019-09-21T21:00:26.036+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T21:00:26.036+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 101 records totaling to 13216 bytes
2019-09-21T21:00:26.036+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T21:00:26.043+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T21:00:26.085+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T21:00:26.086+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T21:00:26.086+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T21:00:26.086+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T21:00:26.087+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T21:00:26.223+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb1/diagnostic.data'
2019-09-21T21:00:26.224+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T21:00:26.224+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T21:00:26.225+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T21:00:26.225+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569070812, 1) (top of oplog: { ts: Timestamp(1569070812, 1), t: 1 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T21:00:26.225+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569070812, 1)
2019-09-21T21:00:26.225+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T21:00:26.225+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T21:00:26.226+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T21:00:26.227+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T21:00:26.227+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T21:00:26.227+0800 I  NETWORK  [initandlisten] Listening on 0.0.0.0
2019-09-21T21:00:26.227+0800 I  NETWORK  [initandlisten] waiting for connections on port 37017
2019-09-21T21:00:26.227+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T21:00:26.262+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "192.168.1.7:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T21:00:26.262+0800 I  REPL     [replexec-0] This node is DESKTOP-ERRND3C:37017 in the config
2019-09-21T21:00:26.262+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T21:00:26.263+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T21:00:26.263+0800 I  CONNPOOL [Replication] Connecting to 192.168.1.7:37018
2019-09-21T21:00:26.265+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T21:00:26.265+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T21:00:26.265+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T21:00:26.265+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T21:00:26.265+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T21:00:26.265+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2019-09-21T21:00:26.265+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T21:00:26.265+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T21:00:26.297+0800 I  REPL     [replexec-1] Member 192.168.1.7:37018 is now in state SECONDARY
2019-09-21T21:00:26.424+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T21:00:26.665+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62111 #3 (1 connection now open)
2019-09-21T21:00:26.665+0800 I  NETWORK  [conn3] received client metadata from 192.168.1.7:62111 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T21:00:26.677+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 192.168.1.7:62111
2019-09-21T21:00:27.956+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62114 #4 (2 connections now open)
2019-09-21T21:00:27.957+0800 I  NETWORK  [conn4] received client metadata from 192.168.1.7:62114 conn4: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T21:00:27.968+0800 I  ACCESS   [conn4] Successfully authenticated as principal __system on local from client 192.168.1.7:62114
2019-09-21T21:00:28.437+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569070812, 1), t: 1 } }
2019-09-21T21:00:28.437+0800 I  ELECTION [conn3] Sending vote response: { term: 2, voteGranted: true, reason: "" }
2019-09-21T21:00:28.443+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 3, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569070812, 1), t: 1 } }
2019-09-21T21:00:28.443+0800 I  ELECTION [conn3] Sending vote response: { term: 3, voteGranted: true, reason: "" }
2019-09-21T21:00:28.798+0800 I  REPL     [replexec-0] Member 192.168.1.7:37018 is now in state PRIMARY
2019-09-21T21:00:30.267+0800 I  REPL     [rsBackgroundSync] sync source candidate: 192.168.1.7:37018
2019-09-21T21:00:30.268+0800 I  CONNPOOL [RS] Connecting to 192.168.1.7:37018
2019-09-21T21:00:30.301+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 192.168.1.7:37018
2019-09-21T21:00:30.303+0800 I  SHARDING [rsSync-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T21:00:42.249+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62120 #7 (3 connections now open)
2019-09-21T21:00:42.253+0800 I  SHARDING [conn7] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T21:00:42.254+0800 I  NETWORK  [conn7] received client metadata from 127.0.0.1:62120 conn7: { driver: { name: "nodejs", version: "3.1.13" }, os: { type: "Windows_NT", name: "win32", architecture: "x64", version: "10.0.10240" }, platform: "Node.js v10.2.0, LE, mongodb-core: 3.1.11" }
2019-09-21T21:00:43.371+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62122 #8 (4 connections now open)
2019-09-21T21:00:44.394+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62124 #9 (5 connections now open)
2019-09-21T21:00:53.019+0800 I  NETWORK  [conn9] end connection 127.0.0.1:62124 (4 connections now open)
2019-09-21T21:00:53.019+0800 I  NETWORK  [conn7] end connection 127.0.0.1:62120 (3 connections now open)
2019-09-21T21:00:53.019+0800 I  NETWORK  [conn8] end connection 127.0.0.1:62122 (2 connections now open)
2019-09-21T21:01:04.495+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62126 #10 (3 connections now open)
2019-09-21T21:01:04.496+0800 I  NETWORK  [conn10] received client metadata from 127.0.0.1:62126 conn10: { driver: { name: "nodejs", version: "3.1.13" }, os: { type: "Windows_NT", name: "win32", architecture: "x64", version: "10.0.10240" }, platform: "Node.js v10.2.0, LE, mongodb-core: 3.1.11" }
2019-09-21T21:01:04.533+0800 I  ACCESS   [conn10] Successfully authenticated as principal user1 on admin from client 127.0.0.1:62126
2019-09-21T21:01:05.566+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62128 #11 (4 connections now open)
2019-09-21T21:01:05.570+0800 I  ACCESS   [conn11] Successfully authenticated as principal user1 on admin from client 127.0.0.1:62128
2019-09-21T21:01:06.574+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62130 #12 (5 connections now open)
2019-09-21T21:01:06.576+0800 I  ACCESS   [conn12] Successfully authenticated as principal user1 on admin from client 127.0.0.1:62130
2019-09-21T21:01:16.659+0800 I  NETWORK  [conn12] end connection 127.0.0.1:62130 (4 connections now open)
2019-09-21T21:01:16.659+0800 I  NETWORK  [conn10] end connection 127.0.0.1:62126 (3 connections now open)
2019-09-21T21:01:16.659+0800 I  NETWORK  [conn11] end connection 127.0.0.1:62128 (2 connections now open)
2019-09-21T21:01:19.422+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62131 #13 (3 connections now open)
2019-09-21T21:01:19.422+0800 I  NETWORK  [conn13] received client metadata from 192.168.1.7:62131 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T21:01:19.434+0800 I  ACCESS   [conn13] Successfully authenticated as principal __system on local from client 192.168.1.7:62131
2019-09-21T21:01:33.939+0800 I  CONTROL  [thread10] CTRL_CLOSE_EVENT signal
2019-09-21T21:01:33.939+0800 I  CONTROL  [consoleTerminate] got CTRL_CLOSE_EVENT, will terminate after current cmd ends
2019-09-21T21:01:33.939+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T21:01:33.940+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T21:01:33.940+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T21:01:33.940+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T21:01:33.940+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 192.168.1.7:37018: CallbackCanceled: Reporter no longer valid
2019-09-21T21:01:33.940+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T21:01:33.940+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T21:01:33.940+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2019-09-21T21:01:33.940+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T21:01:33.940+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T21:01:33.941+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T21:01:33.941+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T21:01:33.941+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T21:01:33.941+0800 I  CONNPOOL [RS] Dropping all pooled connections to 192.168.1.7:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T21:01:33.942+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T21:01:33.942+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T21:01:33.943+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T21:01:33.946+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T21:01:33.946+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T21:01:33.946+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T21:01:33.946+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T21:01:33.953+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T21:01:33.953+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T21:01:33.953+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T21:01:33.979+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T21:01:33.979+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T21:01:33.979+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T21:01:34.037+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T21:01:34.038+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T21:01:34.038+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
