2019-09-21T19:16:02.836+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=5932 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T19:16:03.333+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T19:16:03.335+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T19:16:03.394+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064563:394964][5932:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T19:16:03.417+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T19:16:03.447+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T19:16:03.468+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:16:03.469+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T19:16:03.469+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T19:16:03.469+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T19:16:03.469+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T19:16:03.469+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T19:16:03.469+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T19:16:03.469+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:16:03.472+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T19:16:03.472+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T19:16:03.472+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T19:16:03.472+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T19:16:03.473+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 64b712c7-4174-4b1b-85c3-f2afbaf0c79e and options: { capped: true, size: 10485760 }
2019-09-21T19:16:03.498+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2019-09-21T19:16:03.498+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T19:16:03.629+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T19:16:03.631+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 53b76bed-7d31-4dd0-8b0a-6608282a38df and options: {}
2019-09-21T19:16:03.656+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2019-09-21T19:16:03.656+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 839f9e4a-edc6-4809-8de3-aee595a54eab and options: {}
2019-09-21T19:16:03.680+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2019-09-21T19:16:03.681+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T19:16:03.681+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: eb4c5493-b526-404a-8a45-05e1a55fae63 and options: {}
2019-09-21T19:16:03.707+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2019-09-21T19:16:03.707+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T19:16:03.707+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T19:16:03.707+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2019-09-21T19:16:03.707+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 4c9674fa-0626-4d64-b08a-d63086b878c6 and options: {}
2019-09-21T19:16:03.736+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2019-09-21T19:16:03.737+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T19:16:03.737+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2019-09-21T19:16:03.737+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T19:16:03.738+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:16:03.738+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T19:16:03.738+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T19:16:03.738+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T19:16:03.738+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T19:16:04.002+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T19:16:20.090+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61112 #1 (1 connection now open)
2019-09-21T19:16:20.092+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T19:16:20.092+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T19:16:20.093+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61112 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:35.385+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61117 #2 (2 connections now open)
2019-09-21T19:16:35.419+0800 I  ACCESS   [conn2] Successfully authenticated as principal __system on local from client 127.0.0.1:61117
2019-09-21T19:16:35.419+0800 I  NETWORK  [conn2] end connection 127.0.0.1:61117 (1 connection now open)
2019-09-21T19:16:35.420+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61118 #3 (2 connections now open)
2019-09-21T19:16:35.421+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61118 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:16:35.452+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 127.0.0.1:61118
2019-09-21T19:16:35.453+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:16:35.507+0800 I  STORAGE  [replexec-1] createCollection: local.system.replset with generated UUID: ffbf17c8-ec8a-40f4-b2a9-d1abff7cf19c and options: {}
2019-09-21T19:16:35.534+0800 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.system.replset
2019-09-21T19:16:35.534+0800 I  REPL     [replexec-1] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:16:35.534+0800 I  REPL     [replexec-1] This node is localhost:37018 in the config
2019-09-21T19:16:35.534+0800 I  REPL     [replexec-1] transition to STARTUP2 from STARTUP
2019-09-21T19:16:35.535+0800 I  REPL     [replexec-1] Starting replication storage threads
2019-09-21T19:16:35.535+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state PRIMARY
2019-09-21T19:16:35.541+0800 I  STORAGE  [replexec-1] createCollection: local.temp_oplog_buffer with generated UUID: 3b51de56-af0f-4bf0-878b-0cdb06f389ce and options: { temp: true }
2019-09-21T19:16:35.566+0800 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.temp_oplog_buffer
2019-09-21T19:16:35.567+0800 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
2019-09-21T19:16:35.567+0800 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (3b51de56-af0f-4bf0-878b-0cdb06f389ce).
2019-09-21T19:16:35.576+0800 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: c1374735-ecb8-4040-9d72-19533615d76b and options: { temp: true }
2019-09-21T19:16:35.601+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
2019-09-21T19:16:35.602+0800 I  REPL     [replication-0] sync source candidate: localhost:37017
2019-09-21T19:16:35.602+0800 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
2019-09-21T19:16:35.602+0800 I  REPL     [replication-0] ******
2019-09-21T19:16:35.602+0800 I  REPL     [replication-0] creating replication oplog of size: 990MB...
2019-09-21T19:16:35.602+0800 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: cf7ef70f-054e-4f1c-bdb4-cc92bf25df65 and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T19:16:35.615+0800 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T19:16:35.615+0800 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T19:16:35.615+0800 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
2019-09-21T19:16:35.711+0800 I  REPL     [replication-0] ******
2019-09-21T19:16:35.711+0800 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
2019-09-21T19:16:35.711+0800 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
2019-09-21T19:16:35.728+0800 I  SHARDING [replication-1] Marking collection local.temp_oplog_buffer as collection version: <unsharded>
2019-09-21T19:16:35.729+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.keys
2019-09-21T19:16:35.741+0800 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.keys with provided UUID: 0ad2349c-7899-4b1a-9675-f6c15381c2ed and options: { uuid: UUID("0ad2349c-7899-4b1a-9675-f6c15381c2ed") }
2019-09-21T19:16:35.767+0800 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.keys properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.keys" } using method: Foreground
2019-09-21T19:16:35.767+0800 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 500 megabytes of RAM
2019-09-21T19:16:35.802+0800 I  SHARDING [repl-writer-worker-1] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T19:16:35.802+0800 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.keys finished cloning with status: OK
2019-09-21T19:16:35.803+0800 I  INDEX    [replication-1] index build: inserted 2 keys from external sorter into index in 0 seconds
2019-09-21T19:16:35.812+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.keys
2019-09-21T19:16:35.812+0800 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.users
2019-09-21T19:16:35.813+0800 I  STORAGE  [repl-writer-worker-2] createCollection: admin.system.users with provided UUID: 3d47f0c4-ade7-43b3-826a-d1b46280ed7e and options: { uuid: UUID("3d47f0c4-ade7-43b3-826a-d1b46280ed7e") }
2019-09-21T19:16:35.841+0800 I  INDEX    [repl-writer-worker-2] index build: starting on admin.system.users properties: { v: 2, unique: true, key: { user: 1, db: 1 }, name: "user_1_db_1", ns: "admin.system.users" } using method: Foreground
2019-09-21T19:16:35.841+0800 I  INDEX    [repl-writer-worker-2] build may temporarily use up to 500 megabytes of RAM
2019-09-21T19:16:35.852+0800 I  INDEX    [repl-writer-worker-2] index build: starting on admin.system.users properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.users" } using method: Foreground
2019-09-21T19:16:35.852+0800 I  INDEX    [repl-writer-worker-2] build may temporarily use up to 500 megabytes of RAM
2019-09-21T19:16:35.888+0800 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.users finished cloning with status: OK
2019-09-21T19:16:35.889+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T19:16:35.897+0800 I  INDEX    [replication-0] index build: done building index user_1_db_1 on ns admin.system.users
2019-09-21T19:16:35.897+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T19:16:35.905+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.users
2019-09-21T19:16:35.905+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.version
2019-09-21T19:16:35.906+0800 I  STORAGE  [repl-writer-worker-4] createCollection: admin.system.version with provided UUID: 9f20343a-0893-4c75-b1b3-bd4b17f5965b and options: { uuid: UUID("9f20343a-0893-4c75-b1b3-bd4b17f5965b") }
2019-09-21T19:16:35.931+0800 I  INDEX    [repl-writer-worker-4] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
2019-09-21T19:16:35.931+0800 I  INDEX    [repl-writer-worker-4] build may temporarily use up to 500 megabytes of RAM
2019-09-21T19:16:35.967+0800 I  COMMAND  [repl-writer-worker-5] setting featureCompatibilityVersion to 4.2
2019-09-21T19:16:35.967+0800 I  NETWORK  [repl-writer-worker-5] Skip closing connection for connection # 3
2019-09-21T19:16:35.967+0800 I  NETWORK  [repl-writer-worker-5] Skip closing connection for connection # 1
2019-09-21T19:16:35.967+0800 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.version finished cloning with status: OK
2019-09-21T19:16:35.968+0800 I  INDEX    [replication-1] index build: inserted 2 keys from external sorter into index in 0 seconds
2019-09-21T19:16:35.975+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.version
2019-09-21T19:16:35.976+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:config.transactions
2019-09-21T19:16:35.977+0800 I  STORAGE  [repl-writer-worker-6] createCollection: config.transactions with provided UUID: 59ce2d95-3f83-4e14-bc03-abb88b892ca8 and options: { uuid: UUID("59ce2d95-3f83-4e14-bc03-abb88b892ca8") }
2019-09-21T19:16:36.030+0800 I  INDEX    [repl-writer-worker-6] index build: starting on config.transactions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.transactions" } using method: Hybrid
2019-09-21T19:16:36.030+0800 I  INDEX    [repl-writer-worker-6] build may temporarily use up to 500 megabytes of RAM
2019-09-21T19:16:36.064+0800 I  INITSYNC [replication-1] CollectionCloner ns:config.transactions finished cloning with status: OK
2019-09-21T19:16:36.064+0800 I  INDEX    [replication-1] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T19:16:36.070+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns config.transactions
2019-09-21T19:16:36.081+0800 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:config.system.sessions
2019-09-21T19:16:36.082+0800 I  STORAGE  [repl-writer-worker-7] createCollection: config.system.sessions with provided UUID: e2aeef50-aeb8-4512-af75-5a54ee372125 and options: { uuid: UUID("e2aeef50-aeb8-4512-af75-5a54ee372125") }
2019-09-21T19:16:36.125+0800 I  INDEX    [repl-writer-worker-7] index build: starting on config.system.sessions properties: { v: 2, key: { lastUse: 1 }, name: "lsidTTLIndex", ns: "config.system.sessions", expireAfterSeconds: 1800 } using method: Hybrid
2019-09-21T19:16:36.125+0800 I  INDEX    [repl-writer-worker-7] build may temporarily use up to 500 megabytes of RAM
2019-09-21T19:16:36.164+0800 I  INDEX    [repl-writer-worker-7] index build: starting on config.system.sessions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.system.sessions" } using method: Hybrid
2019-09-21T19:16:36.164+0800 I  INDEX    [repl-writer-worker-7] build may temporarily use up to 500 megabytes of RAM
2019-09-21T19:16:36.198+0800 I  INITSYNC [replication-0] CollectionCloner ns:config.system.sessions finished cloning with status: OK
2019-09-21T19:16:36.199+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T19:16:36.204+0800 I  INDEX    [replication-0] index build: done building index lsidTTLIndex on ns config.system.sessions
2019-09-21T19:16:36.205+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T19:16:36.211+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns config.system.sessions
2019-09-21T19:16:36.227+0800 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:test.c
2019-09-21T19:16:36.228+0800 I  STORAGE  [repl-writer-worker-9] createCollection: test.c with provided UUID: b034ea59-f24e-42ca-8085-4048a1182b15 and options: { uuid: UUID("b034ea59-f24e-42ca-8085-4048a1182b15") }
2019-09-21T19:16:36.277+0800 I  INDEX    [repl-writer-worker-9] index build: starting on test.c properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "test.c" } using method: Hybrid
2019-09-21T19:16:36.277+0800 I  INDEX    [repl-writer-worker-9] build may temporarily use up to 500 megabytes of RAM
2019-09-21T19:16:36.311+0800 I  SHARDING [repl-writer-worker-10] Marking collection test.c as collection version: <unsharded>
2019-09-21T19:16:36.312+0800 I  INITSYNC [replication-0] CollectionCloner ns:test.c finished cloning with status: OK
2019-09-21T19:16:36.312+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T19:16:36.318+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns test.c
2019-09-21T19:16:36.328+0800 I  INITSYNC [replication-0] Finished cloning data: OK. Beginning oplog replay.
2019-09-21T19:16:36.329+0800 I  INITSYNC [replication-1] No need to apply operations. (currently at { : Timestamp(1569064595, 1) })
2019-09-21T19:16:36.329+0800 I  SHARDING [replication-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T19:16:36.330+0800 I  INITSYNC [replication-1] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
2019-09-21T19:16:36.330+0800 I  INITSYNC [replication-1] Initial sync attempt finishing up.
2019-09-21T19:16:36.330+0800 I  INITSYNC [replication-1] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1569064595567), initialSyncAttempts: [], fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1569064595, 1), initialSyncOplogEnd: Timestamp(1569064595, 1), databases: { databasesCloned: 3, admin: { collections: 3, clonedCollections: 3, start: new Date(1569064595727), end: new Date(1569064595976), elapsedMillis: 249, admin.system.keys: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1569064595729), end: new Date(1569064595812), elapsedMillis: 83, receivedBatches: 1 }, admin.system.users: { documentsToCopy: 1, documentsCopied: 1, indexes: 2, fetchedBatches: 1, start: new Date(1569064595812), end: new Date(1569064595905), elapsedMillis: 93, receivedBatches: 1 }, admin.system.version: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1569064595905), end: new Date(1569064595976), elapsedMillis: 71, receivedBatches: 1 } }, config: { collections: 2, clonedCollections: 2, start: new Date(1569064595976), end: new Date(1569064596227), elapsedMillis: 251, config.transactions: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, start: new Date(1569064595976), end: new Date(1569064596081), elapsedMillis: 105, receivedBatches: 0 }, config.system.sessions: { documentsToCopy: 1, documentsCopied: 1, indexes: 2, fetchedBatches: 1, start: new Date(1569064596081), end: new Date(1569064596227), elapsedMillis: 146, receivedBatches: 1 } }, test: { collections: 1, clonedCollections: 1, start: new Date(1569064596227), end: new Date(1569064596328), elapsedMillis: 101, test.c: { documentsToCopy: 1, documentsCopied: 1, indexes: 1, fetchedBatches: 1, start: new Date(1569064596228), end: new Date(1569064596328), elapsedMillis: 100, receivedBatches: 1 } } } }
2019-09-21T19:16:36.330+0800 I  STORAGE  [replication-1] Finishing collection drop for local.temp_oplog_buffer (c1374735-ecb8-4040-9d72-19533615d76b).
2019-09-21T19:16:36.340+0800 I  SHARDING [replication-1] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T19:16:36.347+0800 I  INITSYNC [replication-1] initial sync done; took 0s.
2019-09-21T19:16:36.347+0800 I  REPL     [replication-1] transition to RECOVERING from STARTUP2
2019-09-21T19:16:36.347+0800 I  REPL     [replication-1] Starting replication fetcher thread
2019-09-21T19:16:36.347+0800 I  REPL     [replication-1] Starting replication applier thread
2019-09-21T19:16:36.347+0800 I  REPL     [replication-1] Starting replication reporter thread
2019-09-21T19:16:36.347+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T19:16:36.347+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T19:16:36.348+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T19:16:36.348+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T19:16:37.848+0800 I  STORAGE  [replexec-1] Triggering the first stable checkpoint. Initial Data: Timestamp(1569064595, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569064595, 1)
2019-09-21T19:16:45.728+0800 I  CONNPOOL [RS] Ending connection to host localhost:37017 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2019-09-21T19:16:46.355+0800 I  REPL     [rsBackgroundSync] sync source candidate: localhost:37017
2019-09-21T19:16:46.356+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:37017
2019-09-21T19:16:46.356+0800 I  CONNPOOL [RS] Connecting to localhost:37017
2019-09-21T19:17:24.456+0800 I  ACCESS   [conn1] Supported SASL mechanisms requested for unknown user 'user1@test'
2019-09-21T19:17:24.459+0800 I  ACCESS   [conn1] SASL SCRAM-SHA-1 authentication failed for user1 on test from client 127.0.0.1:61112 ; UserNotFound: Could not find user "user1" for db "test"
2019-09-21T19:18:12.487+0800 I  ACCESS   [conn1] Supported SASL mechanisms requested for unknown user 'user1@test'
2019-09-21T19:18:12.488+0800 I  ACCESS   [conn1] SASL SCRAM-SHA-1 authentication failed for user1 on test from client 127.0.0.1:61112 ; UserNotFound: Could not find user "user1" for db "test"
2019-09-21T19:18:30.335+0800 I  ACCESS   [conn1] Supported SASL mechanisms requested for unknown user 'user1@admim'
2019-09-21T19:18:30.336+0800 I  ACCESS   [conn1] SASL SCRAM-SHA-1 authentication failed for user1 on admim from client 127.0.0.1:61112 ; UserNotFound: Could not find user "user1" for db "admim"
2019-09-21T19:19:26.007+0800 I  COMMAND  [conn3] Received replSetStepUp request
2019-09-21T19:19:26.007+0800 I  ELECTION [conn3] Starting an election due to step up request
2019-09-21T19:19:26.007+0800 I  ELECTION [conn3] skipping dry run and running for election in term 2
2019-09-21T19:19:26.013+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 229 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064756, 1), t: 1 } }
2019-09-21T19:19:26.014+0800 I  ELECTION [replexec-2] VoteRequester(term 2) received an invalid response from localhost:37017: ShutdownInProgress: In the process of shutting down; response message: { operationTime: Timestamp(1569064756, 1), ok: 0.0, errmsg: "In the process of shutting down", code: 91, codeName: "ShutdownInProgress", $clusterTime: { clusterTime: Timestamp(1569064756, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
2019-09-21T19:19:26.014+0800 I  ELECTION [replexec-2] not becoming primary, we received insufficient votes
2019-09-21T19:19:26.014+0800 I  ELECTION [replexec-2] Lost election due to internal error
2019-09-21T19:19:26.014+0800 I  COMMAND  [conn3] replSetStepUp request failed :: caused by :: CommandFailed: Election failed.
2019-09-21T19:19:26.135+0800 I  REPL     [replication-1] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: localhost:37017, my last fetched oplog optime: { ts: Timestamp(1569064756, 1), t: 1 }, latest oplog optime of sync source: { ts: Timestamp(1569064756, 1), t: 1 } (sync source does not know the primary)
2019-09-21T19:19:26.135+0800 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: localhost:37017, OpTime { ts: Timestamp(1569064756, 1), t: 1 }, its sync source index:-1
2019-09-21T19:19:26.135+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source localhost:37017 (config version: 2; last applied optime: { ts: Timestamp(1569064756, 1), t: 1 }; sync source index: -1; primary index: -1) is no longer valid
2019-09-21T19:19:26.135+0800 I  REPL     [rsBackgroundSync] Clearing sync source localhost:37017 to choose a new one.
2019-09-21T19:19:26.135+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T19:19:26.136+0800 I  REPL     [replexec-2] Member localhost:37017 is now in state SECONDARY
2019-09-21T19:19:26.244+0800 I  NETWORK  [conn3] end connection 127.0.0.1:61118 (1 connection now open)
2019-09-21T19:19:26.636+0800 W  NETWORK  [replexec-0] Failed to check socket connectivity: 操作成功完成。
2019-09-21T19:19:26.636+0800 I  CONNPOOL [replexec-0] dropping unhealthy pooled connection to localhost:37017
2019-09-21T19:19:26.636+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:19:28.481+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61141 #15 (2 connections now open)
2019-09-21T19:19:28.482+0800 I  NETWORK  [conn15] end connection 127.0.0.1:61141 (1 connection now open)
2019-09-21T19:19:28.484+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61142 #16 (2 connections now open)
2019-09-21T19:19:28.484+0800 I  NETWORK  [conn16] received client metadata from 127.0.0.1:61142 conn16: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:28.640+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state RS_UNKNOWN due to authentication issue.
2019-09-21T19:19:28.640+0800 I  REPL     [replexec-0] transition to RECOVERING from SECONDARY
2019-09-21T19:19:28.640+0800 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
2019-09-21T19:19:29.140+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:19:29.590+0800 I  CONTROL  [thread6] Ctrl-C signal
2019-09-21T19:19:29.590+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T19:19:29.591+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T19:19:29.591+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T19:19:29.591+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T19:19:29.591+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T19:19:29.591+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:37017: CallbackCanceled: Reporter no longer valid
2019-09-21T19:19:29.591+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T19:19:29.591+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T19:19:29.592+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T19:19:30.137+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T19:19:30.138+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T19:19:30.138+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:19:30.138+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:19:30.138+0800 I  CONNPOOL [RS] Dropping all pooled connections to localhost:37017 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T19:19:30.139+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T19:19:30.139+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T19:19:30.139+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T19:19:30.141+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T19:19:30.141+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T19:19:30.142+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T19:19:30.142+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T19:19:30.147+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T19:19:30.147+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T19:19:30.147+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T19:19:30.158+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T19:19:30.158+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T19:19:30.158+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T19:19:30.226+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T19:19:30.226+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T19:19:30.226+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T19:19:30.282+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T19:19:30.742+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T19:19:30.744+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=5732 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T19:19:30.744+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T19:19:30.745+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T19:19:30.746+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb2 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T19:19:30.746+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T19:19:30.787+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064770:786999][5732:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T19:19:30.881+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064770:881089][5732:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T19:19:30.974+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064770:974201][5732:140713979633712], txn-recover: Main recovery loop: starting at 1/93056 to 2/256
2019-09-21T19:19:30.975+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064770:975199][5732:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T19:19:31.070+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064771:70270][5732:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T19:19:31.150+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064771:149349][5732:140713979633712], txn-recover: Set global recovery timestamp: (1569064756,1)
2019-09-21T19:19:31.227+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569064756, 1)
2019-09-21T19:19:31.238+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T19:19:31.238+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 17 records totaling to 1882 bytes
2019-09-21T19:19:31.238+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T19:19:31.249+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T19:19:31.260+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:19:31.295+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T19:19:31.296+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T19:19:31.297+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T19:19:31.297+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T19:19:31.298+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T19:19:31.436+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T19:19:31.438+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T19:19:31.438+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T19:19:31.439+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T19:19:31.439+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569064756, 1) (top of oplog: { ts: Timestamp(1569064756, 1), t: 1 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T19:19:31.439+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569064756, 1)
2019-09-21T19:19:31.439+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T19:19:31.439+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T19:19:31.439+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T19:19:31.441+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:19:31.441+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T19:19:31.441+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T19:19:31.441+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T19:19:31.441+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:19:31.447+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:19:31.447+0800 I  REPL     [replexec-0] This node is localhost:37018 in the config
2019-09-21T19:19:31.447+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T19:19:31.447+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T19:19:31.447+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:19:31.449+0800 I  REPL     [replexec-1] Member localhost:37017 is now in state RECOVERING
2019-09-21T19:19:31.449+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T19:19:31.449+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T19:19:31.450+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T19:19:31.450+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T19:19:31.450+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T19:19:31.450+0800 I  REPL     [rsBackgroundSync] waiting for 1 pings from other members before syncing
2019-09-21T19:19:31.450+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T19:19:31.450+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T19:19:31.489+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61150 #3 (1 connection now open)
2019-09-21T19:19:31.489+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61150 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:31.950+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state SECONDARY
2019-09-21T19:19:41.548+0800 I  ELECTION [replexec-0] Starting an election, since we've seen no PRIMARY in the past 10000ms
2019-09-21T19:19:41.548+0800 I  ELECTION [replexec-0] conducting a dry run election to see if we could be elected. current term: 2
2019-09-21T19:19:41.548+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 22 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064756, 1), t: 1 } }
2019-09-21T19:19:41.548+0800 I  ELECTION [replexec-1] VoteRequester(term 2 dry run) received a yes vote from localhost:37017; response message: { term: 2, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569064756, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569064756, 1) }
2019-09-21T19:19:41.548+0800 I  ELECTION [replexec-1] dry election run succeeded, running for election in term 3
2019-09-21T19:19:41.554+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 23 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 3, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064756, 1), t: 1 } }
2019-09-21T19:19:41.560+0800 I  ELECTION [replexec-1] VoteRequester(term 3) received a yes vote from localhost:37017; response message: { term: 3, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569064756, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569064756, 1) }
2019-09-21T19:19:41.560+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 3
2019-09-21T19:19:41.560+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2019-09-21T19:19:41.560+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
2019-09-21T19:19:41.560+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2019-09-21T19:19:41.561+0800 I  REPL     [replexec-2] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1569064756, 1), t: 1 }. My Last Applied: { ts: Timestamp(1569064756, 1), t: 1 }
2019-09-21T19:19:41.561+0800 I  REPL     [replexec-2] Exited primary catch-up mode.
2019-09-21T19:19:41.561+0800 I  REPL     [replexec-2] Stopping replication producer
2019-09-21T19:19:42.454+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:19:42.454+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:19:43.454+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:19:43.454+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:19:43.455+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T19:19:43.455+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T19:19:44.498+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61154 #4 (2 connections now open)
2019-09-21T19:19:44.499+0800 I  NETWORK  [conn4] received client metadata from 127.0.0.1:61154 conn4: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:44.508+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61155 #5 (3 connections now open)
2019-09-21T19:19:44.508+0800 I  NETWORK  [conn5] received client metadata from 127.0.0.1:61155 conn5: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:48.219+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61156 #6 (4 connections now open)
2019-09-21T19:19:48.220+0800 I  NETWORK  [conn6] received client metadata from 127.0.0.1:61156 conn6: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:19:48.222+0800 I  SHARDING [conn6] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T19:19:48.223+0800 I  ACCESS   [conn6] SASL SCRAM-SHA-1 authentication failed for user1 on admim from client 127.0.0.1:61156 ; UserNotFound: Could not find user "user1" for db "admim"
2019-09-21T19:19:48.224+0800 I  ACCESS   [conn6] SASL SCRAM-SHA-1 authentication failed for user1 on test from client 127.0.0.1:61156 ; UserNotFound: Could not find user "user1" for db "test"
2019-09-21T19:22:29.104+0800 I  NETWORK  [conn5] end connection 127.0.0.1:61155 (3 connections now open)
2019-09-21T19:22:29.105+0800 I  NETWORK  [conn3] end connection 127.0.0.1:61150 (2 connections now open)
2019-09-21T19:22:29.585+0800 W  NETWORK  [replexec-3] Failed to check socket connectivity: 操作成功完成。
2019-09-21T19:22:29.585+0800 I  CONNPOOL [replexec-3] dropping unhealthy pooled connection to localhost:37017
2019-09-21T19:22:29.585+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:22:30.588+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:22:31.006+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61260 #7 (3 connections now open)
2019-09-21T19:22:31.007+0800 I  ACCESS   [conn7] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61260 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:31.007+0800 I  NETWORK  [conn7] end connection 127.0.0.1:61260 (2 connections now open)
2019-09-21T19:22:31.008+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61261 #8 (3 connections now open)
2019-09-21T19:22:31.009+0800 I  NETWORK  [conn8] received client metadata from 127.0.0.1:61261 conn8: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:31.009+0800 I  ACCESS   [conn8] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61261 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:31.010+0800 I  NETWORK  [conn8] end connection 127.0.0.1:61261 (2 connections now open)
2019-09-21T19:22:31.090+0800 I  REPL     [replexec-4] Member localhost:37017 is now in state RS_UNKNOWN due to authentication issue.
2019-09-21T19:22:31.511+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61262 #10 (3 connections now open)
2019-09-21T19:22:31.511+0800 I  NETWORK  [conn10] received client metadata from 127.0.0.1:61262 conn10: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:31.512+0800 I  ACCESS   [conn10] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61262 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:31.512+0800 I  NETWORK  [conn10] end connection 127.0.0.1:61262 (2 connections now open)
2019-09-21T19:22:31.694+0800 I  CONTROL  [thread8] Ctrl-C signal
2019-09-21T19:22:31.694+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T19:22:31.694+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:22:31.694+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:22:32.009+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61263 #11 (3 connections now open)
2019-09-21T19:22:32.009+0800 I  NETWORK  [conn11] received client metadata from 127.0.0.1:61263 conn11: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:32.010+0800 I  ACCESS   [conn11] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61263 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:32.011+0800 I  NETWORK  [conn11] end connection 127.0.0.1:61263 (2 connections now open)
2019-09-21T19:22:32.013+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61264 #12 (3 connections now open)
2019-09-21T19:22:32.014+0800 I  NETWORK  [conn12] received client metadata from 127.0.0.1:61264 conn12: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:32.014+0800 I  ACCESS   [conn12] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61264 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:32.014+0800 I  NETWORK  [conn12] end connection 127.0.0.1:61264 (2 connections now open)
2019-09-21T19:22:32.515+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61265 #13 (3 connections now open)
2019-09-21T19:22:32.516+0800 I  NETWORK  [conn13] received client metadata from 127.0.0.1:61265 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:32.516+0800 I  ACCESS   [conn13] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61265 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:32.517+0800 I  NETWORK  [conn13] end connection 127.0.0.1:61265 (2 connections now open)
2019-09-21T19:22:33.009+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61267 #14 (3 connections now open)
2019-09-21T19:22:33.009+0800 I  NETWORK  [conn14] received client metadata from 127.0.0.1:61267 conn14: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:33.010+0800 I  ACCESS   [conn14] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61267 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:33.010+0800 I  NETWORK  [conn14] end connection 127.0.0.1:61267 (2 connections now open)
2019-09-21T19:22:33.018+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61268 #15 (3 connections now open)
2019-09-21T19:22:33.018+0800 I  NETWORK  [conn15] received client metadata from 127.0.0.1:61268 conn15: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:33.018+0800 I  ACCESS   [conn15] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61268 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:33.019+0800 I  NETWORK  [conn15] end connection 127.0.0.1:61268 (2 connections now open)
2019-09-21T19:22:33.483+0800 I  NETWORK  [conn4] Error sending response to client: HostUnreachable: Connection reset by peer. Ending connection from 127.0.0.1:61154 (connection id: 4)
2019-09-21T19:22:33.483+0800 I  NETWORK  [conn4] end connection 127.0.0.1:61154 (1 connection now open)
2019-09-21T19:22:33.520+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61270 #16 (2 connections now open)
2019-09-21T19:22:33.521+0800 I  NETWORK  [conn16] received client metadata from 127.0.0.1:61270 conn16: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:33.521+0800 I  ACCESS   [conn16] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61270 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:33.522+0800 I  NETWORK  [conn16] end connection 127.0.0.1:61270 (1 connection now open)
2019-09-21T19:22:34.008+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61272 #17 (2 connections now open)
2019-09-21T19:22:34.009+0800 I  NETWORK  [conn17] received client metadata from 127.0.0.1:61272 conn17: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:34.009+0800 I  ACCESS   [conn17] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61272 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:34.010+0800 I  NETWORK  [conn17] end connection 127.0.0.1:61272 (1 connection now open)
2019-09-21T19:22:34.023+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61273 #18 (2 connections now open)
2019-09-21T19:22:34.023+0800 I  NETWORK  [conn18] received client metadata from 127.0.0.1:61273 conn18: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:34.024+0800 I  ACCESS   [conn18] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61273 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:34.024+0800 I  NETWORK  [conn18] end connection 127.0.0.1:61273 (1 connection now open)
2019-09-21T19:22:34.525+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61275 #19 (2 connections now open)
2019-09-21T19:22:34.526+0800 I  NETWORK  [conn19] received client metadata from 127.0.0.1:61275 conn19: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:34.526+0800 I  ACCESS   [conn19] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61275 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:34.526+0800 I  NETWORK  [conn19] end connection 127.0.0.1:61275 (1 connection now open)
2019-09-21T19:22:35.009+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61276 #20 (2 connections now open)
2019-09-21T19:22:35.010+0800 I  NETWORK  [conn20] received client metadata from 127.0.0.1:61276 conn20: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:35.010+0800 I  ACCESS   [conn20] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61276 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:35.011+0800 I  NETWORK  [conn20] end connection 127.0.0.1:61276 (1 connection now open)
2019-09-21T19:22:35.027+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61277 #21 (2 connections now open)
2019-09-21T19:22:35.028+0800 I  NETWORK  [conn21] received client metadata from 127.0.0.1:61277 conn21: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:35.028+0800 I  ACCESS   [conn21] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61277 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:35.028+0800 I  NETWORK  [conn21] end connection 127.0.0.1:61277 (1 connection now open)
2019-09-21T19:22:35.529+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61278 #22 (2 connections now open)
2019-09-21T19:22:35.529+0800 I  NETWORK  [conn22] received client metadata from 127.0.0.1:61278 conn22: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:35.530+0800 I  ACCESS   [conn22] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61278 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:35.530+0800 I  NETWORK  [conn22] end connection 127.0.0.1:61278 (1 connection now open)
2019-09-21T19:22:36.010+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61279 #23 (2 connections now open)
2019-09-21T19:22:36.011+0800 I  NETWORK  [conn23] received client metadata from 127.0.0.1:61279 conn23: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:36.011+0800 I  ACCESS   [conn23] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61279 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:36.011+0800 I  NETWORK  [conn23] end connection 127.0.0.1:61279 (1 connection now open)
2019-09-21T19:22:36.031+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61280 #24 (2 connections now open)
2019-09-21T19:22:36.031+0800 I  NETWORK  [conn24] received client metadata from 127.0.0.1:61280 conn24: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:36.032+0800 I  ACCESS   [conn24] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61280 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:36.032+0800 I  NETWORK  [conn24] end connection 127.0.0.1:61280 (1 connection now open)
2019-09-21T19:22:36.533+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61281 #25 (2 connections now open)
2019-09-21T19:22:36.534+0800 I  NETWORK  [conn25] received client metadata from 127.0.0.1:61281 conn25: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:36.534+0800 I  ACCESS   [conn25] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61281 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:36.534+0800 I  NETWORK  [conn25] end connection 127.0.0.1:61281 (1 connection now open)
2019-09-21T19:22:37.011+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61282 #26 (2 connections now open)
2019-09-21T19:22:37.011+0800 I  NETWORK  [conn26] received client metadata from 127.0.0.1:61282 conn26: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:37.012+0800 I  ACCESS   [conn26] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61282 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:37.012+0800 I  NETWORK  [conn26] end connection 127.0.0.1:61282 (1 connection now open)
2019-09-21T19:22:37.036+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61283 #27 (2 connections now open)
2019-09-21T19:22:37.036+0800 I  NETWORK  [conn27] received client metadata from 127.0.0.1:61283 conn27: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:37.036+0800 I  ACCESS   [conn27] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61283 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:37.037+0800 I  NETWORK  [conn27] end connection 127.0.0.1:61283 (1 connection now open)
2019-09-21T19:22:37.538+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61284 #28 (2 connections now open)
2019-09-21T19:22:37.538+0800 I  NETWORK  [conn28] received client metadata from 127.0.0.1:61284 conn28: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:37.538+0800 I  ACCESS   [conn28] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61284 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:37.539+0800 I  NETWORK  [conn28] end connection 127.0.0.1:61284 (1 connection now open)
2019-09-21T19:22:38.010+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61285 #29 (2 connections now open)
2019-09-21T19:22:38.011+0800 I  NETWORK  [conn29] received client metadata from 127.0.0.1:61285 conn29: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:38.011+0800 I  ACCESS   [conn29] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61285 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:38.012+0800 I  NETWORK  [conn29] end connection 127.0.0.1:61285 (1 connection now open)
2019-09-21T19:22:38.040+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61286 #30 (2 connections now open)
2019-09-21T19:22:38.041+0800 I  NETWORK  [conn30] received client metadata from 127.0.0.1:61286 conn30: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:38.041+0800 I  ACCESS   [conn30] SASL SCRAM-SHA-1 authentication failed for __system on local from client 127.0.0.1:61286 ; AuthenticationFailed: It is not possible to authenticate as the __system user on servers started without a --keyFile parameter
2019-09-21T19:22:38.042+0800 I  NETWORK  [conn30] end connection 127.0.0.1:61286 (1 connection now open)
2019-09-21T19:22:38.536+0800 I  REPL     [replexec-4] Member localhost:37017 is now in state RS_DOWN - no response within election timeout period
2019-09-21T19:22:38.536+0800 I  REPL     [replexec-4] can't see a majority of the set, relinquishing primary
2019-09-21T19:22:38.536+0800 I  REPL     [replexec-4] Stepping down from primary in response to heartbeat
2019-09-21T19:22:38.536+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:22:38.536+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:22:38.537+0800 I  REPL     [replexec-4] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 0 }
2019-09-21T19:22:38.537+0800 I  REPL     [replexec-4] transition to SECONDARY from PRIMARY
2019-09-21T19:22:38.537+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:22:38.537+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:22:38.537+0800 I  STORAGE  [consoleTerminate] Failed to stepDown in non-command initiated shutdown path PrimarySteppedDown: While waiting for secondaries to catch up before stepping down, this node decided to step down for other reasons
2019-09-21T19:22:38.538+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T19:22:38.538+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T19:22:38.538+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T19:22:38.538+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T19:22:38.538+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T19:22:38.538+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T19:22:38.538+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T19:22:38.543+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T19:22:38.543+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T19:22:38.543+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:22:38.543+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T19:22:38.544+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T19:22:38.544+0800 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:37017 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T19:22:38.544+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T19:22:38.544+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T19:22:38.547+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T19:22:38.547+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T19:22:38.547+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T19:22:38.547+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T19:22:38.547+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T19:22:38.547+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T19:22:38.547+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T19:22:38.634+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T19:22:38.634+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T19:22:38.634+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T19:22:38.672+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T19:22:38.672+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T19:22:38.673+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T19:22:38.728+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T19:22:39.170+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=4848 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T19:22:39.225+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T19:22:39.227+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb2 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T19:22:39.227+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T19:22:39.269+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064959:269317][4848:140713979633712], txn-recover: Recovering log 2 through 3
2019-09-21T19:22:39.369+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064959:369412][4848:140713979633712], txn-recover: Recovering log 3 through 3
2019-09-21T19:22:39.475+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064959:475516][4848:140713979633712], txn-recover: Main recovery loop: starting at 2/12800 to 3/256
2019-09-21T19:22:39.476+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064959:475516][4848:140713979633712], txn-recover: Recovering log 2 through 3
2019-09-21T19:22:39.582+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064959:581617][4848:140713979633712], txn-recover: Recovering log 3 through 3
2019-09-21T19:22:39.683+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064959:682712][4848:140713979633712], txn-recover: Set global recovery timestamp: (1569064943,1)
2019-09-21T19:22:39.724+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569064943, 1)
2019-09-21T19:22:39.737+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T19:22:39.737+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 34 records totaling to 3750 bytes
2019-09-21T19:22:39.737+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T19:22:39.747+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T19:22:39.757+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:22:39.757+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T19:22:39.757+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T19:22:39.757+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T19:22:39.757+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T19:22:39.757+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T19:22:39.757+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T19:22:39.757+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:22:39.797+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T19:22:39.798+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T19:22:39.798+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T19:22:39.798+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T19:22:39.799+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T19:22:39.951+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T19:22:39.952+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T19:22:39.953+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T19:22:39.953+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T19:22:39.954+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569064943, 1) (top of oplog: { ts: Timestamp(1569064943, 1), t: 3 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T19:22:39.954+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569064943, 1)
2019-09-21T19:22:39.954+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T19:22:39.954+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T19:22:39.954+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T19:22:39.955+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:22:39.956+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T19:22:39.956+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T19:22:39.956+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T19:22:39.956+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:22:39.999+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:22:39.999+0800 I  REPL     [replexec-0] This node is localhost:37018 in the config
2019-09-21T19:22:39.999+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T19:22:40.000+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T19:22:40.000+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:22:40.005+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T19:22:40.005+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T19:22:40.005+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T19:22:40.006+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T19:22:40.006+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T19:22:40.006+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2019-09-21T19:22:40.006+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T19:22:40.006+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T19:22:40.017+0800 I  REPL     [replexec-1] Member localhost:37017 is now in state RECOVERING
2019-09-21T19:22:40.096+0800 F  -        [thread1] Got signal: 22 (SIGABRT).
mongod.exe      ...\src\mongo\util\stacktrace_windows.cpp(246)           mongo::printStackTrace+0x43
mongod.exe      ...\src\mongo\util\signal_handlers_synchronous.cpp(239)  mongo::`anonymous namespace'::abruptQuit+0x81
ucrtbase.dll                                                             raise+0x1e8
ucrtbase.dll                                                             abort+0x31
mongod.exe      ...\src\mongo\util\assert_util.cpp(104)                  mongo::invariantFailed+0xde
mongod.exe      ...\src\mongo\util\concurrency\thread_name.cpp(117)      mongo::setThreadName+0x141
mongod.exe      ...\src\mongo\util\signal_handlers.cpp(96)               mongo::`anonymous namespace'::consoleTerminate+0x4a
mongod.exe      ...\src\mongo\util\signal_handlers.cpp(104)              mongo::`anonymous namespace'::CtrlHandler+0xbb
KERNELBASE.dll                                                           CtrlRoutine+0xb4
KERNEL32.DLL                                                             BaseThreadInitThunk+0x22
2019-09-21T19:22:40.096+0800 F  CONTROL  [thread1] *** unhandled exception 0x0000000E at 0x00007FFA8439A1C8, terminating
2019-09-21T19:22:40.096+0800 F  CONTROL  [thread1] *** stack trace for unhandled exception:
2019-09-21T19:22:40.104+0800 I  -        [thread1] KERNELBASE.dll                                                           RaiseException+0x68
mongod.exe      ...\src\mongo\util\signal_handlers_synchronous.cpp(241)  mongo::`anonymous namespace'::abruptQuit+0x9e
ucrtbase.dll                                                             raise+0x1e8
ucrtbase.dll                                                             abort+0x31
mongod.exe      ...\src\mongo\util\assert_util.cpp(104)                  mongo::invariantFailed+0xde
mongod.exe      ...\src\mongo\util\concurrency\thread_name.cpp(117)      mongo::setThreadName+0x141
mongod.exe      ...\src\mongo\util\signal_handlers.cpp(96)               mongo::`anonymous namespace'::consoleTerminate+0x4a
mongod.exe      ...\src\mongo\util\signal_handlers.cpp(104)              mongo::`anonymous namespace'::CtrlHandler+0xbb
KERNELBASE.dll                                                           CtrlRoutine+0xb4
KERNEL32.DLL                                                             BaseThreadInitThunk+0x22
2019-09-21T19:22:40.105+0800 I  CONTROL  [thread1] failed to open minidump file C:\Program Files\MongoDB\Server\4.2019-09-21T11-22-40.mdmp : 拒绝访问。
2019-09-21T19:22:40.105+0800 F  CONTROL  [thread1] *** immediate exit due to unhandled exception
2019-09-21T19:22:41.586+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T19:22:42.042+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=3864 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T19:22:42.088+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T19:22:42.090+0800 W  STORAGE  [initandlisten] Detected unclean shutdown - C:\Users\xy\mongo\mdb2\mongod.lock is not empty.
2019-09-21T19:22:42.090+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb2 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T19:22:42.090+0800 W  STORAGE  [initandlisten] Recovering data from the last clean checkpoint.
2019-09-21T19:22:42.090+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T19:22:42.132+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064962:132364][3864:140713979633712], txn-recover: Recovering log 3 through 4
2019-09-21T19:22:42.214+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064962:213446][3864:140713979633712], txn-recover: Recovering log 4 through 4
2019-09-21T19:22:42.309+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064962:308559][3864:140713979633712], txn-recover: Main recovery loop: starting at 3/256 to 4/256
2019-09-21T19:22:42.310+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064962:309560][3864:140713979633712], txn-recover: Recovering log 3 through 4
2019-09-21T19:22:42.408+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064962:408633][3864:140713979633712], file:index-1-9108990645418486328.wt, txn-recover: Recovering log 4 through 4
2019-09-21T19:22:42.491+0800 I  STORAGE  [initandlisten] WiredTiger message [1569064962:490713][3864:140713979633712], file:index-1-9108990645418486328.wt, txn-recover: Set global recovery timestamp: (1569064943,1)
2019-09-21T19:22:42.547+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569064943, 1)
2019-09-21T19:22:42.581+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T19:22:42.581+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 34 records totaling to 3750 bytes
2019-09-21T19:22:42.581+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T19:22:42.588+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T19:22:42.597+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:22:42.597+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T19:22:42.597+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T19:22:42.597+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T19:22:42.597+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T19:22:42.597+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T19:22:42.598+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T19:22:42.598+0800 I  CONTROL  [initandlisten] 
2019-09-21T19:22:42.631+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T19:22:42.632+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T19:22:42.632+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T19:22:42.632+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T19:22:42.633+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T19:22:42.761+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T19:22:42.763+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T19:22:42.763+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T19:22:42.763+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T19:22:42.764+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569064943, 1) (top of oplog: { ts: Timestamp(1569064943, 1), t: 3 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T19:22:42.764+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569064943, 1)
2019-09-21T19:22:42.764+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T19:22:42.764+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T19:22:42.764+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T19:22:42.766+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T19:22:42.766+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T19:22:42.766+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T19:22:42.766+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T19:22:42.766+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T19:22:42.804+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d860533c0ed0f5f188cbd19') } }
2019-09-21T19:22:42.804+0800 I  REPL     [replexec-0] This node is localhost:37018 in the config
2019-09-21T19:22:42.804+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T19:22:42.804+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T19:22:42.804+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:22:42.806+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T19:22:42.806+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T19:22:42.807+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T19:22:42.807+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T19:22:42.807+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T19:22:42.807+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2019-09-21T19:22:42.807+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T19:22:42.807+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T19:22:42.818+0800 I  REPL     [replexec-1] Member localhost:37017 is now in state SECONDARY
2019-09-21T19:22:43.011+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61297 #3 (1 connection now open)
2019-09-21T19:22:43.012+0800 I  SHARDING [conn3] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T19:22:43.012+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61297 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:43.023+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 127.0.0.1:61297
2019-09-21T19:22:48.891+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61300 #4 (2 connections now open)
2019-09-21T19:22:48.892+0800 I  NETWORK  [conn4] received client metadata from 127.0.0.1:61300 conn4: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:22:48.894+0800 I  ACCESS   [conn4] SASL SCRAM-SHA-1 authentication failed for user1 on admim from client 127.0.0.1:61300 ; UserNotFound: Could not find user "user1" for db "admim"
2019-09-21T19:22:48.895+0800 I  ACCESS   [conn4] SASL SCRAM-SHA-1 authentication failed for user1 on test from client 127.0.0.1:61300 ; UserNotFound: Could not find user "user1" for db "test"
2019-09-21T19:22:51.367+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 3, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064943, 1), t: 3 } }
2019-09-21T19:22:51.367+0800 I  ELECTION [conn3] Sending vote response: { term: 3, voteGranted: true, reason: "" }
2019-09-21T19:22:51.374+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 4, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569064943, 1), t: 3 } }
2019-09-21T19:22:51.374+0800 I  ELECTION [conn3] Sending vote response: { term: 4, voteGranted: true, reason: "" }
2019-09-21T19:22:51.825+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state PRIMARY
2019-09-21T19:22:53.814+0800 I  REPL     [rsBackgroundSync] sync source candidate: localhost:37017
2019-09-21T19:22:53.814+0800 I  CONNPOOL [RS] Connecting to localhost:37017
2019-09-21T19:22:53.829+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:37017
2019-09-21T19:22:53.830+0800 I  SHARDING [rsSync-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T19:22:55.983+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T19:23:13.332+0800 I  ACCESS   [conn4] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61300
2019-09-21T19:23:21.167+0800 I  SHARDING [conn4] Marking collection admin.test as collection version: <unsharded>
2019-09-21T19:23:47.630+0800 I  SHARDING [conn4] Marking collection test.c as collection version: <unsharded>
2019-09-21T19:26:22.774+0800 I  COMMAND  [conn3] Received replSetStepUp request
2019-09-21T19:26:22.774+0800 I  ELECTION [conn3] Starting an election due to step up request
2019-09-21T19:26:22.774+0800 I  ELECTION [conn3] skipping dry run and running for election in term 5
2019-09-21T19:26:22.780+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 257 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 5, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569065173, 1), t: 4 } }
2019-09-21T19:26:22.781+0800 I  ELECTION [replexec-1] VoteRequester(term 5) received an invalid response from localhost:37017: ShutdownInProgress: In the process of shutting down; response message: { operationTime: Timestamp(1569065173, 1), ok: 0.0, errmsg: "In the process of shutting down", code: 91, codeName: "ShutdownInProgress", $clusterTime: { clusterTime: Timestamp(1569065173, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
2019-09-21T19:26:22.781+0800 I  ELECTION [replexec-1] not becoming primary, we received insufficient votes
2019-09-21T19:26:22.781+0800 I  ELECTION [replexec-1] Lost election due to internal error
2019-09-21T19:26:22.781+0800 I  COMMAND  [conn3] replSetStepUp request failed :: caused by :: CommandFailed: Election failed.
2019-09-21T19:26:23.048+0800 I  REPL     [replication-1] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: localhost:37017, my last fetched oplog optime: { ts: Timestamp(1569065173, 1), t: 4 }, latest oplog optime of sync source: { ts: Timestamp(1569065173, 1), t: 4 } (sync source does not know the primary)
2019-09-21T19:26:23.048+0800 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: localhost:37017, OpTime { ts: Timestamp(1569065173, 1), t: 4 }, its sync source index:-1
2019-09-21T19:26:23.048+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source localhost:37017 (config version: 2; last applied optime: { ts: Timestamp(1569065173, 1), t: 4 }; sync source index: -1; primary index: -1) is no longer valid
2019-09-21T19:26:23.048+0800 I  REPL     [rsBackgroundSync] Clearing sync source localhost:37017 to choose a new one.
2019-09-21T19:26:23.048+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T19:26:23.049+0800 I  REPL     [replexec-1] Member localhost:37017 is now in state SECONDARY
2019-09-21T19:26:23.112+0800 I  NETWORK  [conn3] end connection 127.0.0.1:61297 (1 connection now open)
2019-09-21T19:26:23.549+0800 W  NETWORK  [replexec-0] Failed to check socket connectivity: 操作成功完成。
2019-09-21T19:26:23.549+0800 I  CONNPOOL [replexec-0] dropping unhealthy pooled connection to localhost:37017
2019-09-21T19:26:23.549+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:24.552+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:25.555+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:26.556+0800 I  REPL_HB  [replexec-2] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T19:26:26.557+0800 I  REPL     [replexec-2] Member localhost:37017 is now in state RS_DOWN - Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T19:26:26.818+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:27.819+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:28.047+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:37017: InvalidSyncSource: Sync source was cleared. Was localhost:37017
2019-09-21T19:26:28.820+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:29.822+0800 I  REPL_HB  [replexec-1] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T19:26:30.322+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:31.324+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:32.326+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:33.205+0800 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2019-09-21T19:26:33.328+0800 I  REPL_HB  [replexec-2] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T19:26:33.820+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:34.823+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:35.825+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:36.827+0800 I  REPL_HB  [replexec-2] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T19:26:37.327+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:38.328+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T19:26:39.178+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61374 #7 (2 connections now open)
2019-09-21T19:26:39.211+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 127.0.0.1:61374
2019-09-21T19:26:39.211+0800 I  NETWORK  [conn7] end connection 127.0.0.1:61374 (1 connection now open)
2019-09-21T19:26:39.213+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61375 #8 (2 connections now open)
2019-09-21T19:26:39.213+0800 I  NETWORK  [conn8] received client metadata from 127.0.0.1:61375 conn8: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:26:39.226+0800 I  ACCESS   [conn8] Successfully authenticated as principal __system on local from client 127.0.0.1:61375
2019-09-21T19:26:39.343+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state SECONDARY
2019-09-21T19:26:44.154+0800 I  ELECTION [replexec-1] Starting an election, since we've seen no PRIMARY in the past 10000ms
2019-09-21T19:26:44.154+0800 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 5
2019-09-21T19:26:44.154+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 284 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 5, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569065173, 1), t: 4 } }
2019-09-21T19:26:44.154+0800 I  ELECTION [replexec-2] VoteRequester(term 5 dry run) received a yes vote from localhost:37017; response message: { term: 5, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569065173, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569065173, 1) }
2019-09-21T19:26:44.155+0800 I  ELECTION [replexec-2] dry election run succeeded, running for election in term 6
2019-09-21T19:26:44.160+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 285 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 6, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569065173, 1), t: 4 } }
2019-09-21T19:26:44.165+0800 I  ELECTION [replexec-1] VoteRequester(term 6) received a yes vote from localhost:37017; response message: { term: 6, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569065173, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569065173, 1) }
2019-09-21T19:26:44.165+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 6
2019-09-21T19:26:44.165+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2019-09-21T19:26:44.165+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
2019-09-21T19:26:44.166+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2019-09-21T19:26:44.166+0800 I  REPL     [replexec-1] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1569065173, 1), t: 4 }. My Last Applied: { ts: Timestamp(1569065173, 1), t: 4 }
2019-09-21T19:26:44.166+0800 I  REPL     [replexec-1] Exited primary catch-up mode.
2019-09-21T19:26:44.166+0800 I  REPL     [replexec-1] Stopping replication producer
2019-09-21T19:26:45.053+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:26:45.053+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:26:46.054+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T19:26:46.054+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T19:26:46.055+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T19:26:47.222+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61377 #10 (3 connections now open)
2019-09-21T19:26:47.222+0800 I  NETWORK  [conn10] received client metadata from 127.0.0.1:61377 conn10: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:26:47.233+0800 I  ACCESS   [conn10] Successfully authenticated as principal __system on local from client 127.0.0.1:61377
2019-09-21T19:26:47.242+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61378 #11 (4 connections now open)
2019-09-21T19:26:47.242+0800 I  NETWORK  [conn11] received client metadata from 127.0.0.1:61378 conn11: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:26:47.254+0800 I  ACCESS   [conn11] Successfully authenticated as principal __system on local from client 127.0.0.1:61378
2019-09-21T19:27:23.047+0800 I  CONNPOOL [RS] Ending idle connection to host localhost:37017 because the pool meets constraints; 1 connections to that host remain open
2019-09-21T19:27:23.842+0800 I  CONNPOOL [RS] Connecting to localhost:37017
2019-09-21T19:31:23.048+0800 I  CONNPOOL [RS] Dropping all pooled connections to localhost:37017 due to ShutdownInProgress: Pool for localhost:37017 has expired.
2019-09-21T19:31:39.177+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61409 #13 (5 connections now open)
2019-09-21T19:31:39.178+0800 I  NETWORK  [conn13] received client metadata from 127.0.0.1:61409 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:31:39.199+0800 I  ACCESS   [conn13] Successfully authenticated as principal __system on local from client 127.0.0.1:61409
2019-09-21T19:31:39.200+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61412 #14 (6 connections now open)
2019-09-21T19:31:39.201+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61411 #15 (7 connections now open)
2019-09-21T19:31:39.201+0800 I  NETWORK  [conn15] received client metadata from 127.0.0.1:61411 conn15: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:31:39.210+0800 I  NETWORK  [conn14] received client metadata from 127.0.0.1:61412 conn14: { driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T19:31:39.244+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:31:39.250+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:31:39.284+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:31:39.331+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:31:39.363+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:36:39.211+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:36:39.211+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:36:39.243+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:36:39.276+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:36:39.309+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:41:39.211+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:41:39.211+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:41:39.244+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:41:39.276+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:41:39.308+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:46:39.212+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:46:39.212+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:46:39.243+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:46:39.275+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:46:39.308+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:51:39.211+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:51:39.212+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:51:39.244+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:51:39.278+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:51:39.310+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:56:39.211+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:56:39.211+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T19:56:39.243+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:56:39.275+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T19:56:39.307+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:01:39.211+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T20:01:39.211+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:01:39.243+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T20:01:39.275+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T20:01:39.307+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T20:06:39.211+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:06:39.212+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T20:06:39.243+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:06:39.276+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:06:39.307+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:11:39.213+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:11:39.213+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61412
2019-09-21T20:11:39.245+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:11:39.279+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:11:39.311+0800 I  ACCESS   [conn15] Successfully authenticated as principal __system on local from client 127.0.0.1:61411
2019-09-21T20:13:38.316+0800 I  NETWORK  [conn4] end connection 127.0.0.1:61300 (6 connections now open)
2019-09-21T20:13:45.450+0800 I  NETWORK  [conn10] end connection 127.0.0.1:61377 (5 connections now open)
2019-09-21T20:13:45.451+0800 I  NETWORK  [conn8] end connection 127.0.0.1:61375 (4 connections now open)
2019-09-21T20:13:45.452+0800 I  NETWORK  [conn13] end connection 127.0.0.1:61409 (3 connections now open)
2019-09-21T20:13:45.564+0800 I  NETWORK  [conn14] end connection 127.0.0.1:61412 (2 connections now open)
2019-09-21T20:13:45.564+0800 I  NETWORK  [conn15] end connection 127.0.0.1:61411 (1 connection now open)
2019-09-21T20:13:46.319+0800 I  NETWORK  [conn11] Error sending response to client: HostUnreachable: Connection reset by peer. Ending connection from 127.0.0.1:61378 (connection id: 11)
2019-09-21T20:13:46.319+0800 I  NETWORK  [conn11] end connection 127.0.0.1:61378 (0 connections now open)
2019-09-21T20:13:46.507+0800 W  NETWORK  [replexec-2] Failed to check socket connectivity: 操作成功完成。
2019-09-21T20:13:46.508+0800 I  CONNPOOL [replexec-2] dropping unhealthy pooled connection to localhost:37017
2019-09-21T20:13:46.508+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:13:47.208+0800 I  CONTROL  [thread10] Ctrl-C signal
2019-09-21T20:13:47.208+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:13:47.208+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:13:47.208+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:13:47.510+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:13:49.512+0800 I  REPL_HB  [replexec-2] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:13:49.513+0800 I  REPL     [replexec-2] Member localhost:37017 is now in state RS_DOWN - Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:13:50.007+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:13:51.513+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:13:52.514+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:13:53.515+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:13:54.507+0800 I  REPL     [replexec-2] can't see a majority of the set, relinquishing primary
2019-09-21T20:13:54.507+0800 I  REPL     [replexec-2] Stepping down from primary in response to heartbeat
2019-09-21T20:13:54.507+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:13:54.507+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:13:54.507+0800 I  REPL     [replexec-2] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 0 }
2019-09-21T20:13:54.507+0800 I  REPL     [replexec-2] transition to SECONDARY from PRIMARY
2019-09-21T20:13:54.508+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:13:54.508+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:13:54.508+0800 I  STORAGE  [consoleTerminate] Failed to stepDown in non-command initiated shutdown path PrimarySteppedDown: While waiting for secondaries to catch up before stepping down, this node decided to step down for other reasons
2019-09-21T20:13:54.508+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:13:54.509+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:13:54.509+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:13:54.509+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:13:54.509+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:13:54.509+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:13:54.509+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:13:54.516+0800 I  REPL_HB  [replexec-3] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:13:54.960+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:13:54.961+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:13:54.961+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:13:54.961+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:13:54.962+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:13:54.962+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:13:54.962+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:13:54.966+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:13:54.966+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:13:54.966+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:13:54.966+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:13:54.972+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:13:54.972+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:13:54.972+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:13:55.018+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:13:55.018+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:13:55.018+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:13:55.077+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:13:55.077+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:13:55.077+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:15:05.618+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:15:06.085+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=1340 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:15:06.088+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T20:15:06.090+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:15:06.149+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068106:148705][1340:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:15:06.174+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:15:06.207+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T20:15:06.225+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:15:06.228+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:15:06.228+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:15:06.229+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:15:06.229+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:15:06.229+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 4b0e4a6f-d2e2-4632-ad12-e8a1473f6e5a and options: { capped: true, size: 10485760 }
2019-09-21T20:15:06.254+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2019-09-21T20:15:06.254+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:15:06.388+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T20:15:06.389+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: b9dbce1e-d4dc-4fa5-9d77-cf0631039fc7 and options: {}
2019-09-21T20:15:06.416+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2019-09-21T20:15:06.416+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 6579ee2b-b03f-4a72-85c1-8b682e4113b3 and options: {}
2019-09-21T20:15:06.440+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2019-09-21T20:15:06.440+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:15:06.440+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 1569c40e-a40f-4df0-a34e-6fef9dd5d50a and options: {}
2019-09-21T20:15:06.466+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2019-09-21T20:15:06.466+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:15:06.466+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:15:06.466+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2019-09-21T20:15:06.466+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 90b10999-d520-4433-b275-57d4e27adf1c and options: {}
2019-09-21T20:15:06.490+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2019-09-21T20:15:06.490+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:15:06.490+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2019-09-21T20:15:06.490+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:15:06.491+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:15:06.492+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:15:06.492+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T20:15:06.492+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:15:06.492+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T20:15:07.001+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:15:42.997+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61538 #1 (1 connection now open)
2019-09-21T20:15:43.000+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61538 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:16:08.672+0800 I  CONTROL  [thread2] Ctrl-C signal
2019-09-21T20:16:08.672+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:16:08.672+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:16:08.672+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:16:08.673+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:16:08.673+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:16:08.673+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:16:08.673+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:16:08.675+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:16:08.676+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:16:08.676+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:16:08.676+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:16:08.676+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:16:08.676+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:16:08.717+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:16:08.717+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:16:08.717+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:16:08.718+0800 I  STORAGE  [consoleTerminate] Downgrading WiredTiger datafiles.
2019-09-21T20:16:08.795+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068168:794668][1340:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T20:16:08.887+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068168:887743][1340:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T20:16:08.977+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068168:977828][1340:140713979633712], txn-recover: Main recovery loop: starting at 1/23168 to 2/256
2019-09-21T20:16:09.133+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068169:132977][1340:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T20:16:09.224+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068169:224063][1340:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T20:16:09.303+0800 I  STORAGE  [consoleTerminate] WiredTiger message [1569068169:303140][1340:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:16:09.409+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:16:09.409+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:16:09.409+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:16:09.456+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:16:09.687+0800 I  CONTROL  [main] Ctrl-C signal
2019-09-21T20:16:09.687+0800 F  -        [main] Invariant failure mongoInitializersHaveRun src\mongo\util\concurrency\thread_name.cpp 117
2019-09-21T20:16:09.687+0800 F  -        [main] 

***aborting after invariant() failure


2019-09-21T20:16:09.914+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=3916 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:16:09.920+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T20:16:09.922+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb2 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T20:16:09.923+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:16:09.964+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068169:964556][3916:140713979633712], txn-recover: Recovering log 2 through 3
2019-09-21T20:16:10.041+0800 F  -        [thread1] Got signal: 22 (SIGABRT).
mongod.exe      ...\src\mongo\util\stacktrace_windows.cpp(246)           mongo::printStackTrace+0x43
mongod.exe      ...\src\mongo\util\signal_handlers_synchronous.cpp(239)  mongo::`anonymous namespace'::abruptQuit+0x81
ucrtbase.dll                                                             raise+0x1e8
ucrtbase.dll                                                             abort+0x31
mongod.exe      ...\src\mongo\util\assert_util.cpp(104)                  mongo::invariantFailed+0xde
mongod.exe      ...\src\mongo\util\concurrency\thread_name.cpp(117)      mongo::setThreadName+0x141
mongod.exe      ...\src\mongo\util\signal_handlers.cpp(96)               mongo::`anonymous namespace'::consoleTerminate+0x4a
mongod.exe      ...\src\mongo\util\signal_handlers.cpp(104)              mongo::`anonymous namespace'::CtrlHandler+0xbb
KERNELBASE.dll                                                           CtrlRoutine+0xb4
KERNEL32.DLL                                                             BaseThreadInitThunk+0x22
2019-09-21T20:16:10.041+0800 F  CONTROL  [thread1] *** unhandled exception 0x0000000E at 0x00007FFA8439A1C8, terminating
2019-09-21T20:16:10.041+0800 F  CONTROL  [thread1] *** stack trace for unhandled exception:
2019-09-21T20:16:10.055+0800 I  -        [thread1] KERNELBASE.dll                                                           RaiseException+0x68
mongod.exe      ...\src\mongo\util\signal_handlers_synchronous.cpp(241)  mongo::`anonymous namespace'::abruptQuit+0x9e
ucrtbase.dll                                                             raise+0x1e8
ucrtbase.dll                                                             abort+0x31
mongod.exe      ...\src\mongo\util\assert_util.cpp(104)                  mongo::invariantFailed+0xde
mongod.exe      ...\src\mongo\util\concurrency\thread_name.cpp(117)      mongo::setThreadName+0x141
mongod.exe      ...\src\mongo\util\signal_handlers.cpp(96)               mongo::`anonymous namespace'::consoleTerminate+0x4a
mongod.exe      ...\src\mongo\util\signal_handlers.cpp(104)              mongo::`anonymous namespace'::CtrlHandler+0xbb
KERNELBASE.dll                                                           CtrlRoutine+0xb4
KERNEL32.DLL                                                             BaseThreadInitThunk+0x22
2019-09-21T20:16:10.055+0800 I  CONTROL  [thread1] failed to open minidump file C:\Program Files\MongoDB\Server\4.2019-09-21T12-16-10.mdmp : 拒绝访问。
2019-09-21T20:16:10.055+0800 F  CONTROL  [thread1] *** immediate exit due to unhandled exception
2019-09-21T20:16:29.755+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:16:29.756+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=2140 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:16:30.253+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T20:16:30.254+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb2 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T20:16:30.255+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:16:30.293+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068190:293136][2140:140713979633712], txn-recover: Recovering log 2 through 4
2019-09-21T20:16:30.372+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068190:372214][2140:140713979633712], txn-recover: Recovering log 3 through 4
2019-09-21T20:16:30.466+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068190:465325][2140:140713979633712], txn-recover: Recovering log 4 through 4
2019-09-21T20:16:30.560+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068190:560394][2140:140713979633712], txn-recover: Main recovery loop: starting at 2/2048 to 4/256
2019-09-21T20:16:30.814+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068190:814638][2140:140713979633712], txn-recover: Recovering log 2 through 4
2019-09-21T20:16:30.906+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068190:905724][2140:140713979633712], txn-recover: Recovering log 3 through 4
2019-09-21T20:16:31.000+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068190:999816][2140:140713979633712], txn-recover: Recovering log 4 through 4
2019-09-21T20:16:31.082+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068191:81916][2140:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:16:31.157+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:16:31.166+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:16:31.176+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:16:31.176+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T20:16:31.176+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T20:16:31.176+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T20:16:31.176+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T20:16:31.176+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T20:16:31.176+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T20:16:31.176+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:16:31.193+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:16:31.193+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:16:31.193+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:16:31.194+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:16:31.194+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:16:31.323+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T20:16:31.324+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:16:31.324+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:16:31.324+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:16:31.325+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T20:16:31.325+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:16:31.326+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:16:31.326+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:16:31.326+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T20:16:31.326+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:16:31.326+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T20:16:32.002+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:16:42.430+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61544 #1 (1 connection now open)
2019-09-21T20:16:42.431+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:16:42.431+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T20:16:42.431+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61544 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:09.728+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61549 #2 (2 connections now open)
2019-09-21T20:19:09.761+0800 I  ACCESS   [conn2] Successfully authenticated as principal __system on local from client 127.0.0.1:61549
2019-09-21T20:19:09.762+0800 I  NETWORK  [conn2] end connection 127.0.0.1:61549 (1 connection now open)
2019-09-21T20:19:09.763+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61550 #3 (2 connections now open)
2019-09-21T20:19:09.763+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61550 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:19:09.795+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 127.0.0.1:61550
2019-09-21T20:19:09.796+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:19:09.851+0800 I  STORAGE  [replexec-1] createCollection: local.system.replset with generated UUID: eb846e7d-fa81-45a5-a557-3400f9b3ea90 and options: {}
2019-09-21T20:19:09.879+0800 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.system.replset
2019-09-21T20:19:09.879+0800 I  REPL     [replexec-1] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d8614b7a12331c3a99b2c9e') } }
2019-09-21T20:19:09.879+0800 I  REPL     [replexec-1] This node is localhost:37018 in the config
2019-09-21T20:19:09.879+0800 I  REPL     [replexec-1] transition to STARTUP2 from STARTUP
2019-09-21T20:19:09.880+0800 I  REPL     [replexec-1] Starting replication storage threads
2019-09-21T20:19:09.880+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state PRIMARY
2019-09-21T20:19:09.886+0800 I  STORAGE  [replexec-1] createCollection: local.temp_oplog_buffer with generated UUID: 08ef907e-b845-486e-a7b9-98e27b2d4606 and options: { temp: true }
2019-09-21T20:19:09.913+0800 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.temp_oplog_buffer
2019-09-21T20:19:09.913+0800 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
2019-09-21T20:19:09.913+0800 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (08ef907e-b845-486e-a7b9-98e27b2d4606).
2019-09-21T20:19:09.923+0800 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: 295879de-f779-4b14-a0ca-22eba3977580 and options: { temp: true }
2019-09-21T20:19:09.946+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
2019-09-21T20:19:09.947+0800 I  REPL     [replication-0] sync source candidate: localhost:37017
2019-09-21T20:19:09.947+0800 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
2019-09-21T20:19:09.947+0800 I  REPL     [replication-0] ******
2019-09-21T20:19:09.947+0800 I  REPL     [replication-0] creating replication oplog of size: 990MB...
2019-09-21T20:19:09.947+0800 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: 04b4a96a-9433-4963-a8e9-1f79504a11ae and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T20:19:09.959+0800 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:19:09.959+0800 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T20:19:09.959+0800 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:19:10.018+0800 I  REPL     [replication-0] ******
2019-09-21T20:19:10.019+0800 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
2019-09-21T20:19:10.019+0800 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
2019-09-21T20:19:10.036+0800 I  SHARDING [replication-1] Marking collection local.temp_oplog_buffer as collection version: <unsharded>
2019-09-21T20:19:10.036+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.keys
2019-09-21T20:19:10.048+0800 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.keys with provided UUID: 205d36c8-6009-49e3-a1e1-0e163c157633 and options: { uuid: UUID("205d36c8-6009-49e3-a1e1-0e163c157633") }
2019-09-21T20:19:10.078+0800 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.keys properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.keys" } using method: Foreground
2019-09-21T20:19:10.078+0800 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:19:10.112+0800 I  SHARDING [repl-writer-worker-1] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:19:10.112+0800 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.keys finished cloning with status: OK
2019-09-21T20:19:10.113+0800 I  INDEX    [replication-1] index build: inserted 2 keys from external sorter into index in 0 seconds
2019-09-21T20:19:10.120+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.keys
2019-09-21T20:19:10.120+0800 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.version
2019-09-21T20:19:10.121+0800 I  STORAGE  [repl-writer-worker-2] createCollection: admin.system.version with provided UUID: d0fb4bcc-6cbd-43a2-8f0e-715768ad09ca and options: { uuid: UUID("d0fb4bcc-6cbd-43a2-8f0e-715768ad09ca") }
2019-09-21T20:19:10.149+0800 I  INDEX    [repl-writer-worker-2] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
2019-09-21T20:19:10.149+0800 I  INDEX    [repl-writer-worker-2] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:19:10.183+0800 I  COMMAND  [repl-writer-worker-3] setting featureCompatibilityVersion to 4.2
2019-09-21T20:19:10.183+0800 I  NETWORK  [repl-writer-worker-3] Skip closing connection for connection # 3
2019-09-21T20:19:10.183+0800 I  NETWORK  [repl-writer-worker-3] Skip closing connection for connection # 1
2019-09-21T20:19:10.183+0800 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.version finished cloning with status: OK
2019-09-21T20:19:10.184+0800 I  INDEX    [replication-0] index build: inserted 2 keys from external sorter into index in 0 seconds
2019-09-21T20:19:10.190+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.version
2019-09-21T20:19:10.190+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.users
2019-09-21T20:19:10.191+0800 I  STORAGE  [repl-writer-worker-4] createCollection: admin.system.users with provided UUID: f7606230-2246-40c4-8242-8bd0c1aabfd2 and options: { uuid: UUID("f7606230-2246-40c4-8242-8bd0c1aabfd2") }
2019-09-21T20:19:10.241+0800 I  INDEX    [repl-writer-worker-4] index build: starting on admin.system.users properties: { v: 2, unique: true, key: { user: 1, db: 1 }, name: "user_1_db_1", ns: "admin.system.users" } using method: Hybrid
2019-09-21T20:19:10.241+0800 I  INDEX    [repl-writer-worker-4] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:19:10.282+0800 I  INDEX    [repl-writer-worker-4] index build: starting on admin.system.users properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.users" } using method: Hybrid
2019-09-21T20:19:10.282+0800 I  INDEX    [repl-writer-worker-4] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:19:10.316+0800 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.users finished cloning with status: OK
2019-09-21T20:19:10.316+0800 I  INDEX    [replication-1] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T20:19:10.325+0800 I  INDEX    [replication-1] index build: done building index user_1_db_1 on ns admin.system.users
2019-09-21T20:19:10.325+0800 I  INDEX    [replication-1] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T20:19:10.331+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.users
2019-09-21T20:19:10.354+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:config.transactions
2019-09-21T20:19:10.355+0800 I  STORAGE  [repl-writer-worker-6] createCollection: config.transactions with provided UUID: 529f7366-1b69-48e5-97ca-8fdd8a540ebc and options: { uuid: UUID("529f7366-1b69-48e5-97ca-8fdd8a540ebc") }
2019-09-21T20:19:10.407+0800 I  INDEX    [repl-writer-worker-6] index build: starting on config.transactions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.transactions" } using method: Hybrid
2019-09-21T20:19:10.407+0800 I  INDEX    [repl-writer-worker-6] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:19:10.442+0800 I  INITSYNC [replication-1] CollectionCloner ns:config.transactions finished cloning with status: OK
2019-09-21T20:19:10.442+0800 I  INDEX    [replication-1] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:19:10.448+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns config.transactions
2019-09-21T20:19:10.460+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:test.c
2019-09-21T20:19:10.461+0800 I  STORAGE  [repl-writer-worker-7] createCollection: test.c with provided UUID: 53faa251-cc6c-4675-a3cb-a9893b5a4790 and options: { uuid: UUID("53faa251-cc6c-4675-a3cb-a9893b5a4790") }
2019-09-21T20:19:10.513+0800 I  INDEX    [repl-writer-worker-7] index build: starting on test.c properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "test.c" } using method: Hybrid
2019-09-21T20:19:10.513+0800 I  INDEX    [repl-writer-worker-7] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:19:10.547+0800 I  SHARDING [repl-writer-worker-8] Marking collection test.c as collection version: <unsharded>
2019-09-21T20:19:10.547+0800 I  INITSYNC [replication-1] CollectionCloner ns:test.c finished cloning with status: OK
2019-09-21T20:19:10.548+0800 I  INDEX    [replication-1] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T20:19:10.555+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns test.c
2019-09-21T20:19:10.566+0800 I  INITSYNC [replication-1] Finished cloning data: OK. Beginning oplog replay.
2019-09-21T20:19:10.567+0800 I  INITSYNC [replication-0] No need to apply operations. (currently at { : Timestamp(1569068349, 1) })
2019-09-21T20:19:10.568+0800 I  SHARDING [replication-1] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:19:10.568+0800 I  SHARDING [replication-1] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:19:10.568+0800 I  INITSYNC [replication-0] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
2019-09-21T20:19:10.568+0800 I  INITSYNC [replication-0] Initial sync attempt finishing up.
2019-09-21T20:19:10.569+0800 I  INITSYNC [replication-0] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1569068349913), initialSyncAttempts: [], fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1569068349, 1), initialSyncOplogEnd: Timestamp(1569068349, 1), databases: { databasesCloned: 3, admin: { collections: 3, clonedCollections: 3, start: new Date(1569068350035), end: new Date(1569068350354), elapsedMillis: 319, admin.system.keys: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1569068350036), end: new Date(1569068350121), elapsedMillis: 85, receivedBatches: 1 }, admin.system.version: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1569068350121), end: new Date(1569068350190), elapsedMillis: 69, receivedBatches: 1 }, admin.system.users: { documentsToCopy: 1, documentsCopied: 1, indexes: 2, fetchedBatches: 1, start: new Date(1569068350190), end: new Date(1569068350354), elapsedMillis: 164, receivedBatches: 1 } }, config: { collections: 1, clonedCollections: 1, start: new Date(1569068350353), end: new Date(1569068350459), elapsedMillis: 106, config.transactions: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, start: new Date(1569068350354), end: new Date(1569068350459), elapsedMillis: 105, receivedBatches: 0 } }, test: { collections: 1, clonedCollections: 1, start: new Date(1569068350459), end: new Date(1569068350567), elapsedMillis: 108, test.c: { documentsToCopy: 1, documentsCopied: 1, indexes: 1, fetchedBatches: 1, start: new Date(1569068350460), end: new Date(1569068350567), elapsedMillis: 107, receivedBatches: 1 } } } }
2019-09-21T20:19:10.569+0800 I  STORAGE  [replication-1] Finishing collection drop for local.temp_oplog_buffer (295879de-f779-4b14-a0ca-22eba3977580).
2019-09-21T20:19:10.579+0800 I  SHARDING [replication-1] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:19:10.585+0800 I  INITSYNC [replication-1] initial sync done; took 0s.
2019-09-21T20:19:10.585+0800 I  REPL     [replication-1] transition to RECOVERING from STARTUP2
2019-09-21T20:19:10.585+0800 I  REPL     [replication-1] Starting replication fetcher thread
2019-09-21T20:19:10.585+0800 I  REPL     [replication-1] Starting replication applier thread
2019-09-21T20:19:10.585+0800 I  REPL     [replication-1] Starting replication reporter thread
2019-09-21T20:19:10.585+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:19:10.585+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T20:19:10.586+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:19:10.586+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T20:19:12.086+0800 I  STORAGE  [replexec-1] Triggering the first stable checkpoint. Initial Data: Timestamp(1569068349, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569068349, 1)
2019-09-21T20:19:20.037+0800 I  CONNPOOL [RS] Ending connection to host localhost:37017 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2019-09-21T20:19:27.598+0800 I  REPL     [rsBackgroundSync] sync source candidate: localhost:37017
2019-09-21T20:19:27.599+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:37017
2019-09-21T20:19:27.600+0800 I  CONNPOOL [RS] Connecting to localhost:37017
2019-09-21T20:20:03.903+0800 I  ACCESS   [conn1] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61544
2019-09-21T20:21:31.327+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:21:31.327+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:21:31.328+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to localhost:37018
2019-09-21T20:21:31.328+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to localhost:37017
2019-09-21T20:21:31.329+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61566 #14 (3 connections now open)
2019-09-21T20:21:31.329+0800 I  NETWORK  [conn14] received client metadata from 127.0.0.1:61566 conn14: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:21:31.371+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61566
2019-09-21T20:21:31.372+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:21:31.373+0800 I  NETWORK  [LogicalSessionCacheRefresh] Successfully connected to localhost:37017 (1 connections now open to localhost:37017 with a 0 second timeout)
2019-09-21T20:21:31.405+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: config.system.sessions does not exist
2019-09-21T20:21:33.880+0800 I  STORAGE  [repl-writer-worker-4] createCollection: config.system.sessions with provided UUID: 3c3606ca-79f0-4c36-9929-e90585780fd5 and options: { uuid: UUID("3c3606ca-79f0-4c36-9929-e90585780fd5") }
2019-09-21T20:21:33.914+0800 I  INDEX    [repl-writer-worker-4] index build: done building index _id_ on ns config.system.sessions
2019-09-21T20:21:33.956+0800 I  INDEX    [repl-writer-worker-8] index build: starting on config.system.sessions properties: { v: 2, key: { lastUse: 1 }, name: "lsidTTLIndex", expireAfterSeconds: 1800, ns: "config.system.sessions" } using method: Hybrid
2019-09-21T20:21:33.956+0800 I  INDEX    [repl-writer-worker-8] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:21:33.957+0800 I  STORAGE  [repl-writer-worker-8] Index build initialized: 8c1a4cb4-b057-4ec9-aba7-1e4fd979ef31: config.system.sessions (3c3606ca-79f0-4c36-9929-e90585780fd5 ): indexes: 1
2019-09-21T20:21:33.957+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2019-09-21T20:21:33.958+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:21:33.968+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: drain applied 1 side writes (inserted: 1, deleted: 0) for 'lsidTTLIndex' in 0 ms
2019-09-21T20:21:33.968+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index lsidTTLIndex on ns config.system.sessions
2019-09-21T20:21:33.973+0800 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 8c1a4cb4-b057-4ec9-aba7-1e4fd979ef31: config.system.sessions ( 3c3606ca-79f0-4c36-9929-e90585780fd5 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2019-09-21T20:22:39.744+0800 I  COMMAND  [conn3] Received replSetStepUp request
2019-09-21T20:22:39.744+0800 I  ELECTION [conn3] Starting an election due to step up request
2019-09-21T20:22:39.744+0800 I  ELECTION [conn3] skipping dry run and running for election in term 2
2019-09-21T20:22:39.749+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 299 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569068557, 1), t: 1 } }
2019-09-21T20:22:39.750+0800 I  ELECTION [replexec-1] VoteRequester(term 2) received an invalid response from localhost:37017: ShutdownInProgress: In the process of shutting down; response message: { operationTime: Timestamp(1569068557, 1), ok: 0.0, errmsg: "In the process of shutting down", code: 91, codeName: "ShutdownInProgress", $clusterTime: { clusterTime: Timestamp(1569068557, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
2019-09-21T20:22:39.750+0800 I  ELECTION [replexec-1] not becoming primary, we received insufficient votes
2019-09-21T20:22:39.750+0800 I  ELECTION [replexec-1] Lost election due to internal error
2019-09-21T20:22:39.750+0800 I  COMMAND  [conn3] replSetStepUp request failed :: caused by :: CommandFailed: Election failed.
2019-09-21T20:22:40.124+0800 I  REPL     [replexec-2] Member localhost:37017 is now in state SECONDARY
2019-09-21T20:22:40.557+0800 I  NETWORK  [conn3] end connection 127.0.0.1:61550 (2 connections now open)
2019-09-21T20:22:40.560+0800 I  REPL     [replication-1] Restarting oplog query due to error: InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown. Last fetched optime: { ts: Timestamp(1569068557, 1), t: 1 }. Restarts remaining: 1
2019-09-21T20:22:40.560+0800 I  REPL     [replication-1] Scheduled new oplog query Fetcher source: localhost:37017 database: local query: { find: "oplog.rs", filter: { ts: { $gte: Timestamp(1569068557, 1) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 2, readConcern: { afterClusterTime: Timestamp(1569068557, 1) } } query metadata: { $replData: 1, $oplogQueryData: 1, $readPreference: { mode: "secondaryPreferred" } } active: 1 findNetworkTimeout: 7000ms getMoreNetworkTimeout: 10000ms shutting down?: 0 first: 1 firstCommandScheduler: RemoteCommandRetryScheduler request: RemoteCommand 301 -- target:localhost:37017 db:local cmd:{ find: "oplog.rs", filter: { ts: { $gte: Timestamp(1569068557, 1) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 2, readConcern: { afterClusterTime: Timestamp(1569068557, 1) } } active: 1 callbackHandle.valid: 1 callbackHandle.cancelled: 0 attempt: 1 retryPolicy: RetryPolicyImpl maxAttempts: 1 maxTimeMillis: -1ms
2019-09-21T20:22:40.560+0800 I  REPL     [replication-0] Error returned from oplog query (no more query restarts left): InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown
2019-09-21T20:22:40.560+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown
2019-09-21T20:22:40.560+0800 I  REPL     [rsBackgroundSync] Clearing sync source localhost:37017 to choose a new one.
2019-09-21T20:22:40.560+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T20:22:40.561+0800 I  REPL_HB  [replexec-3] Heartbeat to localhost:37017 failed after 2 retries, response status: InterruptedAtShutdown: interrupted at shutdown
2019-09-21T20:22:40.561+0800 I  REPL     [replexec-3] Member localhost:37017 is now in state RS_DOWN - interrupted at shutdown
2019-09-21T20:22:41.061+0800 W  NETWORK  [replexec-1] Failed to check socket connectivity: 操作成功完成。
2019-09-21T20:22:41.061+0800 I  CONNPOOL [replexec-1] dropping unhealthy pooled connection to localhost:37017
2019-09-21T20:22:41.061+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:22:42.062+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:22:42.488+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:37017: InvalidSyncSource: Sync source was cleared. Was localhost:37017
2019-09-21T20:22:42.655+0800 I  CONTROL  [thread5] Ctrl-C signal
2019-09-21T20:22:42.655+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:22:42.655+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:22:42.656+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:22:42.656+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:22:42.656+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:22:42.656+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:22:42.656+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:22:42.656+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:22:43.064+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:22:43.562+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:22:43.562+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:22:43.562+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:22:43.562+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:22:43.562+0800 I  CONNPOOL [RS] Dropping all pooled connections to localhost:37017 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:22:43.563+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:22:43.564+0800 I  ASIO     [ReplicaSetMonitor-TaskExecutor] Killing all outstanding egress activity.
2019-09-21T20:22:43.564+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:22:43.564+0800 I  NETWORK  [conn14] end connection 127.0.0.1:61566 (1 connection now open)
2019-09-21T20:22:43.564+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:22:43.564+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:22:43.567+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:22:43.567+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:22:43.567+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:22:43.567+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:22:43.572+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:22:43.572+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:22:43.572+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:22:43.606+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:22:43.606+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:22:43.607+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:22:43.672+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:22:43.672+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:22:43.672+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:23:13.674+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:23:13.675+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=5684 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:23:14.164+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T20:23:14.165+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:23:14.224+0800 I  STORAGE  [initandlisten] WiredTiger message [1569068594:223985][5684:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:23:14.260+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:23:14.286+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:23:14.301+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:23:14.301+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T20:23:14.301+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T20:23:14.301+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T20:23:14.301+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T20:23:14.301+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T20:23:14.301+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T20:23:14.301+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:23:14.304+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:23:14.304+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:23:14.304+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:23:14.305+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:23:14.305+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: d44471e2-90b1-49a3-9b12-b83c027a1866 and options: { capped: true, size: 10485760 }
2019-09-21T20:23:14.329+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2019-09-21T20:23:14.329+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:23:14.456+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T20:23:14.457+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: a857e1f7-1188-4c37-8306-3dcd10e0b4ea and options: {}
2019-09-21T20:23:14.479+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2019-09-21T20:23:14.480+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 80d3f773-1b23-4721-8472-a57a91b83ceb and options: {}
2019-09-21T20:23:14.500+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2019-09-21T20:23:14.500+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:23:14.501+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: fdf65a8d-4b2f-4f3c-bc4d-7cb3f2005fda and options: {}
2019-09-21T20:23:14.525+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2019-09-21T20:23:14.525+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:23:14.525+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:23:14.525+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2019-09-21T20:23:14.525+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 32a4fc53-4636-4bcb-ab48-43d4456c9a57 and options: {}
2019-09-21T20:23:14.552+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2019-09-21T20:23:14.553+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:23:14.553+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2019-09-21T20:23:14.553+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:23:14.554+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:23:14.554+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:23:14.554+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T20:23:14.555+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:23:14.555+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T20:23:15.003+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:23:56.943+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61579 #1 (1 connection now open)
2019-09-21T20:23:56.946+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:23:56.946+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T20:23:56.946+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61579 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:04.463+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61586 #2 (2 connections now open)
2019-09-21T20:26:04.496+0800 I  ACCESS   [conn2] Successfully authenticated as principal __system on local from client 127.0.0.1:61586
2019-09-21T20:26:04.497+0800 I  NETWORK  [conn2] end connection 127.0.0.1:61586 (1 connection now open)
2019-09-21T20:26:04.498+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61587 #3 (2 connections now open)
2019-09-21T20:26:04.498+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61587 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:26:04.531+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 127.0.0.1:61587
2019-09-21T20:26:04.531+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:26:04.585+0800 I  STORAGE  [replexec-1] createCollection: local.system.replset with generated UUID: dec5ef92-6f93-448e-bc27-d75c3d579d8d and options: {}
2019-09-21T20:26:04.611+0800 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.system.replset
2019-09-21T20:26:04.612+0800 I  REPL     [replexec-1] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d86167c4c70307682f0187c') } }
2019-09-21T20:26:04.612+0800 I  REPL     [replexec-1] This node is localhost:37018 in the config
2019-09-21T20:26:04.612+0800 I  REPL     [replexec-1] transition to STARTUP2 from STARTUP
2019-09-21T20:26:04.612+0800 I  REPL     [replexec-1] Starting replication storage threads
2019-09-21T20:26:04.612+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state PRIMARY
2019-09-21T20:26:04.622+0800 I  STORAGE  [replexec-1] createCollection: local.temp_oplog_buffer with generated UUID: 0a4a6195-629b-449e-befc-f79bdd3dccf5 and options: { temp: true }
2019-09-21T20:26:04.648+0800 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.temp_oplog_buffer
2019-09-21T20:26:04.648+0800 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
2019-09-21T20:26:04.649+0800 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (0a4a6195-629b-449e-befc-f79bdd3dccf5).
2019-09-21T20:26:04.659+0800 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: cabf0620-090b-434d-b347-628fe5178109 and options: { temp: true }
2019-09-21T20:26:04.683+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
2019-09-21T20:26:04.683+0800 I  REPL     [replication-0] sync source candidate: localhost:37017
2019-09-21T20:26:04.683+0800 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
2019-09-21T20:26:04.683+0800 I  REPL     [replication-0] ******
2019-09-21T20:26:04.683+0800 I  REPL     [replication-0] creating replication oplog of size: 990MB...
2019-09-21T20:26:04.683+0800 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: 9608edfc-51de-4873-9cd5-510af3384c0e and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T20:26:04.693+0800 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:26:04.693+0800 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T20:26:04.693+0800 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:26:04.752+0800 I  REPL     [replication-0] ******
2019-09-21T20:26:04.753+0800 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
2019-09-21T20:26:04.753+0800 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
2019-09-21T20:26:04.769+0800 I  SHARDING [replication-0] Marking collection local.temp_oplog_buffer as collection version: <unsharded>
2019-09-21T20:26:04.769+0800 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.version
2019-09-21T20:26:04.781+0800 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.version with provided UUID: 1629d018-06b6-4f39-85b0-28dd32db4f49 and options: { uuid: UUID("1629d018-06b6-4f39-85b0-28dd32db4f49") }
2019-09-21T20:26:04.804+0800 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
2019-09-21T20:26:04.804+0800 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:26:04.839+0800 I  COMMAND  [repl-writer-worker-1] setting featureCompatibilityVersion to 4.2
2019-09-21T20:26:04.839+0800 I  NETWORK  [repl-writer-worker-1] Skip closing connection for connection # 3
2019-09-21T20:26:04.839+0800 I  NETWORK  [repl-writer-worker-1] Skip closing connection for connection # 1
2019-09-21T20:26:04.839+0800 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.version finished cloning with status: OK
2019-09-21T20:26:04.839+0800 I  INDEX    [replication-0] index build: inserted 2 keys from external sorter into index in 0 seconds
2019-09-21T20:26:04.846+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.version
2019-09-21T20:26:04.846+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.keys
2019-09-21T20:26:04.847+0800 I  STORAGE  [repl-writer-worker-2] createCollection: admin.system.keys with provided UUID: 2a84f089-92dc-46d5-b77e-6e9839988a81 and options: { uuid: UUID("2a84f089-92dc-46d5-b77e-6e9839988a81") }
2019-09-21T20:26:04.899+0800 I  INDEX    [repl-writer-worker-2] index build: starting on admin.system.keys properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.keys" } using method: Hybrid
2019-09-21T20:26:04.899+0800 I  INDEX    [repl-writer-worker-2] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:26:04.933+0800 I  SHARDING [repl-writer-worker-3] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:26:04.933+0800 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.keys finished cloning with status: OK
2019-09-21T20:26:04.934+0800 I  INDEX    [replication-1] index build: inserted 2 keys from external sorter into index in 0 seconds
2019-09-21T20:26:04.940+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.keys
2019-09-21T20:26:04.950+0800 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.users
2019-09-21T20:26:04.951+0800 I  STORAGE  [repl-writer-worker-4] createCollection: admin.system.users with provided UUID: 55163fe6-d944-4b90-8f6f-c9e50c70cc72 and options: { uuid: UUID("55163fe6-d944-4b90-8f6f-c9e50c70cc72") }
2019-09-21T20:26:05.001+0800 I  INDEX    [repl-writer-worker-4] index build: starting on admin.system.users properties: { v: 2, unique: true, key: { user: 1, db: 1 }, name: "user_1_db_1", ns: "admin.system.users" } using method: Hybrid
2019-09-21T20:26:05.001+0800 I  INDEX    [repl-writer-worker-4] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:26:05.036+0800 I  INDEX    [repl-writer-worker-4] index build: starting on admin.system.users properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.users" } using method: Hybrid
2019-09-21T20:26:05.036+0800 I  INDEX    [repl-writer-worker-4] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:26:05.070+0800 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.users finished cloning with status: OK
2019-09-21T20:26:05.070+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T20:26:05.077+0800 I  INDEX    [replication-0] index build: done building index user_1_db_1 on ns admin.system.users
2019-09-21T20:26:05.077+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T20:26:05.083+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.users
2019-09-21T20:26:05.104+0800 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:config.transactions
2019-09-21T20:26:05.105+0800 I  STORAGE  [repl-writer-worker-6] createCollection: config.transactions with provided UUID: 23567881-1715-416d-9a57-965da591b80b and options: { uuid: UUID("23567881-1715-416d-9a57-965da591b80b") }
2019-09-21T20:26:05.153+0800 I  INDEX    [repl-writer-worker-6] index build: starting on config.transactions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.transactions" } using method: Hybrid
2019-09-21T20:26:05.153+0800 I  INDEX    [repl-writer-worker-6] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:26:05.187+0800 I  INITSYNC [replication-0] CollectionCloner ns:config.transactions finished cloning with status: OK
2019-09-21T20:26:05.187+0800 I  INDEX    [replication-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:26:05.195+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns config.transactions
2019-09-21T20:26:05.213+0800 I  INITSYNC [replication-0] Finished cloning data: OK. Beginning oplog replay.
2019-09-21T20:26:05.214+0800 I  INITSYNC [replication-1] No need to apply operations. (currently at { : Timestamp(1569068764, 1) })
2019-09-21T20:26:05.215+0800 I  SHARDING [replication-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:26:05.215+0800 I  INITSYNC [replication-1] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
2019-09-21T20:26:05.215+0800 I  INITSYNC [replication-1] Initial sync attempt finishing up.
2019-09-21T20:26:05.215+0800 I  INITSYNC [replication-1] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1569068764648), initialSyncAttempts: [], fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1569068764, 1), initialSyncOplogEnd: Timestamp(1569068764, 1), databases: { databasesCloned: 2, admin: { collections: 3, clonedCollections: 3, start: new Date(1569068764768), end: new Date(1569068765104), elapsedMillis: 336, admin.system.version: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1569068764769), end: new Date(1569068764846), elapsedMillis: 77, receivedBatches: 1 }, admin.system.keys: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1569068764846), end: new Date(1569068764950), elapsedMillis: 104, receivedBatches: 1 }, admin.system.users: { documentsToCopy: 1, documentsCopied: 1, indexes: 2, fetchedBatches: 1, start: new Date(1569068764950), end: new Date(1569068765104), elapsedMillis: 154, receivedBatches: 1 } }, config: { collections: 1, clonedCollections: 1, start: new Date(1569068765104), end: new Date(1569068765213), elapsedMillis: 109, config.transactions: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, start: new Date(1569068765104), end: new Date(1569068765213), elapsedMillis: 109, receivedBatches: 0 } } } }
2019-09-21T20:26:05.216+0800 I  STORAGE  [replication-1] Finishing collection drop for local.temp_oplog_buffer (cabf0620-090b-434d-b347-628fe5178109).
2019-09-21T20:26:05.227+0800 I  SHARDING [replication-1] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:26:05.232+0800 I  INITSYNC [replication-1] initial sync done; took 0s.
2019-09-21T20:26:05.232+0800 I  REPL     [replication-1] transition to RECOVERING from STARTUP2
2019-09-21T20:26:05.232+0800 I  REPL     [replication-1] Starting replication fetcher thread
2019-09-21T20:26:05.232+0800 I  REPL     [replication-1] Starting replication applier thread
2019-09-21T20:26:05.232+0800 I  REPL     [replication-1] Starting replication reporter thread
2019-09-21T20:26:05.232+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:26:05.233+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T20:26:05.233+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:26:05.233+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T20:26:06.734+0800 I  STORAGE  [replexec-1] Triggering the first stable checkpoint. Initial Data: Timestamp(1569068764, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569068764, 1)
2019-09-21T20:26:14.769+0800 I  CONNPOOL [RS] Ending connection to host localhost:37017 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2019-09-21T20:26:21.242+0800 I  REPL     [rsBackgroundSync] sync source candidate: localhost:37017
2019-09-21T20:26:21.243+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:37017
2019-09-21T20:26:21.250+0800 I  CONNPOOL [RS] Connecting to localhost:37017
2019-09-21T20:26:48.255+0800 I  ACCESS   [conn1] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61579
2019-09-21T20:27:47.388+0800 I  STORAGE  [repl-writer-worker-10] createCollection: test.c with provided UUID: 41b275b8-8ec1-4f94-8ca3-6d0e30b67687 and options: { uuid: UUID("41b275b8-8ec1-4f94-8ca3-6d0e30b67687") }
2019-09-21T20:27:47.412+0800 I  INDEX    [repl-writer-worker-10] index build: done building index _id_ on ns test.c
2019-09-21T20:27:47.413+0800 I  SHARDING [repl-writer-worker-12] Marking collection test.c as collection version: <unsharded>
2019-09-21T20:28:11.792+0800 I  STORAGE  [repl-writer-worker-2] createCollection: config.system.sessions with provided UUID: 40e35112-7e85-4522-abe6-7df5722656aa and options: { uuid: UUID("40e35112-7e85-4522-abe6-7df5722656aa") }
2019-09-21T20:28:11.826+0800 I  INDEX    [repl-writer-worker-2] index build: done building index _id_ on ns config.system.sessions
2019-09-21T20:28:11.869+0800 I  INDEX    [repl-writer-worker-6] index build: starting on config.system.sessions properties: { v: 2, key: { lastUse: 1 }, name: "lsidTTLIndex", expireAfterSeconds: 1800, ns: "config.system.sessions" } using method: Hybrid
2019-09-21T20:28:11.869+0800 I  INDEX    [repl-writer-worker-6] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:28:11.869+0800 I  STORAGE  [repl-writer-worker-6] Index build initialized: dd7e0df0-eb19-4d52-ba3b-03a8c546a106: config.system.sessions (40e35112-7e85-4522-abe6-7df5722656aa ): indexes: 1
2019-09-21T20:28:11.869+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2019-09-21T20:28:11.871+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:28:11.880+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: drain applied 1 side writes (inserted: 1, deleted: 0) for 'lsidTTLIndex' in 0 ms
2019-09-21T20:28:11.880+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index lsidTTLIndex on ns config.system.sessions
2019-09-21T20:28:11.886+0800 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: dd7e0df0-eb19-4d52-ba3b-03a8c546a106: config.system.sessions ( 40e35112-7e85-4522-abe6-7df5722656aa ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2019-09-21T20:28:14.555+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:28:14.555+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to localhost:37017
2019-09-21T20:28:14.555+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to localhost:37018
2019-09-21T20:28:14.556+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61606 #14 (3 connections now open)
2019-09-21T20:28:14.557+0800 I  NETWORK  [conn14] received client metadata from 127.0.0.1:61606 conn14: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:28:14.612+0800 I  ACCESS   [conn14] Successfully authenticated as principal __system on local from client 127.0.0.1:61606
2019-09-21T20:28:14.612+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:28:14.614+0800 I  NETWORK  [LogicalSessionCacheReap] Successfully connected to localhost:37017 (1 connections now open to localhost:37017 with a 0 second timeout)
2019-09-21T20:28:14.615+0800 I  NETWORK  [LogicalSessionCacheRefresh] Successfully connected to localhost:37017 (2 connections now open to localhost:37017 with a 0 second timeout)
2019-09-21T20:28:14.710+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:28:14.710+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:28:14.742+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:28:14.742+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:33:14.554+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:33:14.555+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:33:14.589+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:33:14.590+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:33:14.622+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:33:14.623+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:33:14.655+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:33:14.655+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:38:14.555+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:38:14.555+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:38:14.621+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:38:14.622+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:38:14.653+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/localhost:37017,localhost:37018
2019-09-21T20:38:14.654+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/localhost:37017,localhost:37018
2019-09-21T20:39:36.509+0800 I  CONTROL  [thread5] Ctrl-C signal
2019-09-21T20:39:36.509+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:39:36.509+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:39:36.510+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:39:36.510+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:39:36.510+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:39:36.510+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:37017: CallbackCanceled: Reporter no longer valid
2019-09-21T20:39:36.510+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:39:36.510+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:39:36.510+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2019-09-21T20:39:36.510+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:39:36.511+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:39:36.511+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:39:36.511+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:39:36.511+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:39:36.511+0800 I  CONNPOOL [RS] Dropping all pooled connections to localhost:37017 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:39:36.512+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:39:36.513+0800 I  ASIO     [ReplicaSetMonitor-TaskExecutor] Killing all outstanding egress activity.
2019-09-21T20:39:36.513+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Dropping all pooled connections to localhost:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:39:36.513+0800 I  NETWORK  [conn14] end connection 127.0.0.1:61606 (2 connections now open)
2019-09-21T20:39:36.513+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:39:36.513+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:39:36.516+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:39:36.516+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:39:36.517+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:39:36.517+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:39:36.522+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:39:36.523+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:39:36.523+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:39:36.586+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:39:36.586+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:39:36.586+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:39:36.640+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:39:36.640+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:39:36.640+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:39:37.425+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:39:37.874+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=6028 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:39:37.920+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { port: 37018 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T20:39:37.922+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb2 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T20:39:37.922+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:39:37.963+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069577:962120][6028:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T20:39:38.060+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069578:60239][6028:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T20:39:38.152+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069578:152308][6028:140713979633712], txn-recover: Main recovery loop: starting at 1/155776 to 2/256
2019-09-21T20:39:38.153+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069578:152308][6028:140713979633712], txn-recover: Recovering log 1 through 2
2019-09-21T20:39:38.248+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069578:247395][6028:140713979633712], txn-recover: Recovering log 2 through 2
2019-09-21T20:39:38.328+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069578:327472][6028:140713979633712], txn-recover: Set global recovery timestamp: (1569069570,1)
2019-09-21T20:39:38.400+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569069570, 1)
2019-09-21T20:39:38.409+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:39:38.409+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 86 records totaling to 10175 bytes
2019-09-21T20:39:38.410+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:39:38.418+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:39:38.427+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:39:38.427+0800 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2019-09-21T20:39:38.427+0800 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2019-09-21T20:39:38.427+0800 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2019-09-21T20:39:38.427+0800 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2019-09-21T20:39:38.427+0800 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2019-09-21T20:39:38.427+0800 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2019-09-21T20:39:38.427+0800 I  CONTROL  [initandlisten] 
2019-09-21T20:39:38.460+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:39:38.460+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:39:38.461+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:39:38.461+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:39:38.462+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:39:38.591+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T20:39:38.592+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:39:38.593+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:39:38.593+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:39:38.593+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T20:39:38.594+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569069570, 1) (top of oplog: { ts: Timestamp(1569069570, 1), t: 1 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T20:39:38.594+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569069570, 1)
2019-09-21T20:39:38.594+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T20:39:38.594+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:39:38.594+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:39:38.596+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:39:38.596+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:39:38.596+0800 I  NETWORK  [initandlisten] Listening on 127.0.0.1
2019-09-21T20:39:38.596+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T20:39:38.596+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T20:39:38.633+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d86167c4c70307682f0187c') } }
2019-09-21T20:39:38.633+0800 I  REPL     [replexec-0] This node is localhost:37018 in the config
2019-09-21T20:39:38.633+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T20:39:38.634+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T20:39:38.634+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:38.636+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T20:39:38.636+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T20:39:38.636+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T20:39:38.636+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T20:39:38.636+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:39:38.636+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2019-09-21T20:39:38.637+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:39:38.637+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T20:39:38.647+0800 I  REPL     [replexec-1] Member localhost:37017 is now in state PRIMARY
2019-09-21T20:39:38.668+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61755 #3 (1 connection now open)
2019-09-21T20:39:38.669+0800 I  SHARDING [conn3] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:39:38.669+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61755 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:39:38.700+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 127.0.0.1:61755
2019-09-21T20:39:38.793+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:39:39.606+0800 I  COMMAND  [conn3] Received replSetStepUp request
2019-09-21T20:39:39.606+0800 I  ELECTION [conn3] Starting an election due to step up request
2019-09-21T20:39:39.606+0800 I  ELECTION [conn3] skipping dry run and running for election in term 2
2019-09-21T20:39:39.614+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 3 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069570, 1), t: 1 } }
2019-09-21T20:39:39.615+0800 I  ELECTION [replexec-0] VoteRequester(term 2) received an invalid response from localhost:37017: ShutdownInProgress: In the process of shutting down; response message: { operationTime: Timestamp(1569069570, 1), ok: 0.0, errmsg: "In the process of shutting down", code: 91, codeName: "ShutdownInProgress", $clusterTime: { clusterTime: Timestamp(1569069570, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
2019-09-21T20:39:39.615+0800 I  ELECTION [replexec-0] not becoming primary, we received insufficient votes
2019-09-21T20:39:39.615+0800 I  ELECTION [replexec-0] Lost election due to internal error
2019-09-21T20:39:39.615+0800 I  COMMAND  [conn3] replSetStepUp request failed :: caused by :: CommandFailed: Election failed.
2019-09-21T20:39:39.648+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state SECONDARY
2019-09-21T20:39:39.719+0800 I  NETWORK  [conn3] end connection 127.0.0.1:61755 (0 connections now open)
2019-09-21T20:39:40.148+0800 W  NETWORK  [replexec-1] Failed to check socket connectivity: 操作成功完成。
2019-09-21T20:39:40.148+0800 I  CONNPOOL [replexec-1] dropping unhealthy pooled connection to localhost:37017
2019-09-21T20:39:40.148+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:41.149+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:42.150+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:43.152+0800 I  REPL_HB  [replexec-0] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:39:43.152+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state RS_DOWN - Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:39:43.634+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:45.636+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:46.638+0800 I  REPL_HB  [replexec-0] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:39:47.138+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:48.140+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:49.143+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:50.145+0800 I  REPL_HB  [replexec-0] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:39:50.566+0800 I  ELECTION [replexec-1] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2019-09-21T20:39:50.635+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:52.637+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:53.639+0800 I  REPL_HB  [replexec-1] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:39:54.139+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:55.141+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:56.144+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:57.145+0800 I  REPL_HB  [replexec-1] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:39:57.635+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:58.636+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:39:59.638+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:00.638+0800 I  REPL_HB  [replexec-0] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:40:01.138+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:01.356+0800 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2019-09-21T20:40:02.139+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:03.142+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:04.143+0800 I  REPL_HB  [replexec-1] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:40:04.636+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:05.637+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:06.639+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:07.639+0800 I  REPL_HB  [replexec-1] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:40:08.139+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:09.141+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:10.142+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:11.144+0800 I  REPL_HB  [replexec-1] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:40:11.500+0800 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2019-09-21T20:40:11.636+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:12.639+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:13.641+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:14.643+0800 I  REPL_HB  [replexec-0] Heartbeat to localhost:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:37017 (127.0.0.1:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T20:40:15.143+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:15.831+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61805 #4 (1 connection now open)
2019-09-21T20:40:15.865+0800 I  ACCESS   [conn4] Successfully authenticated as principal __system on local from client 127.0.0.1:61805
2019-09-21T20:40:15.865+0800 I  NETWORK  [conn4] end connection 127.0.0.1:61805 (0 connections now open)
2019-09-21T20:40:15.867+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61806 #5 (1 connection now open)
2019-09-21T20:40:15.867+0800 I  NETWORK  [conn5] received client metadata from 127.0.0.1:61806 conn5: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:15.899+0800 I  ACCESS   [conn5] Successfully authenticated as principal __system on local from client 127.0.0.1:61806
2019-09-21T20:40:16.159+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state SECONDARY
2019-09-21T20:40:21.630+0800 I  ELECTION [replexec-1] Starting an election, since we've seen no PRIMARY in the past 10000ms
2019-09-21T20:40:21.630+0800 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 2
2019-09-21T20:40:21.630+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 46 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069570, 1), t: 1 } }
2019-09-21T20:40:21.630+0800 I  ELECTION [replexec-0] VoteRequester(term 2 dry run) received a yes vote from localhost:37017; response message: { term: 2, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569069570, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569069570, 1) }
2019-09-21T20:40:21.630+0800 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 3
2019-09-21T20:40:21.639+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 47 -- target:localhost:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 3, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069570, 1), t: 1 } }
2019-09-21T20:40:21.644+0800 I  ELECTION [replexec-0] VoteRequester(term 3) received a yes vote from localhost:37017; response message: { term: 3, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569069570, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569069570, 1) }
2019-09-21T20:40:21.644+0800 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 3
2019-09-21T20:40:21.644+0800 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
2019-09-21T20:40:21.645+0800 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
2019-09-21T20:40:21.645+0800 I  REPL     [replexec-0] Entering primary catch-up mode.
2019-09-21T20:40:21.645+0800 I  REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1569069570, 1), t: 1 }. My Last Applied: { ts: Timestamp(1569069570, 1), t: 1 }
2019-09-21T20:40:21.645+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2019-09-21T20:40:21.645+0800 I  REPL     [replexec-0] Stopping replication producer
2019-09-21T20:40:21.650+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:40:21.650+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:40:22.651+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:40:22.651+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:40:22.652+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T20:40:23.875+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61809 #7 (2 connections now open)
2019-09-21T20:40:23.875+0800 I  NETWORK  [conn7] received client metadata from 127.0.0.1:61809 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:23.907+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 127.0.0.1:61809
2019-09-21T20:40:23.917+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61810 #8 (3 connections now open)
2019-09-21T20:40:23.918+0800 I  NETWORK  [conn8] received client metadata from 127.0.0.1:61810 conn8: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:23.950+0800 I  ACCESS   [conn8] Successfully authenticated as principal __system on local from client 127.0.0.1:61810
2019-09-21T20:40:31.117+0800 I  CONTROL  [thread6] Ctrl-C signal
2019-09-21T20:40:31.117+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:40:31.117+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T20:40:31.117+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T20:40:31.117+0800 I  REPL     [consoleTerminate] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 1 }
2019-09-21T20:40:31.117+0800 I  REPL     [consoleTerminate] transition to SECONDARY from PRIMARY
2019-09-21T20:40:31.118+0800 I  REPL     [consoleTerminate] Handing off election to localhost:37017
2019-09-21T20:40:31.118+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:40:31.118+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:40:31.118+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:40:31.118+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:40:31.118+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:40:31.118+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:40:31.119+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:40:31.672+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:40:31.672+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:40:31.672+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:40:31.672+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:40:31.673+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:40:31.673+0800 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:37017 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:40:31.673+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:40:31.673+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:40:31.676+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:40:31.676+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:40:31.676+0800 E  QUERY    [conn8] GetMore command executor error: FAILURE, stats: { stage: "COLLSCAN", nReturned: 2, executionTimeMillisEstimate: 0, works: 20, advanced: 2, needTime: 9, needYield: 0, saveState: 9, restoreState: 8, isEOF: 0, direction: "forward", docsExamined: 2 }
2019-09-21T20:40:31.676+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:40:31.676+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:40:31.681+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:40:31.681+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:40:31.681+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:40:31.725+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:40:31.725+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:40:31.725+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:40:31.787+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:40:31.788+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:40:31.788+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:40:32.712+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:40:32.715+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=6356 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:40:33.208+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { bindIp: "0.0.0.0", port: 37018 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T20:40:33.210+0800 I  STORAGE  [initandlisten] Detected data files in C:\Users\xy\mongo\mdb2 created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2019-09-21T20:40:33.210+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:40:33.243+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069633:242504][6356:140713979633712], txn-recover: Recovering log 2 through 3
2019-09-21T20:40:33.327+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069633:326566][6356:140713979633712], txn-recover: Recovering log 3 through 3
2019-09-21T20:40:33.420+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069633:419654][6356:140713979633712], txn-recover: Main recovery loop: starting at 2/3968 to 3/256
2019-09-21T20:40:33.420+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069633:420655][6356:140713979633712], txn-recover: Recovering log 2 through 3
2019-09-21T20:40:33.513+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069633:512743][6356:140713979633712], txn-recover: Recovering log 3 through 3
2019-09-21T20:40:33.593+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069633:592820][6356:140713979633712], txn-recover: Set global recovery timestamp: (1569069622,1)
2019-09-21T20:40:33.636+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(1569069622, 1)
2019-09-21T20:40:33.647+0800 I  STORAGE  [initandlisten] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:40:33.647+0800 I  STORAGE  [initandlisten] The size storer reports that the oplog contains 87 records totaling to 10283 bytes
2019-09-21T20:40:33.647+0800 I  STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:40:33.655+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:40:33.697+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:40:33.698+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:40:33.698+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:40:33.698+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:40:33.699+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:40:33.825+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T20:40:33.826+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:40:33.826+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:40:33.827+0800 I  REPL     [initandlisten] Rollback ID is 1
2019-09-21T20:40:33.827+0800 I  REPL     [initandlisten] Recovering from stable timestamp: Timestamp(1569069622, 1) (top of oplog: { ts: Timestamp(1569069622, 1), t: 3 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2019-09-21T20:40:33.827+0800 I  REPL     [initandlisten] Starting recovery oplog application at the stable timestamp: Timestamp(1569069622, 1)
2019-09-21T20:40:33.827+0800 I  REPL     [initandlisten] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2019-09-21T20:40:33.827+0800 I  SHARDING [initandlisten] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:40:33.827+0800 I  SHARDING [initandlisten] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:40:33.828+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:40:33.828+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:40:33.829+0800 I  NETWORK  [initandlisten] Listening on 0.0.0.0
2019-09-21T20:40:33.829+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T20:40:33.829+0800 I  CONTROL  [LogicalSessionCacheReap] Failed to reap transaction table: NotYetInitialized: Replication has not yet been configured
2019-09-21T20:40:33.866+0800 I  REPL     [replexec-0] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d86167c4c70307682f0187c') } }
2019-09-21T20:40:33.866+0800 I  REPL     [replexec-0] This node is localhost:37018 in the config
2019-09-21T20:40:33.866+0800 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
2019-09-21T20:40:33.867+0800 I  REPL     [replexec-0] Starting replication storage threads
2019-09-21T20:40:33.867+0800 I  CONNPOOL [Replication] Connecting to localhost:37017
2019-09-21T20:40:33.869+0800 I  REPL     [replexec-0] transition to RECOVERING from STARTUP2
2019-09-21T20:40:33.869+0800 I  REPL     [replexec-0] Starting replication fetcher thread
2019-09-21T20:40:33.869+0800 I  REPL     [replexec-0] Starting replication applier thread
2019-09-21T20:40:33.869+0800 I  REPL     [replexec-0] Starting replication reporter thread
2019-09-21T20:40:33.869+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:40:33.869+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2019-09-21T20:40:33.870+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:40:33.870+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T20:40:33.880+0800 I  REPL     [replexec-1] Member localhost:37017 is now in state SECONDARY
2019-09-21T20:40:34.026+0800 I  SHARDING [monitoring-keys-for-HMAC] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:40:34.180+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61816 #3 (1 connection now open)
2019-09-21T20:40:34.180+0800 I  SHARDING [conn3] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:40:34.180+0800 I  NETWORK  [conn3] received client metadata from 127.0.0.1:61816 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:34.212+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 127.0.0.1:61816
2019-09-21T20:40:41.344+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 4, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069622, 1), t: 3 } }
2019-09-21T20:40:41.344+0800 I  ELECTION [conn3] Sending vote response: { term: 4, voteGranted: true, reason: "" }
2019-09-21T20:40:41.350+0800 I  ELECTION [conn3] Received vote request: { replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 5, candidateIndex: 0, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569069622, 1), t: 3 } }
2019-09-21T20:40:41.350+0800 I  ELECTION [conn3] Sending vote response: { term: 5, voteGranted: true, reason: "" }
2019-09-21T20:40:41.387+0800 I  REPL     [replexec-0] Member localhost:37017 is now in state PRIMARY
2019-09-21T20:40:43.876+0800 I  REPL     [rsBackgroundSync] sync source candidate: localhost:37017
2019-09-21T20:40:43.876+0800 I  CONNPOOL [RS] Connecting to localhost:37017
2019-09-21T20:40:43.890+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:37017
2019-09-21T20:40:43.891+0800 I  SHARDING [rsSync-0] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:40:49.059+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61821 #6 (2 connections now open)
2019-09-21T20:40:49.060+0800 I  NETWORK  [conn6] received client metadata from 127.0.0.1:61821 conn6: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:40:49.063+0800 I  ACCESS   [conn6] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61821
2019-09-21T20:41:31.880+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61835 #7 (3 connections now open)
2019-09-21T20:41:31.881+0800 I  NETWORK  [conn7] received client metadata from 127.0.0.1:61835 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:41:31.912+0800 I  ACCESS   [conn7] Successfully authenticated as principal __system on local from client 127.0.0.1:61835
2019-09-21T20:41:42.674+0800 I  NETWORK  [conn6] end connection 127.0.0.1:61821 (2 connections now open)
2019-09-21T20:41:55.845+0800 I  CONTROL  [thread4] Ctrl-C signal
2019-09-21T20:41:55.845+0800 I  CONTROL  [consoleTerminate] got CTRL_C_EVENT, will terminate after current cmd ends
2019-09-21T20:41:55.845+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T20:41:55.845+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T20:41:55.846+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T20:41:55.846+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T20:41:55.846+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:37017: CallbackCanceled: Reporter no longer valid
2019-09-21T20:41:55.846+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T20:41:55.846+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T20:41:55.846+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2019-09-21T20:41:55.846+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T20:41:55.846+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T20:41:55.846+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T20:41:55.846+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:41:55.847+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T20:41:55.847+0800 I  CONNPOOL [RS] Dropping all pooled connections to localhost:37017 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T20:41:55.848+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T20:41:55.848+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T20:41:55.848+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T20:41:55.851+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T20:41:55.852+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T20:41:55.852+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T20:41:55.852+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T20:41:55.857+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T20:41:55.857+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T20:41:55.857+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T20:41:55.871+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T20:41:55.871+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T20:41:55.871+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T20:41:55.928+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T20:41:55.928+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T20:41:55.928+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
2019-09-21T20:42:26.013+0800 I  CONTROL  [main] ***** SERVER RESTARTED *****
2019-09-21T20:42:26.014+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=5440 port=37018 dbpath=C:\Users\xy\mongo\mdb2 64-bit host=DESKTOP-ERRND3C
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten] db version v4.2.0
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten] git version: a4b751dcf51dd249c5865812b390cfd1c0129c30
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten] modules: none
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten] build environment:
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten]     distmod: 2012plus
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2019-09-21T20:42:26.507+0800 I  CONTROL  [initandlisten] options: { config: "m2.conf", net: { bindIp: "0.0.0.0", port: 37018 }, replication: { replSetName: "rs0" }, security: { keyFile: "C:\Users\xy\mongo\mkey" }, storage: { dbPath: "C:\Users\xy\mongo\mdb2", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "C:\Users\xy\mongo\mdb2.log" } }
2019-09-21T20:42:26.509+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=1487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2019-09-21T20:42:26.568+0800 I  STORAGE  [initandlisten] WiredTiger message [1569069746:568075][5440:140713979633712], txn-recover: Set global recovery timestamp: (0,0)
2019-09-21T20:42:26.597+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2019-09-21T20:42:26.632+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2019-09-21T20:42:26.654+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2019-09-21T20:42:26.655+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2019-09-21T20:42:26.655+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2019-09-21T20:42:26.655+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2019-09-21T20:42:26.655+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 95cbbe70-6526-4d9c-9d73-73ef1d61308d and options: { capped: true, size: 10485760 }
2019-09-21T20:42:26.679+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2019-09-21T20:42:26.679+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2019-09-21T20:42:26.804+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory 'C:/Users/xy/mongo/mdb2/diagnostic.data'
2019-09-21T20:42:26.805+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 55c38ff9-46ab-4582-a3a8-fef829972a49 and options: {}
2019-09-21T20:42:26.832+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2019-09-21T20:42:26.832+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 1400b99c-1d79-4581-956c-1715836c6509 and options: {}
2019-09-21T20:42:26.857+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2019-09-21T20:42:26.857+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2019-09-21T20:42:26.857+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 61c8064f-1d8d-4d53-8b8e-39f8784356a4 and options: {}
2019-09-21T20:42:26.883+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2019-09-21T20:42:26.883+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2019-09-21T20:42:26.883+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2019-09-21T20:42:26.883+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2019-09-21T20:42:26.883+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 33d6d437-5fcf-43dc-94a1-57bdd2dad2c3 and options: {}
2019-09-21T20:42:26.912+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2019-09-21T20:42:26.912+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2019-09-21T20:42:26.912+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2019-09-21T20:42:26.913+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2019-09-21T20:42:26.914+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:42:26.914+0800 I  SHARDING [LogicalSessionCacheReap] Marking collection config.system.sessions as collection version: <unsharded>
2019-09-21T20:42:26.914+0800 I  NETWORK  [initandlisten] Listening on 0.0.0.0
2019-09-21T20:42:26.914+0800 I  NETWORK  [initandlisten] waiting for connections on port 37018
2019-09-21T20:42:26.914+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:42:27.003+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2019-09-21T20:42:44.270+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:61847 #1 (1 connection now open)
2019-09-21T20:42:44.272+0800 I  SHARDING [conn1] Marking collection admin.system.users as collection version: <unsharded>
2019-09-21T20:42:44.272+0800 I  ACCESS   [conn1] note: no users configured in admin.system.users, allowing localhost access
2019-09-21T20:42:44.272+0800 I  NETWORK  [conn1] received client metadata from 127.0.0.1:61847 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:47:26.915+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Replication has not yet been configured
2019-09-21T20:47:26.915+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: config.system.sessions does not exist
2019-09-21T20:48:33.032+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61862 #2 (2 connections now open)
2019-09-21T20:48:33.066+0800 I  ACCESS   [conn2] Successfully authenticated as principal __system on local from client 192.168.1.7:61862
2019-09-21T20:48:33.067+0800 I  NETWORK  [conn2] end connection 192.168.1.7:61862 (1 connection now open)
2019-09-21T20:48:33.068+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:61863 #3 (2 connections now open)
2019-09-21T20:48:33.069+0800 I  NETWORK  [conn3] received client metadata from 192.168.1.7:61863 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:48:33.101+0800 I  ACCESS   [conn3] Successfully authenticated as principal __system on local from client 192.168.1.7:61863
2019-09-21T20:48:33.102+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T20:48:33.178+0800 I  STORAGE  [replexec-1] createCollection: local.system.replset with generated UUID: d04d68d5-0a56-41a9-a417-05f6a0bf927c and options: {}
2019-09-21T20:48:33.207+0800 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.system.replset
2019-09-21T20:48:33.208+0800 I  REPL     [replexec-1] New replica set config in use: { _id: "rs0", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-ERRND3C:37017", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "192.168.1.7:37018", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5d861b6069e078823b680d48') } }
2019-09-21T20:48:33.208+0800 I  REPL     [replexec-1] This node is 192.168.1.7:37018 in the config
2019-09-21T20:48:33.208+0800 I  REPL     [replexec-1] transition to STARTUP2 from STARTUP
2019-09-21T20:48:33.208+0800 I  REPL     [replexec-1] Starting replication storage threads
2019-09-21T20:48:33.209+0800 I  REPL     [replexec-0] Member DESKTOP-ERRND3C:37017 is now in state PRIMARY
2019-09-21T20:48:33.218+0800 I  STORAGE  [replexec-1] createCollection: local.temp_oplog_buffer with generated UUID: 873f016d-aef9-45f1-8159-224faa8dd0cf and options: { temp: true }
2019-09-21T20:48:33.244+0800 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.temp_oplog_buffer
2019-09-21T20:48:33.244+0800 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
2019-09-21T20:48:33.244+0800 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (873f016d-aef9-45f1-8159-224faa8dd0cf).
2019-09-21T20:48:33.254+0800 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: bd6788c3-90a2-4186-b16f-e0b754ac594e and options: { temp: true }
2019-09-21T20:48:33.277+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
2019-09-21T20:48:33.278+0800 I  REPL     [replication-0] sync source candidate: DESKTOP-ERRND3C:37017
2019-09-21T20:48:33.278+0800 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
2019-09-21T20:48:33.278+0800 I  REPL     [replication-0] ******
2019-09-21T20:48:33.278+0800 I  REPL     [replication-0] creating replication oplog of size: 990MB...
2019-09-21T20:48:33.278+0800 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: 798e3271-d4d1-451d-802b-50c7823d07d4 and options: { capped: true, size: 1038090240, autoIndexId: false }
2019-09-21T20:48:33.290+0800 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
2019-09-21T20:48:33.291+0800 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2019-09-21T20:48:33.291+0800 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
2019-09-21T20:48:33.346+0800 I  REPL     [replication-0] ******
2019-09-21T20:48:33.346+0800 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
2019-09-21T20:48:33.346+0800 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
2019-09-21T20:48:33.382+0800 I  SHARDING [replication-1] Marking collection local.temp_oplog_buffer as collection version: <unsharded>
2019-09-21T20:48:33.383+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.version
2019-09-21T20:48:33.415+0800 I  STORAGE  [repl-writer-worker-15] createCollection: admin.system.version with provided UUID: 7797de70-114c-41ac-8482-c04ac37dc118 and options: { uuid: UUID("7797de70-114c-41ac-8482-c04ac37dc118") }
2019-09-21T20:48:33.440+0800 I  INDEX    [repl-writer-worker-15] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
2019-09-21T20:48:33.440+0800 I  INDEX    [repl-writer-worker-15] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:48:33.474+0800 I  COMMAND  [repl-writer-worker-0] setting featureCompatibilityVersion to 4.2
2019-09-21T20:48:33.474+0800 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 3
2019-09-21T20:48:33.474+0800 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 1
2019-09-21T20:48:33.474+0800 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.version finished cloning with status: OK
2019-09-21T20:48:33.474+0800 I  INDEX    [replication-1] index build: inserted 2 keys from external sorter into index in 0 seconds
2019-09-21T20:48:33.480+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.version
2019-09-21T20:48:33.481+0800 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.users
2019-09-21T20:48:33.481+0800 I  STORAGE  [repl-writer-worker-1] createCollection: admin.system.users with provided UUID: 8c86bb41-37ea-4d19-a79a-a242316598a3 and options: { uuid: UUID("8c86bb41-37ea-4d19-a79a-a242316598a3") }
2019-09-21T20:48:33.532+0800 I  INDEX    [repl-writer-worker-1] index build: starting on admin.system.users properties: { v: 2, unique: true, key: { user: 1, db: 1 }, name: "user_1_db_1", ns: "admin.system.users" } using method: Hybrid
2019-09-21T20:48:33.532+0800 I  INDEX    [repl-writer-worker-1] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:48:33.572+0800 I  INDEX    [repl-writer-worker-1] index build: starting on admin.system.users properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.users" } using method: Hybrid
2019-09-21T20:48:33.572+0800 I  INDEX    [repl-writer-worker-1] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:48:33.605+0800 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.users finished cloning with status: OK
2019-09-21T20:48:33.606+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T20:48:33.612+0800 I  INDEX    [replication-0] index build: done building index user_1_db_1 on ns admin.system.users
2019-09-21T20:48:33.612+0800 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
2019-09-21T20:48:33.618+0800 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.users
2019-09-21T20:48:33.638+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.keys
2019-09-21T20:48:33.639+0800 I  STORAGE  [repl-writer-worker-3] createCollection: admin.system.keys with provided UUID: f6199117-0e8e-4524-a7b7-47f3aff47559 and options: { uuid: UUID("f6199117-0e8e-4524-a7b7-47f3aff47559") }
2019-09-21T20:48:33.687+0800 I  INDEX    [repl-writer-worker-3] index build: starting on admin.system.keys properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.keys" } using method: Hybrid
2019-09-21T20:48:33.687+0800 I  INDEX    [repl-writer-worker-3] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:48:33.721+0800 I  SHARDING [repl-writer-worker-4] Marking collection admin.system.keys as collection version: <unsharded>
2019-09-21T20:48:33.721+0800 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.keys finished cloning with status: OK
2019-09-21T20:48:33.722+0800 I  INDEX    [replication-1] index build: inserted 2 keys from external sorter into index in 0 seconds
2019-09-21T20:48:33.728+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.keys
2019-09-21T20:48:33.741+0800 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:config.transactions
2019-09-21T20:48:33.742+0800 I  STORAGE  [repl-writer-worker-5] createCollection: config.transactions with provided UUID: 06cc708c-e982-4201-bc18-5f35a64b54cb and options: { uuid: UUID("06cc708c-e982-4201-bc18-5f35a64b54cb") }
2019-09-21T20:48:33.791+0800 I  INDEX    [repl-writer-worker-5] index build: starting on config.transactions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.transactions" } using method: Hybrid
2019-09-21T20:48:33.791+0800 I  INDEX    [repl-writer-worker-5] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:48:33.825+0800 I  INITSYNC [replication-1] CollectionCloner ns:config.transactions finished cloning with status: OK
2019-09-21T20:48:33.825+0800 I  INDEX    [replication-1] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:48:33.831+0800 I  INDEX    [replication-1] index build: done building index _id_ on ns config.transactions
2019-09-21T20:48:33.841+0800 I  INITSYNC [replication-1] Finished cloning data: OK. Beginning oplog replay.
2019-09-21T20:48:33.842+0800 I  INITSYNC [replication-0] No need to apply operations. (currently at { : Timestamp(1569070113, 1) })
2019-09-21T20:48:33.843+0800 I  SHARDING [replication-1] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2019-09-21T20:48:33.843+0800 I  INITSYNC [replication-0] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
2019-09-21T20:48:33.843+0800 I  INITSYNC [replication-0] Initial sync attempt finishing up.
2019-09-21T20:48:33.843+0800 I  INITSYNC [replication-0] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1569070113244), initialSyncAttempts: [], fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1569070113, 1), initialSyncOplogEnd: Timestamp(1569070113, 1), databases: { databasesCloned: 2, admin: { collections: 3, clonedCollections: 3, start: new Date(1569070113382), end: new Date(1569070113740), elapsedMillis: 358, admin.system.version: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1569070113383), end: new Date(1569070113481), elapsedMillis: 98, receivedBatches: 1 }, admin.system.users: { documentsToCopy: 1, documentsCopied: 1, indexes: 2, fetchedBatches: 1, start: new Date(1569070113481), end: new Date(1569070113638), elapsedMillis: 157, receivedBatches: 1 }, admin.system.keys: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, start: new Date(1569070113638), end: new Date(1569070113740), elapsedMillis: 102, receivedBatches: 1 } }, config: { collections: 1, clonedCollections: 1, start: new Date(1569070113740), end: new Date(1569070113841), elapsedMillis: 101, config.transactions: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, start: new Date(1569070113741), end: new Date(1569070113841), elapsedMillis: 100, receivedBatches: 0 } } } }
2019-09-21T20:48:33.843+0800 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (bd6788c3-90a2-4186-b16f-e0b754ac594e).
2019-09-21T20:48:33.854+0800 I  SHARDING [replication-0] Marking collection config.transactions as collection version: <unsharded>
2019-09-21T20:48:33.859+0800 I  INITSYNC [replication-0] initial sync done; took 0s.
2019-09-21T20:48:33.859+0800 I  REPL     [replication-0] transition to RECOVERING from STARTUP2
2019-09-21T20:48:33.859+0800 I  REPL     [replication-0] Starting replication fetcher thread
2019-09-21T20:48:33.860+0800 I  REPL     [replication-0] Starting replication applier thread
2019-09-21T20:48:33.860+0800 I  REPL     [replication-0] Starting replication reporter thread
2019-09-21T20:48:33.860+0800 I  REPL     [rsSync-0] Starting oplog application
2019-09-21T20:48:33.860+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T20:48:33.860+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2019-09-21T20:48:33.860+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2019-09-21T20:48:35.361+0800 I  STORAGE  [replexec-1] Triggering the first stable checkpoint. Initial Data: Timestamp(1569070113, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1569070113, 1)
2019-09-21T20:48:43.382+0800 I  CONNPOOL [RS] Ending connection to host DESKTOP-ERRND3C:37017 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2019-09-21T20:48:53.877+0800 I  REPL     [rsBackgroundSync] sync source candidate: DESKTOP-ERRND3C:37017
2019-09-21T20:48:53.878+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to DESKTOP-ERRND3C:37017
2019-09-21T20:48:53.885+0800 I  CONNPOOL [RS] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T20:49:24.175+0800 I  STORAGE  [repl-writer-worker-15] createCollection: config.system.sessions with provided UUID: 8ca74314-c408-4ad1-ac17-b993991be758 and options: { uuid: UUID("8ca74314-c408-4ad1-ac17-b993991be758") }
2019-09-21T20:49:24.212+0800 I  INDEX    [repl-writer-worker-15] index build: done building index _id_ on ns config.system.sessions
2019-09-21T20:49:24.253+0800 I  INDEX    [repl-writer-worker-3] index build: starting on config.system.sessions properties: { v: 2, key: { lastUse: 1 }, name: "lsidTTLIndex", expireAfterSeconds: 1800, ns: "config.system.sessions" } using method: Hybrid
2019-09-21T20:49:24.253+0800 I  INDEX    [repl-writer-worker-3] build may temporarily use up to 500 megabytes of RAM
2019-09-21T20:49:24.253+0800 I  STORAGE  [repl-writer-worker-3] Index build initialized: 291146ac-ac44-42fa-b5a6-235dd1972c25: config.system.sessions (8ca74314-c408-4ad1-ac17-b993991be758 ): indexes: 1
2019-09-21T20:49:24.254+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2019-09-21T20:49:24.254+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2019-09-21T20:49:24.265+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: drain applied 1 side writes (inserted: 1, deleted: 0) for 'lsidTTLIndex' in 0 ms
2019-09-21T20:49:24.265+0800 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index lsidTTLIndex on ns config.system.sessions
2019-09-21T20:49:24.271+0800 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 291146ac-ac44-42fa-b5a6-235dd1972c25: config.system.sessions ( 8ca74314-c408-4ad1-ac17-b993991be758 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2019-09-21T20:52:26.916+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:52:26.916+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T20:52:26.917+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to 192.168.1.7:37018
2019-09-21T20:52:26.917+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62065 #13 (3 connections now open)
2019-09-21T20:52:26.918+0800 I  NETWORK  [conn13] received client metadata from 192.168.1.7:62065 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T20:52:26.980+0800 I  ACCESS   [conn13] Successfully authenticated as principal __system on local from client 192.168.1.7:62065
2019-09-21T20:52:26.981+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:52:26.982+0800 I  NETWORK  [LogicalSessionCacheRefresh] Successfully connected to DESKTOP-ERRND3C:37017 (1 connections now open to DESKTOP-ERRND3C:37017 with a 0 second timeout)
2019-09-21T20:52:26.982+0800 I  NETWORK  [LogicalSessionCacheReap] Successfully connected to DESKTOP-ERRND3C:37017 (2 connections now open to DESKTOP-ERRND3C:37017 with a 0 second timeout)
2019-09-21T20:52:27.035+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:52:27.035+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:52:27.069+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:52:27.069+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:52:27.101+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:52:27.102+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:53:07.899+0800 I  ACCESS   [conn1] Successfully authenticated as principal user1 on admin from client 127.0.0.1:61847
2019-09-21T20:53:59.251+0800 I  STORAGE  [repl-writer-worker-11] createCollection: d.c with provided UUID: ec6c1548-a84d-4e64-948c-2cb60921b24d and options: { uuid: UUID("ec6c1548-a84d-4e64-948c-2cb60921b24d") }
2019-09-21T20:53:59.276+0800 I  INDEX    [repl-writer-worker-11] index build: done building index _id_ on ns d.c
2019-09-21T20:53:59.277+0800 I  SHARDING [repl-writer-worker-13] Marking collection d.c as collection version: <unsharded>
2019-09-21T20:57:26.916+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:57:26.916+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:57:26.999+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:57:26.999+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:57:27.036+0800 I  NETWORK  [LogicalSessionCacheRefresh] Starting new replica set monitor for rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T20:57:27.036+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs0 is rs0/192.168.1.7:37018,DESKTOP-ERRND3C:37017
2019-09-21T21:00:02.928+0800 I  NETWORK  [conn1] end connection 127.0.0.1:61847 (2 connections now open)
2019-09-21T21:00:18.260+0800 I  COMMAND  [conn3] Received replSetStepUp request
2019-09-21T21:00:18.260+0800 I  ELECTION [conn3] Starting an election due to step up request
2019-09-21T21:00:18.260+0800 I  ELECTION [conn3] skipping dry run and running for election in term 2
2019-09-21T21:00:18.265+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 874 -- target:DESKTOP-ERRND3C:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569070812, 1), t: 1 } }
2019-09-21T21:00:18.266+0800 I  ELECTION [replexec-1] VoteRequester(term 2) received an invalid response from DESKTOP-ERRND3C:37017: ShutdownInProgress: In the process of shutting down; response message: { operationTime: Timestamp(1569070812, 1), ok: 0.0, errmsg: "In the process of shutting down", code: 91, codeName: "ShutdownInProgress", $clusterTime: { clusterTime: Timestamp(1569070812, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
2019-09-21T21:00:18.266+0800 I  ELECTION [replexec-0] not becoming primary, we received insufficient votes
2019-09-21T21:00:18.266+0800 I  ELECTION [replexec-0] Lost election due to internal error
2019-09-21T21:00:18.266+0800 I  COMMAND  [conn3] replSetStepUp request failed :: caused by :: CommandFailed: Election failed.
2019-09-21T21:00:18.959+0800 I  NETWORK  [conn3] end connection 192.168.1.7:61863 (1 connection now open)
2019-09-21T21:00:18.963+0800 I  REPL     [replication-1] Restarting oplog query due to error: InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown. Last fetched optime: { ts: Timestamp(1569070812, 1), t: 1 }. Restarts remaining: 1
2019-09-21T21:00:18.963+0800 I  REPL     [replication-1] Scheduled new oplog query Fetcher source: DESKTOP-ERRND3C:37017 database: local query: { find: "oplog.rs", filter: { ts: { $gte: Timestamp(1569070812, 1) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 2, readConcern: { afterClusterTime: Timestamp(1569070812, 1) } } query metadata: { $replData: 1, $oplogQueryData: 1, $readPreference: { mode: "secondaryPreferred" } } active: 1 findNetworkTimeout: 7000ms getMoreNetworkTimeout: 10000ms shutting down?: 0 first: 1 firstCommandScheduler: RemoteCommandRetryScheduler request: RemoteCommand 875 -- target:DESKTOP-ERRND3C:37017 db:local cmd:{ find: "oplog.rs", filter: { ts: { $gte: Timestamp(1569070812, 1) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 2, readConcern: { afterClusterTime: Timestamp(1569070812, 1) } } active: 1 callbackHandle.valid: 1 callbackHandle.cancelled: 0 attempt: 1 retryPolicy: RetryPolicyImpl maxAttempts: 1 maxTimeMillis: -1ms
2019-09-21T21:00:18.964+0800 I  REPL     [replication-0] Error returned from oplog query (no more query restarts left): InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown
2019-09-21T21:00:18.964+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown
2019-09-21T21:00:18.964+0800 I  REPL     [rsBackgroundSync] Clearing sync source DESKTOP-ERRND3C:37017 to choose a new one.
2019-09-21T21:00:18.964+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2019-09-21T21:00:18.965+0800 I  REPL_HB  [replexec-1] Heartbeat to DESKTOP-ERRND3C:37017 failed after 2 retries, response status: InterruptedAtShutdown: interrupted at shutdown
2019-09-21T21:00:18.965+0800 I  REPL     [replexec-1] Member DESKTOP-ERRND3C:37017 is now in state RS_DOWN - interrupted at shutdown
2019-09-21T21:00:19.466+0800 W  NETWORK  [replexec-0] Failed to check socket connectivity: 操作成功完成。
2019-09-21T21:00:19.466+0800 I  CONNPOOL [replexec-0] dropping unhealthy pooled connection to DESKTOP-ERRND3C:37017
2019-09-21T21:00:19.466+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:00:20.467+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:00:21.470+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:00:22.471+0800 I  REPL_HB  [replexec-1] Heartbeat to DESKTOP-ERRND3C:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to DESKTOP-ERRND3C:37017 (192.168.1.7:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T21:00:22.714+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to DESKTOP-ERRND3C:37017: InvalidSyncSource: Sync source was cleared. Was DESKTOP-ERRND3C:37017
2019-09-21T21:00:22.971+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:00:24.974+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:00:25.975+0800 I  REPL_HB  [replexec-3] Heartbeat to DESKTOP-ERRND3C:37017 failed after 2 retries, response status: HostUnreachable: Error connecting to DESKTOP-ERRND3C:37017 (192.168.1.7:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T21:00:26.164+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:00:26.230+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62112 #18 (2 connections now open)
2019-09-21T21:00:26.262+0800 I  ACCESS   [conn18] Successfully authenticated as principal __system on local from client 192.168.1.7:62112
2019-09-21T21:00:26.262+0800 I  NETWORK  [conn18] end connection 192.168.1.7:62112 (1 connection now open)
2019-09-21T21:00:26.263+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62113 #19 (2 connections now open)
2019-09-21T21:00:26.264+0800 I  NETWORK  [conn19] received client metadata from 192.168.1.7:62113 conn19: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T21:00:26.296+0800 I  ACCESS   [conn19] Successfully authenticated as principal __system on local from client 192.168.1.7:62113
2019-09-21T21:00:26.678+0800 I  REPL     [replexec-0] Member DESKTOP-ERRND3C:37017 is now in state SECONDARY
2019-09-21T21:00:26.920+0800 W  NETWORK  [PeriodicTaskRunner] Failed to check socket connectivity: 操作成功完成。
2019-09-21T21:00:26.920+0800 W  NETWORK  [PeriodicTaskRunner] Failed to check socket connectivity: 操作成功完成。
2019-09-21T21:00:27.955+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:00:28.437+0800 I  ELECTION [replexec-3] Starting an election, since we've seen no PRIMARY in the past 10000ms
2019-09-21T21:00:28.437+0800 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 2
2019-09-21T21:00:28.437+0800 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 891 -- target:DESKTOP-ERRND3C:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: true, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569070812, 1), t: 1 } }
2019-09-21T21:00:28.437+0800 I  ELECTION [replexec-1] VoteRequester(term 2 dry run) received a yes vote from DESKTOP-ERRND3C:37017; response message: { term: 2, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569070812, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569070812, 1) }
2019-09-21T21:00:28.437+0800 I  ELECTION [replexec-1] dry election run succeeded, running for election in term 3
2019-09-21T21:00:28.443+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 892 -- target:DESKTOP-ERRND3C:37017 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs0", dryRun: false, term: 3, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1569070812, 1), t: 1 } }
2019-09-21T21:00:28.449+0800 I  ELECTION [replexec-3] VoteRequester(term 3) received a yes vote from DESKTOP-ERRND3C:37017; response message: { term: 3, voteGranted: true, reason: "", ok: 1.0, $clusterTime: { clusterTime: Timestamp(1569070812, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1569070812, 1) }
2019-09-21T21:00:28.449+0800 I  ELECTION [replexec-3] election succeeded, assuming primary role in term 3
2019-09-21T21:00:28.449+0800 I  REPL     [replexec-3] transition to PRIMARY from SECONDARY
2019-09-21T21:00:28.449+0800 I  REPL     [replexec-3] Resetting sync source to empty, which was :27017
2019-09-21T21:00:28.449+0800 I  REPL     [replexec-3] Entering primary catch-up mode.
2019-09-21T21:00:28.450+0800 I  REPL     [replexec-1] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1569070812, 1), t: 1 }. My Last Applied: { ts: Timestamp(1569070812, 1), t: 1 }
2019-09-21T21:00:28.450+0800 I  REPL     [replexec-1] Exited primary catch-up mode.
2019-09-21T21:00:28.450+0800 I  REPL     [replexec-1] Stopping replication producer
2019-09-21T21:00:28.717+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T21:00:28.717+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T21:00:29.718+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T21:00:29.718+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T21:00:29.718+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2019-09-21T21:00:30.268+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62115 #22 (3 connections now open)
2019-09-21T21:00:30.269+0800 I  NETWORK  [conn22] received client metadata from 192.168.1.7:62115 conn22: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T21:00:30.301+0800 I  ACCESS   [conn22] Successfully authenticated as principal __system on local from client 192.168.1.7:62115
2019-09-21T21:00:30.309+0800 I  NETWORK  [listener] connection accepted from 192.168.1.7:62116 #23 (4 connections now open)
2019-09-21T21:00:30.310+0800 I  NETWORK  [conn23] received client metadata from 192.168.1.7:62116 conn23: { driver: { name: "NetworkInterfaceTL", version: "4.2.0" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 10240)" } }
2019-09-21T21:00:30.341+0800 I  ACCESS   [conn23] Successfully authenticated as principal __system on local from client 192.168.1.7:62116
2019-09-21T21:01:17.741+0800 I  CONNPOOL [RS] Ending idle connection to host DESKTOP-ERRND3C:37017 because the pool meets constraints; 1 connections to that host remain open
2019-09-21T21:01:19.421+0800 I  CONNPOOL [RS] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:01:22.565+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62133 #25 (5 connections now open)
2019-09-21T21:01:22.566+0800 I  NETWORK  [conn25] received client metadata from 127.0.0.1:62133 conn25: { driver: { name: "nodejs", version: "3.1.13" }, os: { type: "Windows_NT", name: "win32", architecture: "x64", version: "10.0.10240" }, platform: "Node.js v10.2.0, LE, mongodb-core: 3.1.11" }
2019-09-21T21:01:22.571+0800 I  ACCESS   [conn25] Successfully authenticated as principal user1 on admin from client 127.0.0.1:62133
2019-09-21T21:01:23.604+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62135 #26 (6 connections now open)
2019-09-21T21:01:23.608+0800 I  ACCESS   [conn26] Successfully authenticated as principal user1 on admin from client 127.0.0.1:62135
2019-09-21T21:01:24.613+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62137 #27 (7 connections now open)
2019-09-21T21:01:24.616+0800 I  ACCESS   [conn27] Successfully authenticated as principal user1 on admin from client 127.0.0.1:62137
2019-09-21T21:01:25.637+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62139 #28 (8 connections now open)
2019-09-21T21:01:25.640+0800 I  ACCESS   [conn28] Successfully authenticated as principal user1 on admin from client 127.0.0.1:62139
2019-09-21T21:01:29.368+0800 I  NETWORK  [listener] connection accepted from 127.0.0.1:62141 #29 (9 connections now open)
2019-09-21T21:01:29.370+0800 I  ACCESS   [conn29] Successfully authenticated as principal user1 on admin from client 127.0.0.1:62141
2019-09-21T21:01:31.660+0800 I  NETWORK  [conn25] end connection 127.0.0.1:62133 (8 connections now open)
2019-09-21T21:01:31.660+0800 I  NETWORK  [conn29] end connection 127.0.0.1:62141 (6 connections now open)
2019-09-21T21:01:31.660+0800 I  NETWORK  [conn27] end connection 127.0.0.1:62137 (7 connections now open)
2019-09-21T21:01:31.660+0800 I  NETWORK  [conn26] end connection 127.0.0.1:62135 (5 connections now open)
2019-09-21T21:01:31.661+0800 I  NETWORK  [conn28] end connection 127.0.0.1:62139 (4 connections now open)
2019-09-21T21:01:33.941+0800 I  NETWORK  [conn22] end connection 192.168.1.7:62115 (3 connections now open)
2019-09-21T21:01:33.942+0800 I  NETWORK  [conn19] end connection 192.168.1.7:62113 (2 connections now open)
2019-09-21T21:01:34.470+0800 W  NETWORK  [replexec-3] Failed to check socket connectivity: 操作成功完成。
2019-09-21T21:01:34.470+0800 I  CONNPOOL [replexec-3] dropping unhealthy pooled connection to DESKTOP-ERRND3C:37017
2019-09-21T21:01:34.470+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:01:34.736+0800 I  NETWORK  [conn23] Error sending response to client: HostUnreachable: Connection reset by peer. Ending connection from 192.168.1.7:62116 (connection id: 23)
2019-09-21T21:01:34.736+0800 I  NETWORK  [conn23] end connection 192.168.1.7:62116 (1 connection now open)
2019-09-21T21:01:35.471+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:01:36.095+0800 I  CONTROL  [thread14] CTRL_CLOSE_EVENT signal
2019-09-21T21:01:36.095+0800 I  CONTROL  [consoleTerminate] got CTRL_CLOSE_EVENT, will terminate after current cmd ends
2019-09-21T21:01:36.095+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2019-09-21T21:01:36.095+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2019-09-21T21:01:36.095+0800 I  REPL     [consoleTerminate] Stepping down from primary, stats: { userOpsKilled: 0, userOpsRunning: 0 }
2019-09-21T21:01:36.095+0800 I  REPL     [consoleTerminate] transition to SECONDARY from PRIMARY
2019-09-21T21:01:36.095+0800 I  REPL     [consoleTerminate] Handing off election to DESKTOP-ERRND3C:37017
2019-09-21T21:01:36.096+0800 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2019-09-21T21:01:36.096+0800 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2019-09-21T21:01:36.096+0800 I  REPL     [consoleTerminate] shutting down replication subsystems
2019-09-21T21:01:36.096+0800 I  REPL     [consoleTerminate] Stopping replication reporter thread
2019-09-21T21:01:36.096+0800 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2019-09-21T21:01:36.096+0800 I  REPL     [consoleTerminate] Stopping replication applier thread
2019-09-21T21:01:36.096+0800 I  REPL     [rsSync-0] Finished oplog application
2019-09-21T21:01:36.472+0800 I  CONNPOOL [Replication] Dropping all pooled connections to DESKTOP-ERRND3C:37017 due to HostUnreachable: Error connecting to DESKTOP-ERRND3C:37017 (192.168.1.7:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T21:01:36.473+0800 I  REPL     [replexec-4] replSetStepUp request to DESKTOP-ERRND3C:37017 failed due to HostUnreachable: Error connecting to DESKTOP-ERRND3C:37017 (192.168.1.7:37017) :: caused by :: Ŀܾ޷ӡ
2019-09-21T21:01:36.473+0800 I  CONNPOOL [Replication] Connecting to DESKTOP-ERRND3C:37017
2019-09-21T21:01:36.998+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2019-09-21T21:01:36.998+0800 I  REPL     [consoleTerminate] Stopping replication storage threads
2019-09-21T21:01:36.998+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T21:01:36.999+0800 I  ASIO     [RS] Killing all outstanding egress activity.
2019-09-21T21:01:36.999+0800 I  ASIO     [Replication] Killing all outstanding egress activity.
2019-09-21T21:01:37.000+0800 I  ASIO     [ReplicaSetMonitor-TaskExecutor] Killing all outstanding egress activity.
2019-09-21T21:01:37.000+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Dropping all pooled connections to 192.168.1.7:37018 due to ShutdownInProgress: Shutting down the connection pool
2019-09-21T21:01:37.000+0800 I  NETWORK  [conn13] end connection 192.168.1.7:62065 (0 connections now open)
2019-09-21T21:01:37.000+0800 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2019-09-21T21:01:37.000+0800 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2019-09-21T21:01:38.000+0800 I  STORAGE  [consoleTerminate] Deregistering all the collections
2019-09-21T21:01:38.000+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2019-09-21T21:01:38.000+0800 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2019-09-21T21:01:38.000+0800 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2019-09-21T21:01:38.000+0800 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2019-09-21T21:01:38.001+0800 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2019-09-21T21:01:38.001+0800 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2019-09-21T21:01:38.009+0800 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2019-09-21T21:01:38.009+0800 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2019-09-21T21:01:38.009+0800 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2019-09-21T21:01:38.050+0800 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2019-09-21T21:01:38.051+0800 I  CONTROL  [consoleTerminate] now exiting
2019-09-21T21:01:38.051+0800 I  CONTROL  [consoleTerminate] shutting down with code:12
